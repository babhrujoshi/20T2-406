


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Home page for CPSC406 Term 2 2020">
      
      
        <link rel="canonical" href="https://babhrujoshi.github.io/20T2-406/notes/Gradient_Descent/">
      
      
        <meta name="author" content="Michael P. Friedlander and Babhru Joshi">
      
      <link rel="shortcut icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-5.2.3">
    
    
      
        <title>Gradient descent - CPSC406 &mdash; Computational Optimization</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.6e35a1a6.min.css">
      
      
    
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
        
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-4736412-7","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview")})</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
  
    <body dir="ltr">
  
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#gradient-descent" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href="https://babhrujoshi.github.io/20T2-406/" title="CPSC406 &mdash; Computational Optimization" class="md-header-nav__button md-logo" aria-label="CPSC406 &mdash; Computational Optimization">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            CPSC406 &mdash; Computational Optimization
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              Gradient descent
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
        
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="https://babhrujoshi.github.io/20T2-406/" title="CPSC406 &mdash; Computational Optimization" class="md-nav__button md-logo" aria-label="CPSC406 &mdash; Computational Optimization">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    CPSC406 &mdash; Computational Optimization
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../.." title="Home Page" class="md-nav__link">
      Home Page
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../grades/" title="Grades" class="md-nav__link">
      Grades
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../schedule/" title="Schedule" class="md-nav__link">
      Schedule
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
    
    <label class="md-nav__link" for="nav-4">
      Lecture notes
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="Lecture notes" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
        </span>
        Lecture notes
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../background/" title="Mathematical background" class="md-nav__link">
      Mathematical background
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Least_squares/" title="Least squares" class="md-nav__link">
      Least squares
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../QR_factorization/" title="QR factorization" class="md-nav__link">
      QR factorization
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Regularized_LS/" title="Regularized least squares" class="md-nav__link">
      Regularized least squares
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Non-linear_LS/" title="Non-linear least squares" class="md-nav__link">
      Non-linear least squares
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../unconstrained/" title="Unconstrained optimization" class="md-nav__link">
      Unconstrained optimization
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Gradient descent
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 9h14V7H3v2m0 4h14v-2H3v2m0 4h14v-2H3v2m16 0h2v-2h-2v2m0-10v2h2V7h-2m0 6h2v-2h-2v2z"/></svg>
        </span>
      </label>
    
    <a href="./" title="Gradient descent" class="md-nav__link md-nav__link--active">
      Gradient descent
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#descent-directions" class="md-nav__link">
    Descent Directions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stepsize-selection" class="md-nav__link">
    Stepsize selection
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#search-directions" class="md-nav__link">
    Search Directions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#convergence-of-gradient-method-with-constant-stepsize" class="md-nav__link">
    Convergence of gradient method with constant stepsize
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#condition-number-of-a-matrix" class="md-nav__link">
    Condition number of a matrix
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#scaled-gradient-method" class="md-nav__link">
    Scaled gradient method
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Newtons_method/" title="Newton's method" class="md-nav__link">
      Newton's method
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Quasi_newton/" title="Quasi-Newton methods" class="md-nav__link">
      Quasi-Newton methods
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Linear_constraint/" title="Linear constraint" class="md-nav__link">
      Linear constraint
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Convex_set/" title="Convex sets" class="md-nav__link">
      Convex sets
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Constrained_optimization/" title="Constrained optimization" class="md-nav__link">
      Constrained optimization
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Convex_function/" title="Convex function" class="md-nav__link">
      Convex function
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Linear_programming/" title="Linear programming" class="md-nav__link">
      Linear programming
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      Homework
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="Homework" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
        </span>
        Homework
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../homework/" title="Submissions" class="md-nav__link">
      Submissions
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../homework/hw1/hw1/" title="Homework 1" class="md-nav__link">
      Homework 1
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../homework/hw2/hw2/" title="Homework 2" class="md-nav__link">
      Homework 2
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../homework/hw3/hw3/" title="Homework 3" class="md-nav__link">
      Homework 3
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../homework/hw4/hw4.md" title="Homework 4" class="md-nav__link">
      Homework 4
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6">
    
    <label class="md-nav__link" for="nav-6">
      In-class Activity
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="In-class Activity" data-md-level="1">
      <label class="md-nav__title" for="nav-6">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
        </span>
        In-class Activity
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../InclassActivity/mlactivity/mlactivity/" title="Digit classification" class="md-nav__link">
      Digit classification
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#descent-directions" class="md-nav__link">
    Descent Directions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stepsize-selection" class="md-nav__link">
    Stepsize selection
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#search-directions" class="md-nav__link">
    Search Directions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#convergence-of-gradient-method-with-constant-stepsize" class="md-nav__link">
    Convergence of gradient method with constant stepsize
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#condition-number-of-a-matrix" class="md-nav__link">
    Condition number of a matrix
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#scaled-gradient-method" class="md-nav__link">
    Scaled gradient method
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  
                
                
                <h1 id="gradient-descent"><strong>Gradient Descent</strong><a class="headerlink" href="#gradient-descent" title="Permanent link">&para;</a></h1>
<h2 id="descent-directions"><strong>Descent Directions</strong><a class="headerlink" href="#descent-directions" title="Permanent link">&para;</a></h2>
<p>In this lecture we will consider unconstrained nonlinear optimization of the form</p>
<div>
<div class="MathJax_Preview">\minimize_{\vx\in\R^n} f(\vx)</div>
<script type="math/tex; mode=display">\minimize_{\vx\in\R^n} f(\vx)</script>
</div>
<p>where <span><span class="MathJax_Preview">f:\R^n \rightarrow \R</span><script type="math/tex">f:\R^n \rightarrow \R</script></span> is continuously differentiable and study an iterative algorithm that solves it. We will consider an iterative algorithm of the form</p>
<div>
<div class="MathJax_Preview">\vx^{k+1} = \vx^k + \alpha^k \vd^k, \quad k = 1,2,\dots</div>
<script type="math/tex; mode=display">\vx^{k+1} = \vx^k + \alpha^k \vd^k, \quad k = 1,2,\dots</script>
</div>
<p>where <span><span class="MathJax_Preview">\vd^k</span><script type="math/tex">\vd^k</script></span> and <span><span class="MathJax_Preview">\alpha^k</span><script type="math/tex">\alpha^k</script></span> are search direction and step length, respectively, at the <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>th iteration of the algorithm. A seach direction <span><span class="MathJax_Preview">\vd \neq 0</span><script type="math/tex">\vd \neq 0</script></span> is a descent direction for <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> at <span><span class="MathJax_Preview">\vx</span><script type="math/tex">\vx</script></span> if the directional derivative is negative, i.e.,</p>
<div>
<div class="MathJax_Preview"> f'(\vx;d) = \nabla f(x)\trans \vd &lt;0.</div>
<script type="math/tex; mode=display"> f'(\vx;d) = \nabla f(x)\trans \vd <0.</script>
</div>
<p><span><span class="MathJax_Preview">\lemma{1}</span><script type="math/tex">\lemma{1}</script></span> If <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> is continously differentiable and <span><span class="MathJax_Preview">\vd</span><script type="math/tex">\vd</script></span> is a decent direction at <span><span class="MathJax_Preview">\vx</span><script type="math/tex">\vx</script></span>, then for some <span><span class="MathJax_Preview">\epsilon &gt;0</span><script type="math/tex">\epsilon >0</script></span>, we have </p>
<div>
<div class="MathJax_Preview">f(\vx+\alpha\vd) &lt; f(\vx), \quad \forall \alpha\in(0,\epsilon].</div>
<script type="math/tex; mode=display">f(\vx+\alpha\vd) < f(\vx), \quad \forall \alpha\in(0,\epsilon].</script>
</div>
<p><span><span class="MathJax_Preview">\proof</span><script type="math/tex">\proof</script></span> (Idea) Because <span><span class="MathJax_Preview">f'(\vx;\vd)&lt;0</span><script type="math/tex">f'(\vx;\vd)<0</script></span>, we get </p>
<div>
<div class="MathJax_Preview">\lim_{\alpha \searrow 0} \frac{f(\vx+\alpha\vd-f(\vx)}{\alpha}  \equiv f'(\vx;\vd)&lt;0</div>
<script type="math/tex; mode=display">\lim_{\alpha \searrow 0} \frac{f(\vx+\alpha\vd-f(\vx)}{\alpha}  \equiv f'(\vx;\vd)<0</script>
</div>
<p>So, there exists <span><span class="MathJax_Preview">\epsilon &gt;0</span><script type="math/tex">\epsilon >0</script></span> such that <span><span class="MathJax_Preview">\frac{f(\vx+\alpha\vd-f(\vx)}{\alpha}&lt;0</span><script type="math/tex">\frac{f(\vx+\alpha\vd-f(\vx)}{\alpha}<0</script></span> for all <span><span class="MathJax_Preview">\alpha \in (0,\epsilon]</span><script type="math/tex">\alpha \in (0,\epsilon]</script></span>.</p>
<p>The general outline of decent scheme is:</p>
<hr />
<p><strong>Descent scheme outline</strong></p>
<p>Initialization: choose <span><span class="MathJax_Preview">\vx^0 \in \R^n</span><script type="math/tex">\vx^0 \in \R^n</script></span></p>
<p>For $ k = 0,1,2,\dots</p>
<ol>
<li>
<p>compute descent direction <span><span class="MathJax_Preview">\vd^k</span><script type="math/tex">\vd^k</script></span></p>
</li>
<li>
<p>compute step size <span><span class="MathJax_Preview">\alpha^k</span><script type="math/tex">\alpha^k</script></span> such that <span><span class="MathJax_Preview">f(\vx^k +\alpha^k\vd^k)&lt; f(x^k)</span><script type="math/tex">f(\vx^k +\alpha^k\vd^k)< f(x^k)</script></span></p>
</li>
<li>
<p>update <span><span class="MathJax_Preview">\vx^{k+1}  = \vx^k +\alpha^k \vd^k</span><script type="math/tex">\vx^{k+1}  = \vx^k +\alpha^k \vd^k</script></span></p>
</li>
<li>
<p>check stopping criteria</p>
</li>
</ol>
<hr />
<p>Each step in the above gradient descent scheme raises a few important question:</p>
<ol>
<li>
<p>How to determine a starting point?</p>
</li>
<li>
<p>What are advantages/disadvantages of different directions <span><span class="MathJax_Preview">\vd^k</span><script type="math/tex">\vd^k</script></span>?</p>
</li>
<li>
<p>How to compute a step length <span><span class="MathJax_Preview">\alpha^k</span><script type="math/tex">\alpha^k</script></span>?</p>
</li>
<li>
<p>when to stop?</p>
</li>
</ol>
<h2 id="stepsize-selection"><strong>Stepsize selection</strong><a class="headerlink" href="#stepsize-selection" title="Permanent link">&para;</a></h2>
<p>These are the selection rules most used in practice:</p>
<ol>
<li>
<p>Constant stepsize: Here, we fix an <span><span class="MathJax_Preview">\bar{\alpha}</span><script type="math/tex">\bar{\alpha}</script></span> and choose $\alpha^k = <span><span class="MathJax_Preview">\bar{\alpha}</span><script type="math/tex">\bar{\alpha}</script></span> for all <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>.</p>
</li>
<li>
<p>Exact linesearch: In exact linesearch, we choose <span><span class="MathJax_Preview">\alpha^k</span><script type="math/tex">\alpha^k</script></span> to minimize <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> along a ray <span><span class="MathJax_Preview">\vd^k</span><script type="math/tex">\vd^k</script></span> starting at <span><span class="MathJax_Preview">\vx^k</span><script type="math/tex">\vx^k</script></span>, i.e.</p>
<div>
<div class="MathJax_Preview"> \alpha^k = \argmin_{\alpha\geq 0} f(\vx^k+\alpha \vd^k)</div>
<script type="math/tex; mode=display"> \alpha^k = \argmin_{\alpha\geq 0} f(\vx^k+\alpha \vd^k)</script>
</div>
</li>
<li>
<p>Backtracking "Armijo" linesearch: For some parameter <span><span class="MathJax_Preview">\mu \in (0,1)</span><script type="math/tex">\mu \in (0,1)</script></span>, reduce <span><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> (eg, <span><span class="MathJax_Preview">\alpha \leftarrow \alpha/2</span><script type="math/tex">\alpha \leftarrow \alpha/2</script></span> begining with <span><span class="MathJax_Preview">\alpha = 1</span><script type="math/tex">\alpha = 1</script></span>) until the following sufficient decrease property is satisfied</p>
<div>
<div class="MathJax_Preview"> f(\vx^k) - f(\vx^k + \alpha d^k) \geq -\mu \alpha \nabla f(\vx^k)\trans\vd^k</div>
<script type="math/tex; mode=display"> f(\vx^k) - f(\vx^k + \alpha d^k) \geq -\mu \alpha \nabla f(\vx^k)\trans\vd^k</script>
</div>
<p><center>
    <img src="../img/lec7/linesearch.png" width = "600"> 
</center></p>
<p>In the above figure, <span><span class="MathJax_Preview">\alpha_3</span><script type="math/tex">\alpha_3</script></span> and <span><span class="MathJax_Preview">\alpha_4</span><script type="math/tex">\alpha_4</script></span> satisfy the sufficient decrease property.</p>
</li>
</ol>
<p><span><span class="MathJax_Preview">\exa{(Exact linesearch for quadratic functions)}</span><script type="math/tex">\exa{(Exact linesearch for quadratic functions)}</script></span> An exact linesearch is typically only possible for quadratic functions:</p>
<div>
<div class="MathJax_Preview">f(\vx) = \frac{1}{2}\vx\trans\mA\vx +\vb\trans\vx + c</div>
<script type="math/tex; mode=display">f(\vx) = \frac{1}{2}\vx\trans\mA\vx +\vb\trans\vx + c</script>
</div>
<p>with <span><span class="MathJax_Preview">\mA \succ 0</span><script type="math/tex">\mA \succ 0</script></span>. Exact line search solves the 1-dimensional optimization problem</p>
<div>
<div class="MathJax_Preview">\min_{\alpha\geq 0} f(\vx + \alpha \vd)</div>
<script type="math/tex; mode=display">\min_{\alpha\geq 0} f(\vx + \alpha \vd)</script>
</div>
<p>where <span><span class="MathJax_Preview">\vd</span><script type="math/tex">\vd</script></span> is a descent direction, and both <span><span class="MathJax_Preview">\vx</span><script type="math/tex">\vx</script></span> and <span><span class="MathJax_Preview">\vd</span><script type="math/tex">\vd</script></span> are fixed. In the quadratic case, we have</p>
<div>
<div class="MathJax_Preview">f(\vx+\alpha\vd) = \frac{1}{2}(\vx+\alpha \vd)\trans\mA(\vx+\alpha \vd) + \vb\trans(\vx+\alpha\vd)+c.</div>
<script type="math/tex; mode=display">f(\vx+\alpha\vd) = \frac{1}{2}(\vx+\alpha \vd)\trans\mA(\vx+\alpha \vd) + \vb\trans(\vx+\alpha\vd)+c.</script>
</div>
<p>Since the gradient of <span><span class="MathJax_Preview">f(\vx+\alpha\vd)</span><script type="math/tex">f(\vx+\alpha\vd)</script></span> w.r.t. <span><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> is </p>
<div>
<div class="MathJax_Preview"> \frac{d}{d\alpha}f(\vx+\alpha\vd) = \alpha\vd\trans\mA\vd+\vx\trans\mA\vd+\vb\trans\vd = \alpha\vd\trans\mA\vd + \nabla f(\vx)\trans\vd,</div>
<script type="math/tex; mode=display"> \frac{d}{d\alpha}f(\vx+\alpha\vd) = \alpha\vd\trans\mA\vd+\vx\trans\mA\vd+\vb\trans\vd = \alpha\vd\trans\mA\vd + \nabla f(\vx)\trans\vd,</script>
</div>
<p>the optimal <span><span class="MathJax_Preview">\alpha = -\frac{\nabla f(\vx)\trans\vd}{\vd\trans\mA\vd}</span><script type="math/tex">\alpha = -\frac{\nabla f(\vx)\trans\vd}{\vd\trans\mA\vd}</script></span>, which is always positive under the assumption that <span><span class="MathJax_Preview">\mA \succ 0</span><script type="math/tex">\mA \succ 0</script></span>.</p>
<h2 id="search-directions"><strong>Search Directions</strong><a class="headerlink" href="#search-directions" title="Permanent link">&para;</a></h2>
<p>The simplest search direction provides the gradient descent algorithm. In gradient descent, the search direction <span><span class="MathJax_Preview">\vd^k : = -\vg_k</span><script type="math/tex">\vd^k : = -\vg_k</script></span>, where <span><span class="MathJax_Preview">\vg_k = \nabla f(\vx^k)</span><script type="math/tex">\vg_k = \nabla f(\vx^k)</script></span>. It is easy to see that the negative gradient descent direction <span><span class="MathJax_Preview">-\vg_k</span><script type="math/tex">-\vg_k</script></span> provides a descent direction. To show that <span><span class="MathJax_Preview">-\vg_k</span><script type="math/tex">-\vg_k</script></span> is a descent direction, consider</p>
<div>
<div class="MathJax_Preview"> f'(\vx^k;\vg_k) = -\vg_k\trans\vg_k = -\|\vg\|_2^2.</div>
<script type="math/tex; mode=display"> f'(\vx^k;\vg_k) = -\vg_k\trans\vg_k = -\|\vg\|_2^2.</script>
</div>
<p>If <span><span class="MathJax_Preview">\vg_k \neq 0</span><script type="math/tex">\vg_k \neq 0</script></span>, i.e. <span><span class="MathJax_Preview">\vx^k</span><script type="math/tex">\vx^k</script></span> is not a stationary point, then <span><span class="MathJax_Preview">-\|\vg\|_2^2 &lt;0</span><script type="math/tex">-\|\vg\|_2^2 <0</script></span>. The negative gradient is also called the steepest descent direction of <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> at <span><span class="MathJax_Preview">\vx</span><script type="math/tex">\vx</script></span>. We say <span><span class="MathJax_Preview">\vd</span><script type="math/tex">\vd</script></span> is the steepest descent direction if it solves</p>
<div>
<div class="MathJax_Preview">\min\{f'(\vx;\vd) : \|\vd\| = 1\}.</div>
<script type="math/tex; mode=display">\min\{f'(\vx;\vd) : \|\vd\| = 1\}.</script>
</div>
<p>The gradent descent algorithm is:</p>
<hr />
<p><strong>Gradient descent</strong></p>
<p>Input: <span><span class="MathJax_Preview">\epsilon &gt;0</span><script type="math/tex">\epsilon >0</script></span> (tolerance), <span><span class="MathJax_Preview">\vx_0</span><script type="math/tex">\vx_0</script></span> (starting point)</p>
<p>For <span><span class="MathJax_Preview">k = 0,1,2,\dots</span><script type="math/tex">k = 0,1,2,\dots</script></span></p>
<ol>
<li>
<p>evaluate gradient <span><span class="MathJax_Preview">\vg^k = \nabla f(\vx^k)</span><script type="math/tex">\vg^k = \nabla f(\vx^k)</script></span></p>
</li>
<li>
<p>choose step length <span><span class="MathJax_Preview">\alpha^k</span><script type="math/tex">\alpha^k</script></span> based on reducing the function <span><span class="MathJax_Preview">\phi(\alpha) = f(\vx^k-\alpha\vg^k)</span><script type="math/tex">\phi(\alpha) = f(\vx^k-\alpha\vg^k)</script></span></p>
</li>
<li>
<p>set <span><span class="MathJax_Preview">\vx^{k+1} = \vx^k -\alpha^k \vg^k</span><script type="math/tex">\vx^{k+1} = \vx^k -\alpha^k \vg^k</script></span></p>
</li>
<li>
<p>stop if <span><span class="MathJax_Preview">\|\nabla f(\vx^{k+1})\|&lt;\epsilon</span><script type="math/tex">\|\nabla f(\vx^{k+1})\|<\epsilon</script></span>.</p>
</li>
</ol>
<hr />
<p>Below is a Julia implementation of gradient method with exact linesearch for minimizing quadratic functions of the form <span><span class="MathJax_Preview">f(\vx) = \frac{1}{2}\vx\trans\mA\vx + \vb\trans\vx</span><script type="math/tex">f(\vx) = \frac{1}{2}\vx\trans\mA\vx + \vb\trans\vx</script></span>. We use gradient method with exact linesearch to minimize <span><span class="MathJax_Preview">f(x,y) = x^2+2y^2</span><script type="math/tex">f(x,y) = x^2+2y^2</script></span> starting at <span><span class="MathJax_Preview">(x_0,y_0) = (2,1)</span><script type="math/tex">(x_0,y_0) = (2,1)</script></span>.</p>
<div class="highlight"><pre><span></span><code><span class="k">function</span> <span class="n">grad_method_exact_ls</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">ϵ</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
    <span class="n">∇f</span> <span class="o">=</span> <span class="n">A</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">xtrace</span> <span class="o">=</span> <span class="n">x</span><span class="o">&#39;</span>
    <span class="k">while</span> <span class="n">norm</span><span class="p">(</span><span class="n">∇f</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">ϵ</span>
        <span class="n">α</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">∇f</span><span class="p">,</span><span class="n">∇f</span><span class="p">)</span> <span class="o">/</span> <span class="n">dot</span><span class="p">(</span><span class="n">∇f</span><span class="p">,</span><span class="n">A</span><span class="o">*</span><span class="n">∇f</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">α</span><span class="o">*</span><span class="n">∇f</span>
        <span class="n">∇f</span> <span class="o">=</span> <span class="n">A</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">b</span>
        <span class="n">f</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="n">x</span><span class="o">&#39;*</span><span class="n">A</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">b</span><span class="o">&#39;*</span><span class="n">x</span>
       <span class="c"># @printf &quot;it = %3d | |∇f| = %8.2e | f = %8.2e\n&quot; k norm(∇f) f</span>
        <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">xtrace</span> <span class="o">=</span> <span class="n">vcat</span><span class="p">(</span><span class="n">xtrace</span><span class="p">,</span><span class="n">x</span><span class="o">&#39;</span><span class="p">)</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="n">xtrace</span>
<span class="k">end</span>

<span class="c">#Apply to f(x) = x^2+2y^2</span>
<span class="n">A</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="mi">0</span><span class="p">;</span><span class="mi">0</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">x0</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">xtrace</span> <span class="o">=</span> <span class="n">grad_method_exact_ls</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x0</span><span class="p">);</span>

<span class="c"># contour plot</span>
<span class="n">f</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">[</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">]</span><span class="o">&#39;*</span><span class="n">A</span><span class="o">*</span><span class="p">[</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">]</span><span class="o">+</span><span class="n">b</span><span class="o">&#39;*</span><span class="p">[</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">]</span>
<span class="n">x1</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">:</span><span class="mf">0.05</span><span class="o">:</span><span class="mi">2</span>
<span class="n">x2</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">:</span><span class="mf">0.05</span><span class="o">:</span><span class="mi">2</span><span class="p">;</span>
<span class="n">contour</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">levels</span> <span class="o">=</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">plot!</span><span class="p">(</span><span class="n">xtrace</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">xtrace</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="mi">3</span> <span class="p">,</span><span class="n">legend</span> <span class="o">=</span> <span class="kc">false</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s">&quot;Path of gradient method&quot;</span><span class="p">)</span>
</code></pre></div>

<p><img alt="" src="../figures/Gradient_Descent_2_1.png" /></p>
<h2 id="convergence-of-gradient-method-with-constant-stepsize"><strong>Convergence of gradient method with constant stepsize</strong><a class="headerlink" href="#convergence-of-gradient-method-with-constant-stepsize" title="Permanent link">&para;</a></h2>
<p>In gradient method with constant stepsize, we set <span><span class="MathJax_Preview">\alpha)k = \bar{\alpha}</span><script type="math/tex">\alpha)k = \bar{\alpha}</script></span> for all iterations. Naturally, the convergence and convergence rate of gradient method with constant stepsize is depends on the size of the constant <span><span class="MathJax_Preview">\bar{\alpha}</span><script type="math/tex">\bar{\alpha}</script></span>. If <span><span class="MathJax_Preview">\bar{\alpha}</span><script type="math/tex">\bar{\alpha}</script></span> is small then gradient method will likely converge (assuming the function is well-behaved), however the convergence will be slow. On the other hand, if the <span><span class="MathJax_Preview">\bar{\alpha}</span><script type="math/tex">\bar{\alpha}</script></span> is large, gradient method can diverge. So, we must choose a stepsize <span><span class="MathJax_Preview">\bar{\alpha} \in (0,\alpha_{\text{max}})</span><script type="math/tex">\bar{\alpha} \in (0,\alpha_{\text{max}})</script></span> for the method to converge. This <span><span class="MathJax_Preview">\alpha_{\text{max}}</span><script type="math/tex">\alpha_{\text{max}}</script></span> will depend on a property of <span><span class="MathJax_Preview">∇f</span><script type="math/tex">∇f</script></span> called Lipschitz continuity.</p>
<hr />
<p><strong><em>Definition</em></strong> (Lipschitz continuity of gradient) A continuously differentiable function <span><span class="MathJax_Preview">f:\R^n \rightarrow  \R</span><script type="math/tex">f:\R^n \rightarrow  \R</script></span> has Lipschitz continuous gradient with parameter <span><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span> if </p>
<div>
<div class="MathJax_Preview"> \|\nabla f(\vx) - \nabla f(\vy)\|_2 \leq L \|\vx- \vy\|_2</div>
<script type="math/tex; mode=display"> \|\nabla f(\vx) - \nabla f(\vy)\|_2 \leq L \|\vx- \vy\|_2</script>
</div>
<p>for all vectors <span><span class="MathJax_Preview">\vx</span><script type="math/tex">\vx</script></span>, <span><span class="MathJax_Preview">\vy</span><script type="math/tex">\vy</script></span> and some constant <span><span class="MathJax_Preview">L&gt;0</span><script type="math/tex">L>0</script></span>.</p>
<hr />
<p><strong><em>Example</em></strong> Let <span><span class="MathJax_Preview">\mA</span><script type="math/tex">\mA</script></span> be a PSD matrix. Let <span><span class="MathJax_Preview">f(\vx) = \frac{1}{2}\vx\trans\mA\vx+\vb\trans\vx+c</span><script type="math/tex">f(\vx) = \frac{1}{2}\vx\trans\mA\vx+\vb\trans\vx+c</script></span>. The gradient is <span><span class="MathJax_Preview">\nabla f(\vx) = \mA\vx-\vb</span><script type="math/tex">\nabla f(\vx) = \mA\vx-\vb</script></span> and </p>
<div>
<div class="MathJax_Preview">\begin{align*} \|\nabla f(\vx) -\nabla f(\vy)\| =&amp; \|(\mA\vx-\vb) - (\mA\vy-\vb)\|\\
=&amp; \|\mA\vx-\mA\vy\|\\
=&amp; \|\mA(\vx-\vy)\|\\
\leq &amp; \|\mA\|_2\|\vx-\vy\|_2\\
\leq &amp; \lambda_{\text{max}}(\mA) \|\vx-\vy\|_2.
\end{align*}</div>
<script type="math/tex; mode=display">\begin{align*} \|\nabla f(\vx) -\nabla f(\vy)\| =& \|(\mA\vx-\vb) - (\mA\vy-\vb)\|\\
=& \|\mA\vx-\mA\vy\|\\
=& \|\mA(\vx-\vy)\|\\
\leq & \|\mA\|_2\|\vx-\vy\|_2\\
\leq & \lambda_{\text{max}}(\mA) \|\vx-\vy\|_2.
\end{align*}</script>
</div>
<p>We now state a lemma that provides an upper bound on the stepsize for convergence of gradient method with constant stepsize.</p>
<hr />
<p><strong><em>Lemma</em></strong> If <span><span class="MathJax_Preview">f:\R^n\rightarrow \R</span><script type="math/tex">f:\R^n\rightarrow \R</script></span> has an L-Lipschitz continuous gradient and a minimizer exists, then the gradient method with constant stepsize converges if <span><span class="MathJax_Preview">\bar{\alpha} \in (0,2/L)</span><script type="math/tex">\bar{\alpha} \in (0,2/L)</script></span>.</p>
<hr />
<p>Using the above lemma, we can show that minimizing the quadratic <span><span class="MathJax_Preview">f(\vx) = \frac{1}{2}\vx\trans\mA\vx +\vb\trans\vx +c</span><script type="math/tex">f(\vx) = \frac{1}{2}\vx\trans\mA\vx +\vb\trans\vx +c</script></span> over <span><span class="MathJax_Preview">\R^n</span><script type="math/tex">\R^n</script></span> with <span><span class="MathJax_Preview">\mA = \bmat 1 &amp; 0 \\0 &amp; 2\emat</span><script type="math/tex">\mA = \bmat 1 & 0 \\0 & 2\emat</script></span> converges if the constant stepsize in the gradient method satisfies <span><span class="MathJax_Preview">\bar{\alpha} \in (0, 1)</span><script type="math/tex">\bar{\alpha} \in (0, 1)</script></span>. This is because <span><span class="MathJax_Preview">\lambda_{\text{max}}(\mA) = 2</span><script type="math/tex">\lambda_{\text{max}}(\mA) = 2</script></span>.</p>
<hr />
<p><strong><em>Lemma</em></strong> (Convergence of the gradient method) For the minimization of <span><span class="MathJax_Preview">f:\R^n\rightarrow\R</span><script type="math/tex">f:\R^n\rightarrow\R</script></span> bounded below with <span><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span>-Lipschitz gradient and one of the following linsearches</p>
<ol>
<li>
<p>constant stepsize <span><span class="MathJax_Preview">\bar{\alpha}\in(0,2?L),</span><script type="math/tex">\bar{\alpha}\in(0,2?L),</script></span></p>
</li>
<li>
<p>exact linesearch, or</p>
</li>
<li>
<p>backtracking linesearch with <span><span class="MathJax_Preview">\mu \in (-0,1)</span><script type="math/tex">\mu \in (-0,1)</script></span>,</p>
</li>
</ol>
<p>the gradient method satisfies <span><span class="MathJax_Preview">f(\vx_{k+1})&lt; f(\vx_k)</span><script type="math/tex">f(\vx_{k+1})< f(\vx_k)</script></span> for all <span><span class="MathJax_Preview">k =0,1,2,\dots</span><script type="math/tex">k =0,1,2,\dots</script></span> unless <span><span class="MathJax_Preview">\nabla f(\vx_k)  = 0</span><script type="math/tex">\nabla f(\vx_k)  = 0</script></span> and <span><span class="MathJax_Preview">\|\nabla f(\vx_k)\| \rightarrow 0</span><script type="math/tex">\|\nabla f(\vx_k)\| \rightarrow 0</script></span> as <span><span class="MathJax_Preview">k \rightarrow \infty</span><script type="math/tex">k \rightarrow \infty</script></span>.</p>
<hr />
<h2 id="condition-number-of-a-matrix"><strong>Condition number of a matrix</strong><a class="headerlink" href="#condition-number-of-a-matrix" title="Permanent link">&para;</a></h2>
<p>The condition number of a <span><span class="MathJax_Preview">n\times n</span><script type="math/tex">n\times n</script></span> positive definite matrix <span><span class="MathJax_Preview">\mA</span><script type="math/tex">\mA</script></span> is defined by </p>
<div>
<div class="MathJax_Preview">\kappa(\mA) = \frac{\lambdamax(\mA)}{\lambdamin(\mA)}.</div>
<script type="math/tex; mode=display">\kappa(\mA) = \frac{\lambdamax(\mA)}{\lambdamin(\mA)}.</script>
</div>
<p>An ill-conditioned matrix have large condition number and the condition number of the Hessian at the solution influences the speed at which the gradient method converges. Generally, if condition number of Hessian is small then gradient method converges quickly and, conversely, if condition number is large then gradient method converges slowly.</p>
<p>Consider the Rosenbrock function <span><span class="MathJax_Preview">f(x_1,x_2) = 100(x_1 - x_2^2)^2 + (1-x_1)^2</span><script type="math/tex">f(x_1,x_2) = 100(x_1 - x_2^2)^2 + (1-x_1)^2</script></span>. We can show that a stationary point of the Rosenbrock function is <span><span class="MathJax_Preview">(x_1, x_2) = (1,1)</span><script type="math/tex">(x_1, x_2) = (1,1)</script></span> and the Hessian of <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> at <span><span class="MathJax_Preview">(1,1)</span><script type="math/tex">(1,1)</script></span> is <span><span class="MathJax_Preview">\bmat 802 &amp; -400\\-400 &amp; 200\emat</span><script type="math/tex">\bmat 802 & -400\\-400 & 200\emat</script></span>. The Hessian has a large condition number and, as a result, any gradient method will have slow convergence. </p>
<div class="highlight"><pre><span></span><code><span class="c">#Gradient method with backtracking</span>
<span class="k">function</span> <span class="n">grad_method_backtracking</span><span class="p">(</span><span class="n">fObj</span><span class="p">,</span> <span class="n">gObj</span><span class="p">,</span> <span class="n">x0</span><span class="p">;</span> <span class="n">ϵ</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="n">μ</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="n">maxits</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">fObj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">∇f</span> <span class="o">=</span> <span class="n">gObj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">xtrace</span> <span class="o">=</span> <span class="n">x</span><span class="o">&#39;</span>
    <span class="k">while</span> <span class="n">norm</span><span class="p">(</span><span class="n">∇f</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">ϵ</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">maxits</span>
        <span class="n">α</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="k">while</span> <span class="p">((</span><span class="n">f</span> <span class="o">-</span> <span class="n">fObj</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">α</span><span class="o">*</span><span class="n">∇f</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">μ</span><span class="o">*</span><span class="n">α</span><span class="o">*</span><span class="n">dot</span><span class="p">(</span><span class="n">∇f</span><span class="p">,</span><span class="n">∇f</span><span class="p">)</span> <span class="p">)</span>
            <span class="n">α</span> <span class="o">/=</span><span class="mi">2</span>
        <span class="k">end</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">-</span><span class="n">α</span><span class="o">*</span><span class="n">∇f</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">fObj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">∇f</span> <span class="o">=</span> <span class="n">gObj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">xtrace</span> <span class="o">=</span> <span class="n">vcat</span><span class="p">(</span><span class="n">xtrace</span><span class="p">,</span><span class="n">x</span><span class="o">&#39;</span><span class="p">)</span>
    <span class="k">end</span>
    <span class="nd">@printf</span> <span class="s">&quot;it = </span><span class="si">%3d</span><span class="s"> | |∇f| = </span><span class="si">%8.2e</span><span class="s"> | f = </span><span class="si">%8.2e</span><span class="se">\n</span><span class="s">&quot;</span> <span class="n">k</span> <span class="n">norm</span><span class="p">(</span><span class="n">∇f</span><span class="p">)</span> <span class="n">f</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">xtrace</span>
<span class="k">end</span>

<span class="c">#Apply gradient method with backtracking to Rosenbrock function</span>
<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="mi">100</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">^</span><span class="mi">2</span>
<span class="n">∇f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">ForwardDiff</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
<span class="n">x0</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span>
<span class="n">x</span><span class="p">,</span> <span class="n">xtrace</span> <span class="o">=</span> <span class="n">grad_method_backtracking</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">∇f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">μ</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="n">maxits</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">);</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>it = 1000 | |∇f| = 1.56e+00 | f = 1.33e+00
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="c">#Contour plot</span>
<span class="n">f</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">)</span> <span class="o">=</span> <span class="mi">100</span><span class="p">(</span><span class="n">x2</span><span class="o">-</span><span class="n">x1</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x1</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>
<span class="n">x1</span> <span class="o">=</span> <span class="mi">0</span><span class="o">:</span><span class="mf">0.05</span><span class="o">:</span><span class="mi">3</span>
<span class="n">x2</span> <span class="o">=</span> <span class="mi">3</span><span class="o">:</span><span class="mf">0.05</span><span class="o">:</span><span class="mi">6</span>
<span class="n">contour</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">levels</span> <span class="o">=</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">plot!</span><span class="p">(</span><span class="n">xtrace</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">xtrace</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span><span class="n">marker</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span><span class="n">legend</span> <span class="o">=</span> <span class="kc">false</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s">&quot;Path of gradient method&quot;</span><span class="p">)</span>
</code></pre></div>

<p><img alt="" src="../figures/Gradient_Descent_3_1.png" /></p>
<p>In the above plot, we see that the gradient method converges slowly. We can increase the convergence rate by transforming the minimization problem so that Hessian is well-behaved at the minimizer.</p>
<h2 id="scaled-gradient-method"><strong>Scaled gradient method</strong><a class="headerlink" href="#scaled-gradient-method" title="Permanent link">&para;</a></h2>
<p>Scaled gradient method applies a linear change of variable so that the resulting problem is well-behaved. Consider the minimization of a function <span><span class="MathJax_Preview">f(\vx)</span><script type="math/tex">f(\vx)</script></span> where <span><span class="MathJax_Preview">f:\R^n \rightarrow \R</span><script type="math/tex">f:\R^n \rightarrow \R</script></span>. In scaled gradient method, we fix a non-singular <span><span class="MathJax_Preview">n\times n</span><script type="math/tex">n\times n</script></span> matrix <span><span class="MathJax_Preview">\mS</span><script type="math/tex">\mS</script></span> and minimize</p>
<div>
<div class="MathJax_Preview"> \minimize_{\vy\in\R^n} g(\vy),</div>
<script type="math/tex; mode=display"> \minimize_{\vy\in\R^n} g(\vy),</script>
</div>
<p>where <span><span class="MathJax_Preview">g(\vy) := f(\mS\vx)</span><script type="math/tex">g(\vy) := f(\mS\vx)</script></span>. That is, we make a linear change of variable with <span><span class="MathJax_Preview">\vx = \mS\vy</span><script type="math/tex">\vx = \mS\vy</script></span>. Applying gradient method to the scaled problem yields </p>
<div>
<div class="MathJax_Preview">\vy_{k+1} = \vy_k -\alpha_k \nabla g(\vy_k)</div>
<script type="math/tex; mode=display">\vy_{k+1} = \vy_k -\alpha_k \nabla g(\vy_k)</script>
</div>
<p>with <span><span class="MathJax_Preview">\nabla g(\vy) = \mS\trans \nabla f(\mS \vy)</span><script type="math/tex">\nabla g(\vy) = \mS\trans \nabla f(\mS \vy)</script></span>. Multiplying on the left by <span><span class="MathJax_Preview">\mS</span><script type="math/tex">\mS</script></span>, we get <span><span class="MathJax_Preview">\vx_{k+1} = \vx_k -\alpha_k \mS\mS\trans\nabla f(\vx_k)</span><script type="math/tex">\vx_{k+1} = \vx_k -\alpha_k \mS\mS\trans\nabla f(\vx_k)</script></span>. So, the scaled gradient method iterate, with <span><span class="MathJax_Preview">\mD:= \mS\mS\trans</span><script type="math/tex">\mD:= \mS\mS\trans</script></span> is</p>
<div>
<div class="MathJax_Preview">\vx_{k+1} = \vx_k -\alpha_k\mD\nabla f(\vx_k).</div>
<script type="math/tex; mode=display">\vx_{k+1} = \vx_k -\alpha_k\mD\nabla f(\vx_k).</script>
</div>
<p>It is easy to show that the scaled gradient <span><span class="MathJax_Preview">-D\nabla f(\vx)</span><script type="math/tex">-D\nabla f(\vx)</script></span> is a descent direction (check <span><span class="MathJax_Preview">f'(\vx;-\mD\nabla f(\vx)) &lt;0</span><script type="math/tex">f'(\vx;-\mD\nabla f(\vx)) <0</script></span>). The scaled gradient method is</p>
<hr />
<p><strong>Scaled Gradient method</strong></p>
<p>Input: <span><span class="MathJax_Preview">\epsilon &gt;0</span><script type="math/tex">\epsilon >0</script></span> (tolerance), <span><span class="MathJax_Preview">\vx_0</span><script type="math/tex">\vx_0</script></span> (starting point)</p>
<p>For <span><span class="MathJax_Preview">k = 0,1,2,\dots</span><script type="math/tex">k = 0,1,2,\dots</script></span></p>
<ol>
<li>
<p>Choose scaling matrix <span><span class="MathJax_Preview">\mD_k</span><script type="math/tex">\mD_k</script></span></p>
</li>
<li>
<p>evaluate scaled gradient <span><span class="MathJax_Preview">\vd_k = \mD_k \nabla f(\vx_k)</span><script type="math/tex">\vd_k = \mD_k \nabla f(\vx_k)</script></span></p>
</li>
<li>
<p>choose step length <span><span class="MathJax_Preview">\alpha_k</span><script type="math/tex">\alpha_k</script></span> based on reducing the function <span><span class="MathJax_Preview">\phi(\alpha) = f(\vx_k-\alpha\vd_k)</span><script type="math/tex">\phi(\alpha) = f(\vx_k-\alpha\vd_k)</script></span></p>
</li>
<li>
<p>set <span><span class="MathJax_Preview">\vx_{k+1} = \vx_k -\alpha_k \vg_k</span><script type="math/tex">\vx_{k+1} = \vx_k -\alpha_k \vg_k</script></span></p>
</li>
<li>
<p>stop if <span><span class="MathJax_Preview">\|\nabla f(\vx_{k+1})\|&lt;\epsilon</span><script type="math/tex">\|\nabla f(\vx_{k+1})\|<\epsilon</script></span>.</p>
</li>
</ol>
<hr />
<p>In the above algorithm, the choice of <span><span class="MathJax_Preview">\mD</span><script type="math/tex">\mD</script></span> should make <span><span class="MathJax_Preview">\nabla^2 g(\vy) = \mD^{\frac{1}{2}}\nabla^2 f(\mD^{\frac{1}{2}}\vy)\mD^{\frac{1}{2}} = \mD^{\frac{1}{2}}\nabla^2 f(\vx)\mD^{\frac{1}{2}}</span><script type="math/tex">\nabla^2 g(\vy) = \mD^{\frac{1}{2}}\nabla^2 f(\mD^{\frac{1}{2}}\vy)\mD^{\frac{1}{2}} = \mD^{\frac{1}{2}}\nabla^2 f(\vx)\mD^{\frac{1}{2}}</script></span> as well conditioned as possible. For example, in Newton method, we choose <span><span class="MathJax_Preview">\mD_k = (\nabla^2 f(\vx))^{-1}</span><script type="math/tex">\mD_k = (\nabla^2 f(\vx))^{-1}</script></span> which results in <span><span class="MathJax_Preview">\mD^{\frac{1}{2}}\nabla^2 f(\vx)\mD^{\frac{1}{2}} = \mI</span><script type="math/tex">\mD^{\frac{1}{2}}\nabla^2 f(\vx)\mD^{\frac{1}{2}} = \mI</script></span>, which has the condition number of 1. Similarly, in damped-Newton, we choose <span><span class="MathJax_Preview">\mD_k = (\nabla^2 f(\vx)+\lambda\mI)^{-1}</span><script type="math/tex">\mD_k = (\nabla^2 f(\vx)+\lambda\mI)^{-1}</script></span>, where condition number approaches 1 as <span><span class="MathJax_Preview">\lambda \rightarrow \infty</span><script type="math/tex">\lambda \rightarrow \infty</script></span></p>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../unconstrained/" title="Unconstrained optimization" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Unconstrained optimization
              </div>
            </div>
          </a>
        
        
          <a href="../Newtons_method/" title="Newton's method" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Newton's method
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2020 Michael Friedlander and Babhru Joshi
          </div>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/vendor.d710d30a.min.js"></script>
      <script src="../../assets/javascripts/bundle.a45f732b.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents"}</script>
      
      <script>
        app = initialize({
          base: "../..",
          features: [],
          search: Object.assign({
            worker: "../../assets/javascripts/worker/search.c03f0417.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
        <script src="../../javascripts/extra.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>