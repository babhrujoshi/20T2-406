{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Computational Optimization (CSPC 406 2020 Term 2) \u00b6 This course covers the main algorithms for continuous optimization, including unconstrained and constrained problems, large-scale problems, and duality theory and sensitivity. We will also cover numerical linear algebra operations needed in optimization, including LU, QR, and Cholesky decompositions. Discrete optimization problems are not covered. Lectures \u00b6 Mondays, Wednesdays, and Fridays, 2-3 pm, Location: Online Teaching staff (2020 Term 2) \u00b6 Instructor: Babhru Joshi Email: b.joshi@math.ubc.ca Office hours: Mondays 3-4 pm or by appointment Location: Online Teaching Assistants: Naomi Graham, Emma Hansen, Yibo Jiao, and Weiwei Sun Office hours: Wednesdays 1-2 pm and Fridays 10-11 am, Location: Online Textbook \u00b6 Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MATLAB , Amir Beck (SIAM, 2014). This book is available online through the UBC Library. Course requirements \u00b6 One of CPSC 302, CPSC 303, or MATH 307. Online Etiquette \u00b6 During class, students will be fully engaged with the class, to the best of their ability. Students are encouraged to leave their video connection on for the majority of the class, especially during breakout-room sessions. Students should willingly volunteer to answer questions and should be prepared to be called upon to answer questions if needed. Please mute your mic when you are not speaking. Course discussion board \u00b6 Discussion board is hosted on Piazza. Students can signup using the following link .","title":"Home Page"},{"location":"#computational-optimization-cspc-406-2020-term-2","text":"This course covers the main algorithms for continuous optimization, including unconstrained and constrained problems, large-scale problems, and duality theory and sensitivity. We will also cover numerical linear algebra operations needed in optimization, including LU, QR, and Cholesky decompositions. Discrete optimization problems are not covered.","title":"Computational Optimization (CSPC 406 2020 Term 2)"},{"location":"#lectures","text":"Mondays, Wednesdays, and Fridays, 2-3 pm, Location: Online","title":"Lectures"},{"location":"#teaching-staff-2020-term-2","text":"Instructor: Babhru Joshi Email: b.joshi@math.ubc.ca Office hours: Mondays 3-4 pm or by appointment Location: Online Teaching Assistants: Naomi Graham, Emma Hansen, Yibo Jiao, and Weiwei Sun Office hours: Wednesdays 1-2 pm and Fridays 10-11 am, Location: Online","title":"Teaching staff (2020 Term 2)"},{"location":"#textbook","text":"Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MATLAB , Amir Beck (SIAM, 2014). This book is available online through the UBC Library.","title":"Textbook"},{"location":"#course-requirements","text":"One of CPSC 302, CPSC 303, or MATH 307.","title":"Course requirements"},{"location":"#online-etiquette","text":"During class, students will be fully engaged with the class, to the best of their ability. Students are encouraged to leave their video connection on for the majority of the class, especially during breakout-room sessions. Students should willingly volunteer to answer questions and should be prepared to be called upon to answer questions if needed. Please mute your mic when you are not speaking.","title":"Online Etiquette"},{"location":"#course-discussion-board","text":"Discussion board is hosted on Piazza. Students can signup using the following link .","title":"Course discussion board"},{"location":"grades/","text":"Grades and policies \u00b6 Grade distribution \u00b6 The grading scheme that gives the highest individual grade will be used to determine your course grade. Scheme 1 \u00b6 participation : 10% assignments (6): 30% midterm exam: 30% final exam: 30% Scheme 2 \u00b6 assignments (6): 30% midterm exam: 30% final exam: 40% Collaboration \u00b6 Most homeworks involve programming tasks. You may collaborate and consult with other students in the course, but you must hand in your own assignments and your own code. If you have collaborated or consulted with someone while working on your assignment, you must acknowledge this explicitly in your submitted assignment. If you are unsure about any of these rules, feel free to consult with your instructor or visit the departmental webpage on Collaboration and Plagiarism . Late submissions \u00b6 Each student has a three lecture-day allowance to use throughout the term. For instance, if an assignment is due by Wednesday, then submission by Friday counts as one delay; a submission by Monday counts as two delays; a submission by the following Wednesday counts as three delays (and consumes the entire three-day allowance). Apart from using delays, late submissions are not accepted. Once a solution set has been posted, no more late submissions are permitted; consequently, you may not always be able to use all of your delays. Policies \u00b6 No makeup exam for the midterm or final. If you missed the midterm exam you must document a justification. Midterm exam grade will not be counted if it is lower than your final exam grade. To pass the course you must do the assigned coursework, write the midterm and final exams, pass the final exam, and obtain an overall pass average according to the grading scheme. The instructors reserve the right to modify the grading scheme at any time.","title":"Grades"},{"location":"grades/#grades-and-policies","text":"","title":"Grades and policies"},{"location":"grades/#grade-distribution","text":"The grading scheme that gives the highest individual grade will be used to determine your course grade.","title":"Grade distribution"},{"location":"grades/#scheme-1","text":"participation : 10% assignments (6): 30% midterm exam: 30% final exam: 30%","title":"Scheme 1"},{"location":"grades/#scheme-2","text":"assignments (6): 30% midterm exam: 30% final exam: 40%","title":"Scheme 2"},{"location":"grades/#collaboration","text":"Most homeworks involve programming tasks. You may collaborate and consult with other students in the course, but you must hand in your own assignments and your own code. If you have collaborated or consulted with someone while working on your assignment, you must acknowledge this explicitly in your submitted assignment. If you are unsure about any of these rules, feel free to consult with your instructor or visit the departmental webpage on Collaboration and Plagiarism .","title":"Collaboration"},{"location":"grades/#late-submissions","text":"Each student has a three lecture-day allowance to use throughout the term. For instance, if an assignment is due by Wednesday, then submission by Friday counts as one delay; a submission by Monday counts as two delays; a submission by the following Wednesday counts as three delays (and consumes the entire three-day allowance). Apart from using delays, late submissions are not accepted. Once a solution set has been posted, no more late submissions are permitted; consequently, you may not always be able to use all of your delays.","title":"Late submissions"},{"location":"grades/#policies","text":"No makeup exam for the midterm or final. If you missed the midterm exam you must document a justification. Midterm exam grade will not be counted if it is lower than your final exam grade. To pass the course you must do the assigned coursework, write the midterm and final exams, pass the final exam, and obtain an overall pass average according to the grading scheme. The instructors reserve the right to modify the grading scheme at any time.","title":"Policies"},{"location":"schedule/","text":"Course schedule \u00b6 This is a tentative schedule. It will change often. Lecture Date / Day Topic Notes Homework 1 11 Jan / Mon Introduction 2 13 Jan / Wed Least squares Slides , Notebook 3 15 Jan / Fri QR factorization Slides hw 1 out 4 18 Jan / Mon Regularized least squares Slides , Notebook 5 20 Jan / Wed Non-linear least squares 6 22 Jan / Fri Unconstrained optimization hw 1 due at 11:59 pm PST 7 25 Jan / Mon Unconstrained optimization 8 27 Jan / Wed Quadratic functions 9 29 Jan / Fri Descent methods 10 1 Feb / Mon Descent methods 11 3 Feb / Wed Descent methods 12 5 Feb / Fri Newton's Method 13 8 Feb / Mon Cholesky Factorization 14 10 Feb / Wed Cholesky Factorization 15 12 Feb / Fri Case study 15 Feb / Mon no lecture (Family day) 17 Feb / Wed no lecture (Reading break) 19 Feb / Fri no lecture (Reading break) 16 22 Feb / Mon Midterm 17 24 Feb / Wed Quasi-Newton Methods 18 26 Feb / Fri Linear constraint 19 1 Mar / Mon Linear constraint 20 3 Mar / Wed Convex sets 21 5 Mar / Fri Convex sets 22 8 Mar / Mon Constrained optimization 23 10 Mar / Wed Constrained optimization 24 12 Mar / Fri Constrained optimization 25 15 Mar / Mon Convex functions 26 17 Mar / Wed Convex functions 27 19 Mar / Fri Linear programming applications 28 22 Mar / Mon Extreme points 29 24 Mar / Wed Extreme points 30 26 Mar / Fri Standard form 31 29 Mar / Mon Standard form 32 31 Mar / Wed Simplex method 33 2 Apr / Fri Simplex method 34 5 Apr / Mon Simplex method 35 7 Apr / Wed Duality 36 9 Apr / Fri Duality 37 12 Apr / Mon Duality 38 14 Apr / Wed Interior point methods","title":"Schedule"},{"location":"schedule/#course-schedule","text":"This is a tentative schedule. It will change often. Lecture Date / Day Topic Notes Homework 1 11 Jan / Mon Introduction 2 13 Jan / Wed Least squares Slides , Notebook 3 15 Jan / Fri QR factorization Slides hw 1 out 4 18 Jan / Mon Regularized least squares Slides , Notebook 5 20 Jan / Wed Non-linear least squares 6 22 Jan / Fri Unconstrained optimization hw 1 due at 11:59 pm PST 7 25 Jan / Mon Unconstrained optimization 8 27 Jan / Wed Quadratic functions 9 29 Jan / Fri Descent methods 10 1 Feb / Mon Descent methods 11 3 Feb / Wed Descent methods 12 5 Feb / Fri Newton's Method 13 8 Feb / Mon Cholesky Factorization 14 10 Feb / Wed Cholesky Factorization 15 12 Feb / Fri Case study 15 Feb / Mon no lecture (Family day) 17 Feb / Wed no lecture (Reading break) 19 Feb / Fri no lecture (Reading break) 16 22 Feb / Mon Midterm 17 24 Feb / Wed Quasi-Newton Methods 18 26 Feb / Fri Linear constraint 19 1 Mar / Mon Linear constraint 20 3 Mar / Wed Convex sets 21 5 Mar / Fri Convex sets 22 8 Mar / Mon Constrained optimization 23 10 Mar / Wed Constrained optimization 24 12 Mar / Fri Constrained optimization 25 15 Mar / Mon Convex functions 26 17 Mar / Wed Convex functions 27 19 Mar / Fri Linear programming applications 28 22 Mar / Mon Extreme points 29 24 Mar / Wed Extreme points 30 26 Mar / Fri Standard form 31 29 Mar / Mon Standard form 32 31 Mar / Wed Simplex method 33 2 Apr / Fri Simplex method 34 5 Apr / Mon Simplex method 35 7 Apr / Wed Duality 36 9 Apr / Fri Duality 37 12 Apr / Mon Duality 38 14 Apr / Wed Interior point methods","title":"Course schedule"},{"location":"homework/","text":"Homework submissions \u00b6 Here are some guidelines for homework submissions: Solutions must be submitted electronically, using the Canvas system. Solutions should be typeset. No handwritten solutions! You have several options available to you, include PDF files prepared using LaTeX and Jupyter notebooks. Jupyter notebook submissions are especially encouraged because these are a convenient way of packaging typeset solutions and corresponding results and code. If you choose to use a Jupyter notebook, please submit a PDF and the ipynb file. Adhere to the policy on collaboration and late submissions .","title":"Submissions"},{"location":"homework/#homework-submissions","text":"Here are some guidelines for homework submissions: Solutions must be submitted electronically, using the Canvas system. Solutions should be typeset. No handwritten solutions! You have several options available to you, include PDF files prepared using LaTeX and Jupyter notebooks. Jupyter notebook submissions are especially encouraged because these are a convenient way of packaging typeset solutions and corresponding results and code. If you choose to use a Jupyter notebook, please submit a PDF and the ipynb file. Adhere to the policy on collaboration and late submissions .","title":"Homework submissions"},{"location":"homework/hw1/hw1/","text":"CPSC 406: Homework 1 (Due Jan 22, 11:59 pm PST) \u00b6 Submission Instructions: The solution will be submitted electronically, using the Canvas system. Solution should be typeset using LaTeX (handwritten solution will not be accepted). Using jupyter notebook to typeset the solution, which supports Julia and Python, is recommended. If you are using jupyter notebook, submit both the ipynb file and its pdf output. If you are not using juypter notebook, the pdf file must contain the codes relavant to the homework. In both cases, results of the code should be in the pdf file. See here for policy on collaboration and late submissions. Backsolve Here, we will explore the computational complexity of solving the system \\mR\\vx = \\vb, \\qquad \\mR\\in \\R^{n\\times n} \\mR\\vx = \\vb, \\qquad \\mR\\in \\R^{n\\times n} when \\mR \\mR is either upper triangular ( R_{ij} = 0 R_{ij} = 0 whenever i > j i > j ) or lower triangular ( R_{ij} = 0 R_{ij} = 0 whenever i < j i < j ). If \\mR \\mR were fully dense, then solving this system takes O(n^3) O(n^3) flops. We will show that when \\mR \\mR is upper or lower triangular, this system takes O(n^2) O(n^2) flops. Assume that the diagonal elements |R_{ii}| > \\epsilon |R_{ii}| > \\epsilon for \\epsilon \\epsilon suitably large in all cases. Consider \\mR \\mR lower triangular, e.g. we solve the system \\bmat R_{11} & 0 & \\cdots & 0 \\\\ R_{21} & R_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ R_{n,1} & R_{n,2} & \\cdots & R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. \\bmat R_{11} & 0 & \\cdots & 0 \\\\ R_{21} & R_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ R_{n,1} & R_{n,2} & \\cdots & R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. Show how to find x_1 x_1 . (This should take O(1) O(1) flops.) Given x_1,...,x_i x_1,...,x_i , show how to find x_{i+1} x_{i+1} . (This should take O(i) O(i) flops.) Putting it all together, we get O(1) + O(2) + \\cdots + O(n-1) + O(n) = O(n^2) \\text{ flops.} O(1) + O(2) + \\cdots + O(n-1) + O(n) = O(n^2) \\text{ flops.} Now consider \\mR \\mR upper triangular, e.g. we solve the system \\bmat R_{11} & \\cdots & R_{1,n-1} &R_{1,n} \\\\ 0 & \\cdots & R_{2,n-1} &R_{2,n} \\\\ \\vdots & \\ddots & \\vdots &\\vdots \\\\ 0 & \\cdots & 0 &R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. \\bmat R_{11} & \\cdots & R_{1,n-1} &R_{1,n} \\\\ 0 & \\cdots & R_{2,n-1} &R_{2,n} \\\\ \\vdots & \\ddots & \\vdots &\\vdots \\\\ 0 & \\cdots & 0 &R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. Show how to find x_n x_n . (This should take O(1) O(1) flops.) Given x_{i+1},...,x_n x_{i+1},...,x_n , show how to find x_{i} x_{i} . (This should take O(n-i) O(n-i) flops.) Putting it all together, we get O(n) + O(n-1) + \\cdots + O(2) + O(1) = O(n^2) \\text{ flops.} O(n) + O(n-1) + \\cdots + O(2) + O(1) = O(n^2) \\text{ flops.} Linear data fit Download data (Here is the csv format of the same data ). Fit the best line f(z) = x_1 + x_2z f(z) = x_1 + x_2z to the points (z_1,y_1),...,(z_n,y_n) (z_1,y_1),...,(z_n,y_n) ; that is, find the best approximation of the line f(z) f(z) to y y in the 2-norm sense. Plot the fit, and report \\|\\vr\\|_2 \\|\\vr\\|_2 the norm of the fit residual. Polynomial data fit Using the same data as above, fit the best order- d d polynomial to the points (z_1,y_1),...,(z_n,y_n) (z_1,y_1),...,(z_n,y_n) , for d = 2,3,4,5 d = 2,3,4,5 . That is, find x_1,...,x_{d+1} x_1,...,x_{d+1} such that f(z) = x_1 + x_2z + x_3z^2 + \\cdots + x_{d+1}z^d f(z) = x_1 + x_2z + x_3z^2 + \\cdots + x_{d+1}z^d best approximates the data in the 2-norm sense (minimizing \\sum_i (f(z_i)-y_i)^2 \\sum_i (f(z_i)-y_i)^2 ). Plot the fit, and report \\|\\vr\\|_2 \\|\\vr\\|_2 the norm of the fit residual. About how many degrees is needed for a reasonable fit? QR factorization Consider a full rank, underdetermined but consistent linear system \\mA\\vx = \\vb \\mA\\vx = \\vb , where \\mA \\mA is m\\times n m\\times n with m < n m < n . Show how to use the QR factorization to obtain a soution of this system. The following script can be used to generate random matrices in Julia, given dimensions m=10 m=10 and n=20 n=20 : m = 10 n = 20 A = randn ( m , n ) x = randn ( n ) b = A * x Write a Julia code for solving for x x using the procedure outlined in the previous part of the question. Record the runtime using the Julia calls \\texttt{time()} \\texttt{time()} . (Make sure you are not running anything else or it will interfere with the timing results.) Record the runtimes for matrices of sizes (m,n) = (10,20) (m,n) = (10,20) , (100,200) (100,200) , (100,2000) (100,2000) , (100,20000) (100,20000) , and (100,200000) (100,200000) . Compare the runtimes against finding x x using \\vx = \\mA\\backslash \\vb \\vx = \\mA\\backslash \\vb . The underdetermined and consistent linear system \\mA\\vx=\\vb \\mA\\vx=\\vb has infinitely many solutions. For the case where \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} is full rank, show how to use the QR factorization to obtain the least norm solution, i.e. find \\vx_{LN} \\vx_{LN} that solves \\min_{\\vx\\in\\R^n} \\|\\vx\\|_2^2 \\quad \\text{ subject to } \\quad \\mA\\vx = \\vb \\min_{\\vx\\in\\R^n} \\|\\vx\\|_2^2 \\quad \\text{ subject to } \\quad \\mA\\vx = \\vb using QR decomposition.","title":"Homework 1"},{"location":"homework/hw1/hw1/#cpsc-406-homework-1-due-jan-22-1159-pm-pst","text":"Submission Instructions: The solution will be submitted electronically, using the Canvas system. Solution should be typeset using LaTeX (handwritten solution will not be accepted). Using jupyter notebook to typeset the solution, which supports Julia and Python, is recommended. If you are using jupyter notebook, submit both the ipynb file and its pdf output. If you are not using juypter notebook, the pdf file must contain the codes relavant to the homework. In both cases, results of the code should be in the pdf file. See here for policy on collaboration and late submissions. Backsolve Here, we will explore the computational complexity of solving the system \\mR\\vx = \\vb, \\qquad \\mR\\in \\R^{n\\times n} \\mR\\vx = \\vb, \\qquad \\mR\\in \\R^{n\\times n} when \\mR \\mR is either upper triangular ( R_{ij} = 0 R_{ij} = 0 whenever i > j i > j ) or lower triangular ( R_{ij} = 0 R_{ij} = 0 whenever i < j i < j ). If \\mR \\mR were fully dense, then solving this system takes O(n^3) O(n^3) flops. We will show that when \\mR \\mR is upper or lower triangular, this system takes O(n^2) O(n^2) flops. Assume that the diagonal elements |R_{ii}| > \\epsilon |R_{ii}| > \\epsilon for \\epsilon \\epsilon suitably large in all cases. Consider \\mR \\mR lower triangular, e.g. we solve the system \\bmat R_{11} & 0 & \\cdots & 0 \\\\ R_{21} & R_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ R_{n,1} & R_{n,2} & \\cdots & R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. \\bmat R_{11} & 0 & \\cdots & 0 \\\\ R_{21} & R_{22} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ R_{n,1} & R_{n,2} & \\cdots & R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. Show how to find x_1 x_1 . (This should take O(1) O(1) flops.) Given x_1,...,x_i x_1,...,x_i , show how to find x_{i+1} x_{i+1} . (This should take O(i) O(i) flops.) Putting it all together, we get O(1) + O(2) + \\cdots + O(n-1) + O(n) = O(n^2) \\text{ flops.} O(1) + O(2) + \\cdots + O(n-1) + O(n) = O(n^2) \\text{ flops.} Now consider \\mR \\mR upper triangular, e.g. we solve the system \\bmat R_{11} & \\cdots & R_{1,n-1} &R_{1,n} \\\\ 0 & \\cdots & R_{2,n-1} &R_{2,n} \\\\ \\vdots & \\ddots & \\vdots &\\vdots \\\\ 0 & \\cdots & 0 &R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. \\bmat R_{11} & \\cdots & R_{1,n-1} &R_{1,n} \\\\ 0 & \\cdots & R_{2,n-1} &R_{2,n} \\\\ \\vdots & \\ddots & \\vdots &\\vdots \\\\ 0 & \\cdots & 0 &R_{n,n}\\\\ \\emat \\bmat x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\emat = \\bmat b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\emat. Show how to find x_n x_n . (This should take O(1) O(1) flops.) Given x_{i+1},...,x_n x_{i+1},...,x_n , show how to find x_{i} x_{i} . (This should take O(n-i) O(n-i) flops.) Putting it all together, we get O(n) + O(n-1) + \\cdots + O(2) + O(1) = O(n^2) \\text{ flops.} O(n) + O(n-1) + \\cdots + O(2) + O(1) = O(n^2) \\text{ flops.} Linear data fit Download data (Here is the csv format of the same data ). Fit the best line f(z) = x_1 + x_2z f(z) = x_1 + x_2z to the points (z_1,y_1),...,(z_n,y_n) (z_1,y_1),...,(z_n,y_n) ; that is, find the best approximation of the line f(z) f(z) to y y in the 2-norm sense. Plot the fit, and report \\|\\vr\\|_2 \\|\\vr\\|_2 the norm of the fit residual. Polynomial data fit Using the same data as above, fit the best order- d d polynomial to the points (z_1,y_1),...,(z_n,y_n) (z_1,y_1),...,(z_n,y_n) , for d = 2,3,4,5 d = 2,3,4,5 . That is, find x_1,...,x_{d+1} x_1,...,x_{d+1} such that f(z) = x_1 + x_2z + x_3z^2 + \\cdots + x_{d+1}z^d f(z) = x_1 + x_2z + x_3z^2 + \\cdots + x_{d+1}z^d best approximates the data in the 2-norm sense (minimizing \\sum_i (f(z_i)-y_i)^2 \\sum_i (f(z_i)-y_i)^2 ). Plot the fit, and report \\|\\vr\\|_2 \\|\\vr\\|_2 the norm of the fit residual. About how many degrees is needed for a reasonable fit? QR factorization Consider a full rank, underdetermined but consistent linear system \\mA\\vx = \\vb \\mA\\vx = \\vb , where \\mA \\mA is m\\times n m\\times n with m < n m < n . Show how to use the QR factorization to obtain a soution of this system. The following script can be used to generate random matrices in Julia, given dimensions m=10 m=10 and n=20 n=20 : m = 10 n = 20 A = randn ( m , n ) x = randn ( n ) b = A * x Write a Julia code for solving for x x using the procedure outlined in the previous part of the question. Record the runtime using the Julia calls \\texttt{time()} \\texttt{time()} . (Make sure you are not running anything else or it will interfere with the timing results.) Record the runtimes for matrices of sizes (m,n) = (10,20) (m,n) = (10,20) , (100,200) (100,200) , (100,2000) (100,2000) , (100,20000) (100,20000) , and (100,200000) (100,200000) . Compare the runtimes against finding x x using \\vx = \\mA\\backslash \\vb \\vx = \\mA\\backslash \\vb . The underdetermined and consistent linear system \\mA\\vx=\\vb \\mA\\vx=\\vb has infinitely many solutions. For the case where \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} is full rank, show how to use the QR factorization to obtain the least norm solution, i.e. find \\vx_{LN} \\vx_{LN} that solves \\min_{\\vx\\in\\R^n} \\|\\vx\\|_2^2 \\quad \\text{ subject to } \\quad \\mA\\vx = \\vb \\min_{\\vx\\in\\R^n} \\|\\vx\\|_2^2 \\quad \\text{ subject to } \\quad \\mA\\vx = \\vb using QR decomposition.","title":"CPSC 406: Homework 1 (Due Jan  22, 11:59 pm PST)"},{"location":"notes/Cholesky_factorization/","text":"Cholesky factorization \u00b6 In this lecture, we will consider Cholesky factorization of positive definite matrices. A n\\times n n\\times n symmetric matrix \\mA \\mA is positive definite if \\vx\\trans\\mA\\vx >0 \\vx\\trans\\mA\\vx >0 for all \\vx \\neq \\vzero \\vx \\neq \\vzero . Additionally, every principal submatrix of a positive definite matrix is also postive matrix. This is because for any n\\times k n\\times k full-rank matrix \\mX \\mX , the matrix \\mX\\trans\\mA\\mX \\mX\\trans\\mA\\mX is positive definite.","title":"**Cholesky factorization**"},{"location":"notes/Cholesky_factorization/#cholesky-factorization","text":"In this lecture, we will consider Cholesky factorization of positive definite matrices. A n\\times n n\\times n symmetric matrix \\mA \\mA is positive definite if \\vx\\trans\\mA\\vx >0 \\vx\\trans\\mA\\vx >0 for all \\vx \\neq \\vzero \\vx \\neq \\vzero . Additionally, every principal submatrix of a positive definite matrix is also postive matrix. This is because for any n\\times k n\\times k full-rank matrix \\mX \\mX , the matrix \\mX\\trans\\mA\\mX \\mX\\trans\\mA\\mX is positive definite.","title":"Cholesky factorization"},{"location":"notes/Constrained_optimization/","text":"Constrained Optimization \u00b6 Let f:\\R^n \\rightarrow \\R f:\\R^n \\rightarrow \\R be a differentiable function and \\setS \\setS be a convex set. In this lecture, we will look at first-order necessary conditions of the minimization program \\begin{equation}\\label{constrained_prob}\\min_{\\vx\\in\\R^n} f(\\vx) \\text{ s.t. } \\vx \\in \\setS.\\end{equation} \\begin{equation}\\label{constrained_prob}\\min_{\\vx\\in\\R^n} f(\\vx) \\text{ s.t. } \\vx \\in \\setS.\\end{equation} In the case where \\setS = \\R^n \\setS = \\R^n , the first order necessary condition for \\vx^* \\vx^* to be a minimizer is \\nabla f(\\vx^*) = 0 \\nabla f(\\vx^*) = 0 . More generally, for arbitrary convex set \\setS \\setS , \\begin{equation}\\label{first_order}\\text{if }\\vx^* \\text{ is a minimizer then it satisfies } \\nabla f(\\vx^*)\\trans (\\vx - \\vx^*) \\geq 0 \\text{ for all } \\vx \\in \\setS.\\end{equation} \\begin{equation}\\label{first_order}\\text{if }\\vx^* \\text{ is a minimizer then it satisfies } \\nabla f(\\vx^*)\\trans (\\vx - \\vx^*) \\geq 0 \\text{ for all } \\vx \\in \\setS.\\end{equation} We can express the above optimality condition in terms of normal cones as well. The normal cone of a set \\setS \\setS at a point \\vx \\vx is defined as \\setN_\\setS(\\vx) = \\{\\vg\\in\\R^n | \\vg\\trans(\\vz-\\vx)\\leq 0,\\ \\forall z \\in \\setS\\}. \\setN_\\setS(\\vx) = \\{\\vg\\in\\R^n | \\vg\\trans(\\vz-\\vx)\\leq 0,\\ \\forall z \\in \\setS\\}. The first order necessary condition in terms of normal cone is: \\text{if } \\vx^* \\text{is a minimizer of \\eqref{constrained_prob}, then -} \\nabla f(\\vx^*) \\in \\setN_\\setS(\\vx^*). \\text{if } \\vx^* \\text{is a minimizer of \\eqref{constrained_prob}, then -} \\nabla f(\\vx^*) \\in \\setN_\\setS(\\vx^*). Figure below shows the normal cone of a set \\setS \\setS at different points in the set. Examples \u00b6 Example 1 (Normal cone of set \\setS \\setS at an interior point) We say that \\vx \\in \\R^n \\vx \\in \\R^n is in the interior of a set \\setS \\setS (denoted \\text{int}\\setS \\text{int}\\setS ) if there exist \\epsilon >0 \\epsilon >0 such that $\\vx+\\epsilon \\vv \\in \\setS$ for all \\vv \\in \\R^n. \\vv \\in \\R^n. \\text{For any point } \\vx \\in \\text{int} \\setS, \\text{ we have } \\setN_\\setS(\\vx) = \\{0\\}. \\text{For any point } \\vx \\in \\text{int} \\setS, \\text{ we have } \\setN_\\setS(\\vx) = \\{0\\}. To see this, note that a vector \\vg \\vg in \\setN_\\setS(\\vx) \\setN_\\setS(\\vx) makes a non-positive inner product with every displacement from \\vx \\vx that is contained in \\setS \\setS . However, since \\vx \\vx is in the interior, a valid displacement is \\epsilon \\vg \\epsilon \\vg for some \\epsilon >0 \\epsilon >0 . Example 2 (Normal cone of affine set) Consider an affine set \\setS = \\{\\vx | \\mA\\vx=\\vb\\} \\setS = \\{\\vx | \\mA\\vx=\\vb\\} . By definition of normal cone, we have \\setN_\\setS(\\vx) = \\{\\vg\\in\\R^n | \\vg\\trans(\\vu-\\vx)\\leq 0,\\ \\forall\\ \\vu\\in\\setS\\}. \\setN_\\setS(\\vx) = \\{\\vg\\in\\R^n | \\vg\\trans(\\vu-\\vx)\\leq 0,\\ \\forall\\ \\vu\\in\\setS\\}. In the above definition, note that both \\vu \\vu and \\vx \\vx are in the set \\setS \\setS . Thus, \\vu -\\vv \\in \\vnull(\\mA) \\vu -\\vv \\in \\vnull(\\mA) . So, \\setN_\\setS(\\vx) = \\{\\vg\\in\\R^n | \\vg\\trans\\vu\\leq 0,\\ \\forall\\ \\vu\\in\\vnull(\\mA)\\}. \\setN_\\setS(\\vx) = \\{\\vg\\in\\R^n | \\vg\\trans\\vu\\leq 0,\\ \\forall\\ \\vu\\in\\vnull(\\mA)\\}. Note that if \\vu \\in \\vnull(\\mA) \\vu \\in \\vnull(\\mA) and \\vg\\trans\\vu<0 \\vg\\trans\\vu<0 , then -\\vu \\in \\vnull(\\mA) -\\vu \\in \\vnull(\\mA) and \\vg\\trans(-\\vu)>0 \\vg\\trans(-\\vu)>0 . Therefore, it must be that \\setN_\\setS(\\vx) = \\{\\vg\\in\\R^n | \\vg\\trans\\vu= 0,\\ \\forall\\ \\vu\\in\\vnull(\\mA)\\} = \\range(\\mA\\trans). \\setN_\\setS(\\vx) = \\{\\vg\\in\\R^n | \\vg\\trans\\vu= 0,\\ \\forall\\ \\vu\\in\\vnull(\\mA)\\} = \\range(\\mA\\trans). Example 3 (Normal cone to affine half-space) Consider the set \\setS = \\{\\vx\\ |\\ \\mA\\vx\\leq\\vb \\ \\} \\setS = \\{\\vx\\ |\\ \\mA\\vx\\leq\\vb \\ \\} . Fix \\vx \\in \\setS \\vx \\in \\setS . Then, we can separate inequalities that are active vs those that are not. Let \\setB \\setB be the index set such that \\va_i\\trans\\vx = b_i \\va_i\\trans\\vx = b_i for all i\\in\\setB i\\in\\setB . Similarly, let \\setN \\setN be the index set such that \\va_i\\trans\\vx < b_i \\va_i\\trans\\vx < b_i for all i\\in\\setN i\\in\\setN . The normal cone of \\setS \\setS at a point \\vx \\vx is \\setN_\\setS(\\vx) = \\{\\mA\\trans\\vz | z_i = 0\\ \\forall\\ i\\in\\setN,\\ z_i\\geq0\\ \\forall\\ i\\in\\setB\\}. \\setN_\\setS(\\vx) = \\{\\mA\\trans\\vz | z_i = 0\\ \\forall\\ i\\in\\setN,\\ z_i\\geq0\\ \\forall\\ i\\in\\setB\\}. Use examples 1 and 2 to show this. Projected gradient method \u00b6 The goal in this section is to describe a gradient descent based method to solve constrained optimization programs of the form \\eqref{constrained_prob}. The gradient descent iterate at a point \\tilde{\\vx}_k \\tilde{\\vx}_k is \\vx_{k+1} = \\vx - \\alpha \\nabla f(\\vx_k) \\vx_{k+1} = \\vx - \\alpha \\nabla f(\\vx_k) . However, the iterate \\tilde{\\vx}_{k+1} \\tilde{\\vx}_{k+1} may not belong in the constraint set \\setS \\setS . In projected gradient descent, we simply choose the point in the set closest to \\tilde{x}_{k+1} \\tilde{x}_{k+1} as the next iterate \\vx_{k+1} \\vx_{k+1} of the descent algorithm, i.e. \\vx_{k+1} = \\argmin_{\\vx \\in \\setS} \\|\\vx - \\tilde{\\vx}_{k+1}\\|_2^2. \\vx_{k+1} = \\argmin_{\\vx \\in \\setS} \\|\\vx - \\tilde{\\vx}_{k+1}\\|_2^2. In the above program, \\vx_{k+1} \\vx_{k+1} is called the projection of \\tilde{\\vx}_{k+1} \\tilde{\\vx}_{k+1} on the set \\setS \\setS and is denoted by \\text{proj}_\\setS(\\tilde{\\vx}_{k+1}) \\text{proj}_\\setS(\\tilde{\\vx}_{k+1}) . For any set \\setS \\in \\R^n \\setS \\in \\R^n , the projection \\text{proj}_\\setS(\\cdot) \\text{proj}_\\setS(\\cdot) is a function from \\R^n\u2192 \\R^n \\R^n\u2192 \\R^n such that \\text{proj}_\\setS(\\vz) = \\argmin_{\\vx \\in\\setS} \\|\\vx - \\vz\\|_2^2. \\text{proj}_\\setS(\\vz) = \\argmin_{\\vx \\in\\setS} \\|\\vx - \\vz\\|_2^2. The projected gradient descent algorithm with constant stepsize is: Projected gradient descent_ Input: initialization \\vx_0 \\vx_0 and step size \\alpha \\alpha For k = 0,1,2,\\dots k = 0,1,2,\\dots compute descent direction \\nabla f(\\vx_k) \\nabla f(\\vx_k) compute \\tilde{\\vx}_{k+1} = \\vx_k -\\alpha \\nabla f(\\vx_k) \\tilde{\\vx}_{k+1} = \\vx_k -\\alpha \\nabla f(\\vx_k) Project \\tilde{\\vx}_{k+1} \\tilde{\\vx}_{k+1} onto \\setS \\setS and set \\vx_{k+1} = \\text{proj}_\\setS(\\tilde{\\vx}_{k+1}) \\vx_{k+1} = \\text{proj}_\\setS(\\tilde{\\vx}_{k+1}) check stoping criteria Some useful properties \u00b6 Geometry of Projection: The first order necessary optimality condition for \\vx^* \\vx^* to be the minimizer of \\|\\vx - \\vz\\|_2^2 \\|\\vx - \\vz\\|_2^2 subject to \\vx \\vx in a convex set \\setS \\setS is -\\nabla f(\\vx^*) \\in \\setN_\\setS(\\vx^*) -\\nabla f(\\vx^*) \\in \\setN_\\setS(\\vx^*) (or equivalently \u2207 f(\\vx^*)\\trans(\\vz - \\vx^*) \\geq 0 \u2207 f(\\vx^*)\\trans(\\vz - \\vx^*) \\geq 0 for all \\vz \\in \\setS \\vz \\in \\setS ). For projection, these are sufficient conditions as well. So, \\vx^* = \\text{proj}_\\setS(\\vz) \u21d4 -(\\vx^* - \\vz) \\in \\setN_\\setS(\\vx^*) \\vx^* = \\text{proj}_\\setS(\\vz) \u21d4 -(\\vx^* - \\vz) \\in \\setN_\\setS(\\vx^*) Fixed point interpretation of Projected Gradient Descent Let \\vx^* = \\argmin_{\\vx \\in \\setS} f(\\vx) \\vx^* = \\argmin_{\\vx \\in \\setS} f(\\vx) and \\alpha >0 \\alpha >0 . Then \\vx^* \\vx^* is a fixed point of p(\\vx) = \\text{proj}_\\setS(\\vx - \\alpha \\nabla f(\\vx)) p(\\vx) = \\text{proj}_\\setS(\\vx - \\alpha \\nabla f(\\vx)) , i.e. \\vx^* =\\text{proj}_\\setS(\\vx^* - \\alpha \\nabla f(\\vx^*)) \\vx^* =\\text{proj}_\\setS(\\vx^* - \\alpha \\nabla f(\\vx^*)) . Proof: From optimality condition, we have \\begin{align*} & \\nabla f(\\vx^*)\\trans(\\vz-\\vx^*) \\geq 0 \\text{ for all } \\vz \\in \\setS\\\\ \\Leftrightarrow &(\\alpha \\nabla f(\\vx^*)\\trans(\\vz-\\vx^*) \\geq 0 \\text{ for all } \\vz \\in \\setS,\\ \\alpha\\geq 0\\\\ \u21d4 &(\\vx^* - (\\vx^* -\\alpha \\nabla f(\\vx^*)\\trans(\\vz-\\vx^*)) \\geq 0 \\text{ for all } \\vz \\in \\setS,\\ \\alpha\\geq 0\\\\ \\end{align*} \\begin{align*} & \\nabla f(\\vx^*)\\trans(\\vz-\\vx^*) \\geq 0 \\text{ for all } \\vz \\in \\setS\\\\ \\Leftrightarrow &(\\alpha \\nabla f(\\vx^*)\\trans(\\vz-\\vx^*) \\geq 0 \\text{ for all } \\vz \\in \\setS,\\ \\alpha\\geq 0\\\\ \u21d4 &(\\vx^* - (\\vx^* -\\alpha \\nabla f(\\vx^*)\\trans(\\vz-\\vx^*)) \\geq 0 \\text{ for all } \\vz \\in \\setS,\\ \\alpha\\geq 0\\\\ \\end{align*} Let \\vy = \\vx^* -\\alpha \\nabla f(\\vx^*) \\vy = \\vx^* -\\alpha \\nabla f(\\vx^*) . Thus, we get (\\vx^* - \\vy)\\trans(\\vz-\\vx^*) \\geq 0 \\text{ for all } \\vz \\in \\setS. (\\vx^* - \\vy)\\trans(\\vz-\\vx^*) \\geq 0 \\text{ for all } \\vz \\in \\setS. Note that this is the optimality condition for \\vx^* \\vx^* to be the projection of \\vy \\vy onto the set \\setS \\setS . Thus, \\vx^* =\\text{proj}_\\setS(\\vx^* - \\alpha \\nabla f(\\vx^*)) \\vx^* =\\text{proj}_\\setS(\\vx^* - \\alpha \\nabla f(\\vx^*)) . Contractive property The projection onto a convex set \\setS \\setS is contractive. That is, for any \\vz, \\vy \\in \\R^n \\vz, \\vy \\in \\R^n , we have \\|\\text{proj}_\\setS(\\vz) - \\text{proj}_\\setS(\\vy)\\|_2 \\leq \\|\\vz - \\vy\\|_2 \\|\\text{proj}_\\setS(\\vz) - \\text{proj}_\\setS(\\vy)\\|_2 \\leq \\|\\vz - \\vy\\|_2 . Examples \u00b6 Example 1 (Euclidean ball) Let \\setS = \\{\\vx | \\|\\vx\\|_2\\leq 1\\} \\setS = \\{\\vx | \\|\\vx\\|_2\\leq 1\\} and let \\vx = \\text{proj}_\\setS(\\vz) \\vx = \\text{proj}_\\setS(\\vz) . Then \\vx = \\left\\{\\begin{array}{ll} \\vz & \\text{if } \\|\\vz\\|_2 \\leq 1\\\\ \\frac{\\vz}{\\|\\vz\\|_2} & \\text{if } \\|\\vz\\|>1 \\end{array}\\right. \\vx = \\left\\{\\begin{array}{ll} \\vz & \\text{if } \\|\\vz\\|_2 \\leq 1\\\\ \\frac{\\vz}{\\|\\vz\\|_2} & \\text{if } \\|\\vz\\|>1 \\end{array}\\right. Example 2 (Inf. norm ball) Let \\setS = \\{\\vx | \\|\\vx\\|_\u221e\\leq 1\\} \\setS = \\{\\vx | \\|\\vx\\|_\u221e\\leq 1\\} and let \\vx = \\text{proj}_\\setS(\\vz) \\vx = \\text{proj}_\\setS(\\vz) . Then x_i = \\text{sign}(z_i)\\min\\{1,|z_i|\\} x_i = \\text{sign}(z_i)\\min\\{1,|z_i|\\} .","title":"Constrained optimization"},{"location":"notes/Constrained_optimization/#constrained-optimization","text":"Let f:\\R^n \\rightarrow \\R f:\\R^n \\rightarrow \\R be a differentiable function and \\setS \\setS be a convex set. In this lecture, we will look at first-order necessary conditions of the minimization program \\begin{equation}\\label{constrained_prob}\\min_{\\vx\\in\\R^n} f(\\vx) \\text{ s.t. } \\vx \\in \\setS.\\end{equation} \\begin{equation}\\label{constrained_prob}\\min_{\\vx\\in\\R^n} f(\\vx) \\text{ s.t. } \\vx \\in \\setS.\\end{equation} In the case where \\setS = \\R^n \\setS = \\R^n , the first order necessary condition for \\vx^* \\vx^* to be a minimizer is \\nabla f(\\vx^*) = 0 \\nabla f(\\vx^*) = 0 . More generally, for arbitrary convex set \\setS \\setS , \\begin{equation}\\label{first_order}\\text{if }\\vx^* \\text{ is a minimizer then it satisfies } \\nabla f(\\vx^*)\\trans (\\vx - \\vx^*) \\geq 0 \\text{ for all } \\vx \\in \\setS.\\end{equation} \\begin{equation}\\label{first_order}\\text{if }\\vx^* \\text{ is a minimizer then it satisfies } \\nabla f(\\vx^*)\\trans (\\vx - \\vx^*) \\geq 0 \\text{ for all } \\vx \\in \\setS.\\end{equation} We can express the above optimality condition in terms of normal cones as well. The normal cone of a set \\setS \\setS at a point \\vx \\vx is defined as \\setN_\\setS(\\vx) = \\{\\vg\\in\\R^n | \\vg\\trans(\\vz-\\vx)\\leq 0,\\ \\forall z \\in \\setS\\}. \\setN_\\setS(\\vx) = \\{\\vg\\in\\R^n | \\vg\\trans(\\vz-\\vx)\\leq 0,\\ \\forall z \\in \\setS\\}. The first order necessary condition in terms of normal cone is: \\text{if } \\vx^* \\text{is a minimizer of \\eqref{constrained_prob}, then -} \\nabla f(\\vx^*) \\in \\setN_\\setS(\\vx^*). \\text{if } \\vx^* \\text{is a minimizer of \\eqref{constrained_prob}, then -} \\nabla f(\\vx^*) \\in \\setN_\\setS(\\vx^*). Figure below shows the normal cone of a set \\setS \\setS at different points in the set.","title":"Constrained Optimization"},{"location":"notes/Constrained_optimization/#examples","text":"Example 1 (Normal cone of set \\setS \\setS at an interior point) We say that \\vx \\in \\R^n \\vx \\in \\R^n is in the interior of a set \\setS \\setS (denoted \\text{int}\\setS \\text{int}\\setS ) if there exist \\epsilon >0 \\epsilon >0 such that $\\vx+\\epsilon \\vv \\in \\setS$ for all \\vv \\in \\R^n. \\vv \\in \\R^n. \\text{For any point } \\vx \\in \\text{int} \\setS, \\text{ we have } \\setN_\\setS(\\vx) = \\{0\\}. \\text{For any point } \\vx \\in \\text{int} \\setS, \\text{ we have } \\setN_\\setS(\\vx) = \\{0\\}. To see this, note that a vector \\vg \\vg in \\setN_\\setS(\\vx) \\setN_\\setS(\\vx) makes a non-positive inner product with every displacement from \\vx \\vx that is contained in \\setS \\setS . However, since \\vx \\vx is in the interior, a valid displacement is \\epsilon \\vg \\epsilon \\vg for some \\epsilon >0 \\epsilon >0 . Example 2 (Normal cone of affine set) Consider an affine set \\setS = \\{\\vx | \\mA\\vx=\\vb\\} \\setS = \\{\\vx | \\mA\\vx=\\vb\\} . By definition of normal cone, we have \\setN_\\setS(\\vx) = \\{\\vg\\in\\R^n | \\vg\\trans(\\vu-\\vx)\\leq 0,\\ \\forall\\ \\vu\\in\\setS\\}. \\setN_\\setS(\\vx) = \\{\\vg\\in\\R^n | \\vg\\trans(\\vu-\\vx)\\leq 0,\\ \\forall\\ \\vu\\in\\setS\\}. In the above definition, note that both \\vu \\vu and \\vx \\vx are in the set \\setS \\setS . Thus, \\vu -\\vv \\in \\vnull(\\mA) \\vu -\\vv \\in \\vnull(\\mA) . So, \\setN_\\setS(\\vx) = \\{\\vg\\in\\R^n | \\vg\\trans\\vu\\leq 0,\\ \\forall\\ \\vu\\in\\vnull(\\mA)\\}. \\setN_\\setS(\\vx) = \\{\\vg\\in\\R^n | \\vg\\trans\\vu\\leq 0,\\ \\forall\\ \\vu\\in\\vnull(\\mA)\\}. Note that if \\vu \\in \\vnull(\\mA) \\vu \\in \\vnull(\\mA) and \\vg\\trans\\vu<0 \\vg\\trans\\vu<0 , then -\\vu \\in \\vnull(\\mA) -\\vu \\in \\vnull(\\mA) and \\vg\\trans(-\\vu)>0 \\vg\\trans(-\\vu)>0 . Therefore, it must be that \\setN_\\setS(\\vx) = \\{\\vg\\in\\R^n | \\vg\\trans\\vu= 0,\\ \\forall\\ \\vu\\in\\vnull(\\mA)\\} = \\range(\\mA\\trans). \\setN_\\setS(\\vx) = \\{\\vg\\in\\R^n | \\vg\\trans\\vu= 0,\\ \\forall\\ \\vu\\in\\vnull(\\mA)\\} = \\range(\\mA\\trans). Example 3 (Normal cone to affine half-space) Consider the set \\setS = \\{\\vx\\ |\\ \\mA\\vx\\leq\\vb \\ \\} \\setS = \\{\\vx\\ |\\ \\mA\\vx\\leq\\vb \\ \\} . Fix \\vx \\in \\setS \\vx \\in \\setS . Then, we can separate inequalities that are active vs those that are not. Let \\setB \\setB be the index set such that \\va_i\\trans\\vx = b_i \\va_i\\trans\\vx = b_i for all i\\in\\setB i\\in\\setB . Similarly, let \\setN \\setN be the index set such that \\va_i\\trans\\vx < b_i \\va_i\\trans\\vx < b_i for all i\\in\\setN i\\in\\setN . The normal cone of \\setS \\setS at a point \\vx \\vx is \\setN_\\setS(\\vx) = \\{\\mA\\trans\\vz | z_i = 0\\ \\forall\\ i\\in\\setN,\\ z_i\\geq0\\ \\forall\\ i\\in\\setB\\}. \\setN_\\setS(\\vx) = \\{\\mA\\trans\\vz | z_i = 0\\ \\forall\\ i\\in\\setN,\\ z_i\\geq0\\ \\forall\\ i\\in\\setB\\}. Use examples 1 and 2 to show this.","title":"Examples"},{"location":"notes/Constrained_optimization/#projected-gradient-method","text":"The goal in this section is to describe a gradient descent based method to solve constrained optimization programs of the form \\eqref{constrained_prob}. The gradient descent iterate at a point \\tilde{\\vx}_k \\tilde{\\vx}_k is \\vx_{k+1} = \\vx - \\alpha \\nabla f(\\vx_k) \\vx_{k+1} = \\vx - \\alpha \\nabla f(\\vx_k) . However, the iterate \\tilde{\\vx}_{k+1} \\tilde{\\vx}_{k+1} may not belong in the constraint set \\setS \\setS . In projected gradient descent, we simply choose the point in the set closest to \\tilde{x}_{k+1} \\tilde{x}_{k+1} as the next iterate \\vx_{k+1} \\vx_{k+1} of the descent algorithm, i.e. \\vx_{k+1} = \\argmin_{\\vx \\in \\setS} \\|\\vx - \\tilde{\\vx}_{k+1}\\|_2^2. \\vx_{k+1} = \\argmin_{\\vx \\in \\setS} \\|\\vx - \\tilde{\\vx}_{k+1}\\|_2^2. In the above program, \\vx_{k+1} \\vx_{k+1} is called the projection of \\tilde{\\vx}_{k+1} \\tilde{\\vx}_{k+1} on the set \\setS \\setS and is denoted by \\text{proj}_\\setS(\\tilde{\\vx}_{k+1}) \\text{proj}_\\setS(\\tilde{\\vx}_{k+1}) . For any set \\setS \\in \\R^n \\setS \\in \\R^n , the projection \\text{proj}_\\setS(\\cdot) \\text{proj}_\\setS(\\cdot) is a function from \\R^n\u2192 \\R^n \\R^n\u2192 \\R^n such that \\text{proj}_\\setS(\\vz) = \\argmin_{\\vx \\in\\setS} \\|\\vx - \\vz\\|_2^2. \\text{proj}_\\setS(\\vz) = \\argmin_{\\vx \\in\\setS} \\|\\vx - \\vz\\|_2^2. The projected gradient descent algorithm with constant stepsize is: Projected gradient descent_ Input: initialization \\vx_0 \\vx_0 and step size \\alpha \\alpha For k = 0,1,2,\\dots k = 0,1,2,\\dots compute descent direction \\nabla f(\\vx_k) \\nabla f(\\vx_k) compute \\tilde{\\vx}_{k+1} = \\vx_k -\\alpha \\nabla f(\\vx_k) \\tilde{\\vx}_{k+1} = \\vx_k -\\alpha \\nabla f(\\vx_k) Project \\tilde{\\vx}_{k+1} \\tilde{\\vx}_{k+1} onto \\setS \\setS and set \\vx_{k+1} = \\text{proj}_\\setS(\\tilde{\\vx}_{k+1}) \\vx_{k+1} = \\text{proj}_\\setS(\\tilde{\\vx}_{k+1}) check stoping criteria","title":"Projected gradient method"},{"location":"notes/Constrained_optimization/#some-useful-properties","text":"Geometry of Projection: The first order necessary optimality condition for \\vx^* \\vx^* to be the minimizer of \\|\\vx - \\vz\\|_2^2 \\|\\vx - \\vz\\|_2^2 subject to \\vx \\vx in a convex set \\setS \\setS is -\\nabla f(\\vx^*) \\in \\setN_\\setS(\\vx^*) -\\nabla f(\\vx^*) \\in \\setN_\\setS(\\vx^*) (or equivalently \u2207 f(\\vx^*)\\trans(\\vz - \\vx^*) \\geq 0 \u2207 f(\\vx^*)\\trans(\\vz - \\vx^*) \\geq 0 for all \\vz \\in \\setS \\vz \\in \\setS ). For projection, these are sufficient conditions as well. So, \\vx^* = \\text{proj}_\\setS(\\vz) \u21d4 -(\\vx^* - \\vz) \\in \\setN_\\setS(\\vx^*) \\vx^* = \\text{proj}_\\setS(\\vz) \u21d4 -(\\vx^* - \\vz) \\in \\setN_\\setS(\\vx^*) Fixed point interpretation of Projected Gradient Descent Let \\vx^* = \\argmin_{\\vx \\in \\setS} f(\\vx) \\vx^* = \\argmin_{\\vx \\in \\setS} f(\\vx) and \\alpha >0 \\alpha >0 . Then \\vx^* \\vx^* is a fixed point of p(\\vx) = \\text{proj}_\\setS(\\vx - \\alpha \\nabla f(\\vx)) p(\\vx) = \\text{proj}_\\setS(\\vx - \\alpha \\nabla f(\\vx)) , i.e. \\vx^* =\\text{proj}_\\setS(\\vx^* - \\alpha \\nabla f(\\vx^*)) \\vx^* =\\text{proj}_\\setS(\\vx^* - \\alpha \\nabla f(\\vx^*)) . Proof: From optimality condition, we have \\begin{align*} & \\nabla f(\\vx^*)\\trans(\\vz-\\vx^*) \\geq 0 \\text{ for all } \\vz \\in \\setS\\\\ \\Leftrightarrow &(\\alpha \\nabla f(\\vx^*)\\trans(\\vz-\\vx^*) \\geq 0 \\text{ for all } \\vz \\in \\setS,\\ \\alpha\\geq 0\\\\ \u21d4 &(\\vx^* - (\\vx^* -\\alpha \\nabla f(\\vx^*)\\trans(\\vz-\\vx^*)) \\geq 0 \\text{ for all } \\vz \\in \\setS,\\ \\alpha\\geq 0\\\\ \\end{align*} \\begin{align*} & \\nabla f(\\vx^*)\\trans(\\vz-\\vx^*) \\geq 0 \\text{ for all } \\vz \\in \\setS\\\\ \\Leftrightarrow &(\\alpha \\nabla f(\\vx^*)\\trans(\\vz-\\vx^*) \\geq 0 \\text{ for all } \\vz \\in \\setS,\\ \\alpha\\geq 0\\\\ \u21d4 &(\\vx^* - (\\vx^* -\\alpha \\nabla f(\\vx^*)\\trans(\\vz-\\vx^*)) \\geq 0 \\text{ for all } \\vz \\in \\setS,\\ \\alpha\\geq 0\\\\ \\end{align*} Let \\vy = \\vx^* -\\alpha \\nabla f(\\vx^*) \\vy = \\vx^* -\\alpha \\nabla f(\\vx^*) . Thus, we get (\\vx^* - \\vy)\\trans(\\vz-\\vx^*) \\geq 0 \\text{ for all } \\vz \\in \\setS. (\\vx^* - \\vy)\\trans(\\vz-\\vx^*) \\geq 0 \\text{ for all } \\vz \\in \\setS. Note that this is the optimality condition for \\vx^* \\vx^* to be the projection of \\vy \\vy onto the set \\setS \\setS . Thus, \\vx^* =\\text{proj}_\\setS(\\vx^* - \\alpha \\nabla f(\\vx^*)) \\vx^* =\\text{proj}_\\setS(\\vx^* - \\alpha \\nabla f(\\vx^*)) . Contractive property The projection onto a convex set \\setS \\setS is contractive. That is, for any \\vz, \\vy \\in \\R^n \\vz, \\vy \\in \\R^n , we have \\|\\text{proj}_\\setS(\\vz) - \\text{proj}_\\setS(\\vy)\\|_2 \\leq \\|\\vz - \\vy\\|_2 \\|\\text{proj}_\\setS(\\vz) - \\text{proj}_\\setS(\\vy)\\|_2 \\leq \\|\\vz - \\vy\\|_2 .","title":"Some useful properties"},{"location":"notes/Constrained_optimization/#examples_1","text":"Example 1 (Euclidean ball) Let \\setS = \\{\\vx | \\|\\vx\\|_2\\leq 1\\} \\setS = \\{\\vx | \\|\\vx\\|_2\\leq 1\\} and let \\vx = \\text{proj}_\\setS(\\vz) \\vx = \\text{proj}_\\setS(\\vz) . Then \\vx = \\left\\{\\begin{array}{ll} \\vz & \\text{if } \\|\\vz\\|_2 \\leq 1\\\\ \\frac{\\vz}{\\|\\vz\\|_2} & \\text{if } \\|\\vz\\|>1 \\end{array}\\right. \\vx = \\left\\{\\begin{array}{ll} \\vz & \\text{if } \\|\\vz\\|_2 \\leq 1\\\\ \\frac{\\vz}{\\|\\vz\\|_2} & \\text{if } \\|\\vz\\|>1 \\end{array}\\right. Example 2 (Inf. norm ball) Let \\setS = \\{\\vx | \\|\\vx\\|_\u221e\\leq 1\\} \\setS = \\{\\vx | \\|\\vx\\|_\u221e\\leq 1\\} and let \\vx = \\text{proj}_\\setS(\\vz) \\vx = \\text{proj}_\\setS(\\vz) . Then x_i = \\text{sign}(z_i)\\min\\{1,|z_i|\\} x_i = \\text{sign}(z_i)\\min\\{1,|z_i|\\} .","title":"Examples"},{"location":"notes/Convex_function/","text":"Convex functions \u00b6 A function f:\\mathcal{C} \u2192 \\R f:\\mathcal{C} \u2192 \\R is convex if $\\mathcal{C} is convex and the function satisfies f(\\theta \\vx + (1-\\theta)\\vy) \\leq \\theta f(\\vx) + (1-\\theta) f(\\vy),\\text{ for all } \\vx,\\vy \\in\\mathcal{C} \\text{ and } \\theta\\in[0,1]. f(\\theta \\vx + (1-\\theta)\\vy) \\leq \\theta f(\\vx) + (1-\\theta) f(\\vy),\\text{ for all } \\vx,\\vy \\in\\mathcal{C} \\text{ and } \\theta\\in[0,1]. Intuitively, the function is convex if the line segment between f(\\vx) f(\\vx) and f(\\vy) f(\\vy) , for any \\vx \\vx an \\vy \\vy , lies above or on the graph of the function. If the line segment lies strictly above the graph of the function, then we say the function is strictly convex, i.e. f:\\mathcal{C} \u2192 \\R f:\\mathcal{C} \u2192 \\R is strictly convex if \\mathcal{C} \\mathcal{C} is convex and the function satisfies f(\\theta \\vx + (1-\\theta)\\vy) < \\theta f(\\vx) + (1-\\theta) f(\\vy),\\text{ for all } \\vx,\\vy \\in\\mathcal{C} \\text{ and } \\theta\\in [0,1]. f(\\theta \\vx + (1-\\theta)\\vy) < \\theta f(\\vx) + (1-\\theta) f(\\vy),\\text{ for all } \\vx,\\vy \\in\\mathcal{C} \\text{ and } \\theta\\in [0,1]. These definitions can be used to describe (strictly) concave functions as well. We say a function f f is (strictly) concave if -f -f is (strictly) convex. Below are some examples of convex and concave functions: Convex functions Affine: \\va\\trans\\vx + \\beta \\va\\trans\\vx + \\beta for any \\va\\in\\R^n \\va\\in\\R^n , \\beta \\in \\R \\beta \\in \\R . Exponential: e^{\\alpha x} e^{\\alpha x} for any \\alpha \\in \\R \\alpha \\in \\R . Powers: x^\\alpha x^\\alpha on \\R_{ ++ } \\R_{ ++ } for all \\alpha \\geq 1 \\alpha \\geq 1 or \\alpha \\leq 0 \\alpha \\leq 0 . Absolute value: |x|^p |x|^p for al p\\geq 1 p\\geq 1 . Negative entropy: x\\log x x\\log x on \\R_{ ++ } \\R_{ ++ } . Concave functions Affine: (see above) Powers: x^\\alpha x^\\alpha on \\R_{++} \\R_{++} for all 0\\leq x\\leq 1 0\\leq x\\leq 1 . Logarithm: \\log x \\log x on \\R_{ ++ } \\R_{ ++ } . In the following sections, unless otherwise stated, we assume the function is convex. However, the properties of these convex function can be adapted to concave functions as well. Restriction to lines \u00b6 A useful property of convex function is that every restriction of the function on a line is also convex. For a fixed \\vx\\in\\R^n \\vx\\in\\R^n and \\vd \\in \\R^n \\vd \\in \\R^n , the restriction of f f along \\vd \\vd is f(\\vx +\\alpha \\vd) f(\\vx +\\alpha \\vd) . Here, \\alpha \\in\\R \\alpha \\in\\R sweeps the graph of the function starting at \\vx \\vx in the direction of \\vd \\vd . More formally, the function f:\\mathcal{C} \\rightarrow \\R f:\\mathcal{C} \\rightarrow \\R , where \\mathcal{C}\\subseteq \\R^n \\mathcal{C}\\subseteq \\R^n is a convex set, is convex if and only if \\psi(\\alpha) = f(\\vx+\\alpha \\vd) \\psi(\\alpha) = f(\\vx+\\alpha \\vd) is convex for all \\vx \\in \\mathcal{C} \\vx \\in \\mathcal{C} and \\vd \\in \\R^n \\vd \\in \\R^n . Operations that preserve convexity \u00b6 Non-negative multiple : The function \\alpha f \\alpha f is convex if f f is convex and \\alpha \\geq 0 \\alpha \\geq 0 . Sum (including infinite sums) : The function f_1 + f_2 f_1 + f_2 is convex if f_1 f_1 and f_2 f_2 are convex functions. Composition with affine function : Fix \\mA \\in\\R^{m\\times n} \\mA \\in\\R^{m\\times n} and \\vb \\in \\R^m \\vb \\in \\R^m . The function f(\\mA\\vx+\\vb) f(\\mA\\vx+\\vb) is convex if f f is convex. Example Log barrier: The function f(x) = -\\sum_{i = 1}^m \\log(\\va_i\\trans\\vx - b_i) f(x) = -\\sum_{i = 1}^m \\log(\\va_i\\trans\\vx - b_i) is convex over the set \\cap _{i = 1}^m\\{\\vx\\in\\R^n | \\va_i\\trans\\vx>b_i\\} \\cap _{i = 1}^m\\{\\vx\\in\\R^n | \\va_i\\trans\\vx>b_i\\} Norm of affine function: The function f(x) = \\|\\mA\\vx-\\vb\\| f(x) = \\|\\mA\\vx-\\vb\\| is convex. The function f(x_1, x_2, x_3) = e^{x_1 - x_2 + x_3} +e^{2x_2} + x_1 f(x_1, x_2, x_3) = e^{x_1 - x_2 + x_3} +e^{2x_2} + x_1 is convex. Characterizations of convex function \u00b6 One intuitive property of the convex function is that every point on the graph of the function is supported by a hyperplane. This property is summarized by the following first order characterization of convex functions. Lemma Let f:\\mathcal{C}\\rightarrow \\R f:\\mathcal{C}\\rightarrow \\R be continuously differentiable over a convex set \\mathcal{C} \\subseteq \\R^n \\mathcal{C} \\subseteq \\R^n . Then f f is convex if and only if f(\\vx) + \\nabla f(\\vx)\\trans(\\vy - \\vx) \\leq f(\\vy) \\text{ for all } \\vx, \\vy \\in \\mathcal{C}. f(\\vx) + \\nabla f(\\vx)\\trans(\\vy - \\vx) \\leq f(\\vy) \\text{ for all } \\vx, \\vy \\in \\mathcal{C}. Proof Idea By convexity of f f , we have f(\\theta \\vy +(1-\\theta) \\vx) \\leq \\theta f(\\vy)+(1-\\theta)f(\\vx) f(\\theta \\vy +(1-\\theta) \\vx) \\leq \\theta f(\\vy)+(1-\\theta)f(\\vx) for all \\vx,\\vy\\in\\mathcal{C} \\vx,\\vy\\in\\mathcal{C} and \\theta \\in [0,1] \\theta \\in [0,1] . This implies that \\frac{f(\\theta \\vy +(1-\\theta) \\vx) - f(x)}{\\theta} \\leq f(\\vy)-f(\\vx). \\frac{f(\\theta \\vy +(1-\\theta) \\vx) - f(x)}{\\theta} \\leq f(\\vy)-f(\\vx). Taking \\lim \\theta \u2198 0 \\lim \\theta \u2198 0 , we get f'(\\vx; \\vy-\\vx) \\leq f(\\vy) - f(\\vx) f'(\\vx; \\vy-\\vx) \\leq f(\\vy) - f(\\vx) . Since, f'(\\vx; \\vy-\\vx) = \\nabla f(\\vx)\\trans (\\vy-\\vx) f'(\\vx; \\vy-\\vx) = \\nabla f(\\vx)\\trans (\\vy-\\vx) , we have f(\\vx) + \\nabla f(\\vx)\\trans(\\vy - \\vx) \\leq f(\\vy) f(\\vx) + \\nabla f(\\vx)\\trans(\\vy - \\vx) \\leq f(\\vy) . This completes part of the proof. Now, we state the second order characterization of convex functions. The second order characterization relates the Hessian of the function with convexity. Lemma Let f:\\mathcal{C}\\rightarrow \\R f:\\mathcal{C}\\rightarrow \\R be twice continuously differentiable over a convex set \\mathcal{C} \\subseteq \\R^n \\mathcal{C} \\subseteq \\R^n . Then f f is convex if and only if \\nabla^2f(\\vx) \u2ab0 0 \\nabla^2f(\\vx) \u2ab0 0 for all \\vx \\in \\mathcal{C} \\vx \\in \\mathcal{C} . The level set of function f:\\mathcal{C}\u2192\\R f:\\mathcal{C}\u2192\\R is the set [f\\leq \\alpha] := \\{\\vx \\in \\mathcal{C}\\ |\\ f(\\vx) \\leq \\alpha\\}. [f\\leq \\alpha] := \\{\\vx \\in \\mathcal{C}\\ |\\ f(\\vx) \\leq \\alpha\\}. Lemma If f f is convex then all of its level sets are convex. Proof Take \\vx,\\vy \\in [f \\leq \\alpha] \\vx,\\vy \\in [f \\leq \\alpha] . Then f(\\vx) \\leq \\alpha f(\\vx) \\leq \\alpha and f(\\vy) \\leq \\alpha f(\\vy) \\leq \\alpha . Since f f is convex, we have that for all \\theta \\in [0,1] \\theta \\in [0,1] \\begin{align} f(\\theta \\vx + (1-\\theta)\\vy) &\\leq \\theta f(\\vx) + (1-\\theta)f(\\vy)\\\\ &\\leq \\theta \\alpha + (1-\\theta) \\alpha\\\\ &= \\alpha.\\end{align} \\begin{align} f(\\theta \\vx + (1-\\theta)\\vy) &\\leq \\theta f(\\vx) + (1-\\theta)f(\\vy)\\\\ &\\leq \\theta \\alpha + (1-\\theta) \\alpha\\\\ &= \\alpha.\\end{align} Thus, \\theta \\vx +(1-\\theta)\\vy \\in [f\\leq \\alpha] \\theta \\vx +(1-\\theta)\\vy \\in [f\\leq \\alpha] . QED Corollary The set of minimizers of \\begin{equation}\\label{convex opt} \\min_{\\vx} f(\\vx) \\text{ subject to } \\vx \\in \\mathcal{C} \\end{equation} is convex. Optimality conditions \u00b6 Convex function enjoys simple global optimality conditions. The main reason for this is any local minimizer of a convex function is also a global minimizer. In order to formalize this property of convex optimization, we first define local and global minimizers. We say \\vx^* \\vx^* is a local minimizer of \\eqref{convex opt} if f(\\vx^*)\\leq f(\\vx) f(\\vx^*)\\leq f(\\vx) for all \\vx \\in \\mathcal{C}\\cap \\mathcal{B}_{\u03f5}(\\vx^*) \\vx \\in \\mathcal{C}\\cap \\mathcal{B}_{\u03f5}(\\vx^*) . Here, \\mathcal{B}_{\u03f5}(\\vx^*) \\mathcal{B}_{\u03f5}(\\vx^*) is the set \\{\\vx\\in \\R^n | \\|\\vx-\\vx^*\\| \\leq \u03f5\\} \\{\\vx\\in \\R^n | \\|\\vx-\\vx^*\\| \\leq \u03f5\\} . While local minimizer is a statement for points around a point, global minimizer is a statement for all points in the set \\mathcal{C} \\mathcal{C} . We say \\vx^* \\vx^* is a global minimizer of \\eqref{convex opt} if f(\\vx^*)\\leq f(\\vx) f(\\vx^*)\\leq f(\\vx) for all \\vx \\in \\mathcal{C} \\vx \\in \\mathcal{C} . lemma Consider the optimization program \\eqref{convex opt}. If \\vx^* \\vx^* is a local minimizer of \\eqref{convex opt}, then it is also a global minimizer of \\eqref{convex opt}. Proof Suppose \\bar{\\vx} \\bar{\\vx} is a local minimizer, but not a global minimizer. Then there exists \\vy \\in \\mathcal{C} \\vy \\in \\mathcal{C} such that f(\\vy) \\leq f(\\bar{\\vx}) f(\\vy) \\leq f(\\bar{\\vx}) . Also, for any \\theta \\in [0,1] \\theta \\in [0,1] , we have \\begin{align*}f(\\theta \\bar{\\vx} + (1-\\theta)\\vy)&\\leq \\theta f(\\bar{\\vx}) + (1-\\theta)f(\\vy)\\\\&\\leq\\theta \\bar{\\vx} + (1-\\theta)f(\\bar{\\vx})\\\\ &= f(\\bar{\\vx}).\\end{align*} \\begin{align*}f(\\theta \\bar{\\vx} + (1-\\theta)\\vy)&\\leq \\theta f(\\bar{\\vx}) + (1-\\theta)f(\\vy)\\\\&\\leq\\theta \\bar{\\vx} + (1-\\theta)f(\\bar{\\vx})\\\\ &= f(\\bar{\\vx}).\\end{align*} This is contradiction. QED Recall that a point \\vx \\vx is a minimizer of a twice continuously differentiable f:\\R^n \u2192\\R f:\\R^n \u2192\\R if and only if \\nabla f(\\vx) = 0 \\nabla f(\\vx) = 0 and \\nabla^2 f(\\vx) \u2ab0 0 \\nabla^2 f(\\vx) \u2ab0 0 . However, if a function is convex, \\nabla f(\\vx) = 0 \\nabla f(\\vx) = 0 is a sufficient and necessary condition for \\vx \\vx to be a minimizer. Since all minimizers of a convex function is a global minimizer, \\nabla f(\\vx) = 0 \\nabla f(\\vx) = 0 is sufficient and necessary to assert global optimality of \\vx \\vx . Lemma (unconstrained) Let f:\\R^n \u2192 \\R f:\\R^n \u2192 \\R be a continuously differentiable (strict) convex function. A point \\vx^* \\vx^* is a (unique) global minimizer of f(\\vx) f(\\vx) if and only if \\nabla f(\\vx^*) = 0 \\nabla f(\\vx^*) = 0 . Lemma (constrained) Let f:\\mathcal{C} \u2192 \\R f:\\mathcal{C} \u2192 \\R be a continuously differentiable (strict) convex function. A point \\vx^* \\vx^* is a (unique) global minimizer of f(\\vx) f(\\vx) over the set \\mathcal{C} \\mathcal{C} if and only if \\nabla f(\\vx^*)\\trans (\\vz - \\vx^*) \\geq 0 \\nabla f(\\vx^*)\\trans (\\vz - \\vx^*) \\geq 0 for all \\vz \\in \\mathcal{C}. \\vz \\in \\mathcal{C}.","title":"Convex function"},{"location":"notes/Convex_function/#convex-functions","text":"A function f:\\mathcal{C} \u2192 \\R f:\\mathcal{C} \u2192 \\R is convex if $\\mathcal{C} is convex and the function satisfies f(\\theta \\vx + (1-\\theta)\\vy) \\leq \\theta f(\\vx) + (1-\\theta) f(\\vy),\\text{ for all } \\vx,\\vy \\in\\mathcal{C} \\text{ and } \\theta\\in[0,1]. f(\\theta \\vx + (1-\\theta)\\vy) \\leq \\theta f(\\vx) + (1-\\theta) f(\\vy),\\text{ for all } \\vx,\\vy \\in\\mathcal{C} \\text{ and } \\theta\\in[0,1]. Intuitively, the function is convex if the line segment between f(\\vx) f(\\vx) and f(\\vy) f(\\vy) , for any \\vx \\vx an \\vy \\vy , lies above or on the graph of the function. If the line segment lies strictly above the graph of the function, then we say the function is strictly convex, i.e. f:\\mathcal{C} \u2192 \\R f:\\mathcal{C} \u2192 \\R is strictly convex if \\mathcal{C} \\mathcal{C} is convex and the function satisfies f(\\theta \\vx + (1-\\theta)\\vy) < \\theta f(\\vx) + (1-\\theta) f(\\vy),\\text{ for all } \\vx,\\vy \\in\\mathcal{C} \\text{ and } \\theta\\in [0,1]. f(\\theta \\vx + (1-\\theta)\\vy) < \\theta f(\\vx) + (1-\\theta) f(\\vy),\\text{ for all } \\vx,\\vy \\in\\mathcal{C} \\text{ and } \\theta\\in [0,1]. These definitions can be used to describe (strictly) concave functions as well. We say a function f f is (strictly) concave if -f -f is (strictly) convex. Below are some examples of convex and concave functions: Convex functions Affine: \\va\\trans\\vx + \\beta \\va\\trans\\vx + \\beta for any \\va\\in\\R^n \\va\\in\\R^n , \\beta \\in \\R \\beta \\in \\R . Exponential: e^{\\alpha x} e^{\\alpha x} for any \\alpha \\in \\R \\alpha \\in \\R . Powers: x^\\alpha x^\\alpha on \\R_{ ++ } \\R_{ ++ } for all \\alpha \\geq 1 \\alpha \\geq 1 or \\alpha \\leq 0 \\alpha \\leq 0 . Absolute value: |x|^p |x|^p for al p\\geq 1 p\\geq 1 . Negative entropy: x\\log x x\\log x on \\R_{ ++ } \\R_{ ++ } . Concave functions Affine: (see above) Powers: x^\\alpha x^\\alpha on \\R_{++} \\R_{++} for all 0\\leq x\\leq 1 0\\leq x\\leq 1 . Logarithm: \\log x \\log x on \\R_{ ++ } \\R_{ ++ } . In the following sections, unless otherwise stated, we assume the function is convex. However, the properties of these convex function can be adapted to concave functions as well.","title":"Convex functions"},{"location":"notes/Convex_function/#restriction-to-lines","text":"A useful property of convex function is that every restriction of the function on a line is also convex. For a fixed \\vx\\in\\R^n \\vx\\in\\R^n and \\vd \\in \\R^n \\vd \\in \\R^n , the restriction of f f along \\vd \\vd is f(\\vx +\\alpha \\vd) f(\\vx +\\alpha \\vd) . Here, \\alpha \\in\\R \\alpha \\in\\R sweeps the graph of the function starting at \\vx \\vx in the direction of \\vd \\vd . More formally, the function f:\\mathcal{C} \\rightarrow \\R f:\\mathcal{C} \\rightarrow \\R , where \\mathcal{C}\\subseteq \\R^n \\mathcal{C}\\subseteq \\R^n is a convex set, is convex if and only if \\psi(\\alpha) = f(\\vx+\\alpha \\vd) \\psi(\\alpha) = f(\\vx+\\alpha \\vd) is convex for all \\vx \\in \\mathcal{C} \\vx \\in \\mathcal{C} and \\vd \\in \\R^n \\vd \\in \\R^n .","title":"Restriction to lines"},{"location":"notes/Convex_function/#operations-that-preserve-convexity","text":"Non-negative multiple : The function \\alpha f \\alpha f is convex if f f is convex and \\alpha \\geq 0 \\alpha \\geq 0 . Sum (including infinite sums) : The function f_1 + f_2 f_1 + f_2 is convex if f_1 f_1 and f_2 f_2 are convex functions. Composition with affine function : Fix \\mA \\in\\R^{m\\times n} \\mA \\in\\R^{m\\times n} and \\vb \\in \\R^m \\vb \\in \\R^m . The function f(\\mA\\vx+\\vb) f(\\mA\\vx+\\vb) is convex if f f is convex. Example Log barrier: The function f(x) = -\\sum_{i = 1}^m \\log(\\va_i\\trans\\vx - b_i) f(x) = -\\sum_{i = 1}^m \\log(\\va_i\\trans\\vx - b_i) is convex over the set \\cap _{i = 1}^m\\{\\vx\\in\\R^n | \\va_i\\trans\\vx>b_i\\} \\cap _{i = 1}^m\\{\\vx\\in\\R^n | \\va_i\\trans\\vx>b_i\\} Norm of affine function: The function f(x) = \\|\\mA\\vx-\\vb\\| f(x) = \\|\\mA\\vx-\\vb\\| is convex. The function f(x_1, x_2, x_3) = e^{x_1 - x_2 + x_3} +e^{2x_2} + x_1 f(x_1, x_2, x_3) = e^{x_1 - x_2 + x_3} +e^{2x_2} + x_1 is convex.","title":"Operations that preserve convexity"},{"location":"notes/Convex_function/#characterizations-of-convex-function","text":"One intuitive property of the convex function is that every point on the graph of the function is supported by a hyperplane. This property is summarized by the following first order characterization of convex functions. Lemma Let f:\\mathcal{C}\\rightarrow \\R f:\\mathcal{C}\\rightarrow \\R be continuously differentiable over a convex set \\mathcal{C} \\subseteq \\R^n \\mathcal{C} \\subseteq \\R^n . Then f f is convex if and only if f(\\vx) + \\nabla f(\\vx)\\trans(\\vy - \\vx) \\leq f(\\vy) \\text{ for all } \\vx, \\vy \\in \\mathcal{C}. f(\\vx) + \\nabla f(\\vx)\\trans(\\vy - \\vx) \\leq f(\\vy) \\text{ for all } \\vx, \\vy \\in \\mathcal{C}. Proof Idea By convexity of f f , we have f(\\theta \\vy +(1-\\theta) \\vx) \\leq \\theta f(\\vy)+(1-\\theta)f(\\vx) f(\\theta \\vy +(1-\\theta) \\vx) \\leq \\theta f(\\vy)+(1-\\theta)f(\\vx) for all \\vx,\\vy\\in\\mathcal{C} \\vx,\\vy\\in\\mathcal{C} and \\theta \\in [0,1] \\theta \\in [0,1] . This implies that \\frac{f(\\theta \\vy +(1-\\theta) \\vx) - f(x)}{\\theta} \\leq f(\\vy)-f(\\vx). \\frac{f(\\theta \\vy +(1-\\theta) \\vx) - f(x)}{\\theta} \\leq f(\\vy)-f(\\vx). Taking \\lim \\theta \u2198 0 \\lim \\theta \u2198 0 , we get f'(\\vx; \\vy-\\vx) \\leq f(\\vy) - f(\\vx) f'(\\vx; \\vy-\\vx) \\leq f(\\vy) - f(\\vx) . Since, f'(\\vx; \\vy-\\vx) = \\nabla f(\\vx)\\trans (\\vy-\\vx) f'(\\vx; \\vy-\\vx) = \\nabla f(\\vx)\\trans (\\vy-\\vx) , we have f(\\vx) + \\nabla f(\\vx)\\trans(\\vy - \\vx) \\leq f(\\vy) f(\\vx) + \\nabla f(\\vx)\\trans(\\vy - \\vx) \\leq f(\\vy) . This completes part of the proof. Now, we state the second order characterization of convex functions. The second order characterization relates the Hessian of the function with convexity. Lemma Let f:\\mathcal{C}\\rightarrow \\R f:\\mathcal{C}\\rightarrow \\R be twice continuously differentiable over a convex set \\mathcal{C} \\subseteq \\R^n \\mathcal{C} \\subseteq \\R^n . Then f f is convex if and only if \\nabla^2f(\\vx) \u2ab0 0 \\nabla^2f(\\vx) \u2ab0 0 for all \\vx \\in \\mathcal{C} \\vx \\in \\mathcal{C} . The level set of function f:\\mathcal{C}\u2192\\R f:\\mathcal{C}\u2192\\R is the set [f\\leq \\alpha] := \\{\\vx \\in \\mathcal{C}\\ |\\ f(\\vx) \\leq \\alpha\\}. [f\\leq \\alpha] := \\{\\vx \\in \\mathcal{C}\\ |\\ f(\\vx) \\leq \\alpha\\}. Lemma If f f is convex then all of its level sets are convex. Proof Take \\vx,\\vy \\in [f \\leq \\alpha] \\vx,\\vy \\in [f \\leq \\alpha] . Then f(\\vx) \\leq \\alpha f(\\vx) \\leq \\alpha and f(\\vy) \\leq \\alpha f(\\vy) \\leq \\alpha . Since f f is convex, we have that for all \\theta \\in [0,1] \\theta \\in [0,1] \\begin{align} f(\\theta \\vx + (1-\\theta)\\vy) &\\leq \\theta f(\\vx) + (1-\\theta)f(\\vy)\\\\ &\\leq \\theta \\alpha + (1-\\theta) \\alpha\\\\ &= \\alpha.\\end{align} \\begin{align} f(\\theta \\vx + (1-\\theta)\\vy) &\\leq \\theta f(\\vx) + (1-\\theta)f(\\vy)\\\\ &\\leq \\theta \\alpha + (1-\\theta) \\alpha\\\\ &= \\alpha.\\end{align} Thus, \\theta \\vx +(1-\\theta)\\vy \\in [f\\leq \\alpha] \\theta \\vx +(1-\\theta)\\vy \\in [f\\leq \\alpha] . QED Corollary The set of minimizers of \\begin{equation}\\label{convex opt} \\min_{\\vx} f(\\vx) \\text{ subject to } \\vx \\in \\mathcal{C} \\end{equation} is convex.","title":"Characterizations of convex function"},{"location":"notes/Convex_function/#optimality-conditions","text":"Convex function enjoys simple global optimality conditions. The main reason for this is any local minimizer of a convex function is also a global minimizer. In order to formalize this property of convex optimization, we first define local and global minimizers. We say \\vx^* \\vx^* is a local minimizer of \\eqref{convex opt} if f(\\vx^*)\\leq f(\\vx) f(\\vx^*)\\leq f(\\vx) for all \\vx \\in \\mathcal{C}\\cap \\mathcal{B}_{\u03f5}(\\vx^*) \\vx \\in \\mathcal{C}\\cap \\mathcal{B}_{\u03f5}(\\vx^*) . Here, \\mathcal{B}_{\u03f5}(\\vx^*) \\mathcal{B}_{\u03f5}(\\vx^*) is the set \\{\\vx\\in \\R^n | \\|\\vx-\\vx^*\\| \\leq \u03f5\\} \\{\\vx\\in \\R^n | \\|\\vx-\\vx^*\\| \\leq \u03f5\\} . While local minimizer is a statement for points around a point, global minimizer is a statement for all points in the set \\mathcal{C} \\mathcal{C} . We say \\vx^* \\vx^* is a global minimizer of \\eqref{convex opt} if f(\\vx^*)\\leq f(\\vx) f(\\vx^*)\\leq f(\\vx) for all \\vx \\in \\mathcal{C} \\vx \\in \\mathcal{C} . lemma Consider the optimization program \\eqref{convex opt}. If \\vx^* \\vx^* is a local minimizer of \\eqref{convex opt}, then it is also a global minimizer of \\eqref{convex opt}. Proof Suppose \\bar{\\vx} \\bar{\\vx} is a local minimizer, but not a global minimizer. Then there exists \\vy \\in \\mathcal{C} \\vy \\in \\mathcal{C} such that f(\\vy) \\leq f(\\bar{\\vx}) f(\\vy) \\leq f(\\bar{\\vx}) . Also, for any \\theta \\in [0,1] \\theta \\in [0,1] , we have \\begin{align*}f(\\theta \\bar{\\vx} + (1-\\theta)\\vy)&\\leq \\theta f(\\bar{\\vx}) + (1-\\theta)f(\\vy)\\\\&\\leq\\theta \\bar{\\vx} + (1-\\theta)f(\\bar{\\vx})\\\\ &= f(\\bar{\\vx}).\\end{align*} \\begin{align*}f(\\theta \\bar{\\vx} + (1-\\theta)\\vy)&\\leq \\theta f(\\bar{\\vx}) + (1-\\theta)f(\\vy)\\\\&\\leq\\theta \\bar{\\vx} + (1-\\theta)f(\\bar{\\vx})\\\\ &= f(\\bar{\\vx}).\\end{align*} This is contradiction. QED Recall that a point \\vx \\vx is a minimizer of a twice continuously differentiable f:\\R^n \u2192\\R f:\\R^n \u2192\\R if and only if \\nabla f(\\vx) = 0 \\nabla f(\\vx) = 0 and \\nabla^2 f(\\vx) \u2ab0 0 \\nabla^2 f(\\vx) \u2ab0 0 . However, if a function is convex, \\nabla f(\\vx) = 0 \\nabla f(\\vx) = 0 is a sufficient and necessary condition for \\vx \\vx to be a minimizer. Since all minimizers of a convex function is a global minimizer, \\nabla f(\\vx) = 0 \\nabla f(\\vx) = 0 is sufficient and necessary to assert global optimality of \\vx \\vx . Lemma (unconstrained) Let f:\\R^n \u2192 \\R f:\\R^n \u2192 \\R be a continuously differentiable (strict) convex function. A point \\vx^* \\vx^* is a (unique) global minimizer of f(\\vx) f(\\vx) if and only if \\nabla f(\\vx^*) = 0 \\nabla f(\\vx^*) = 0 . Lemma (constrained) Let f:\\mathcal{C} \u2192 \\R f:\\mathcal{C} \u2192 \\R be a continuously differentiable (strict) convex function. A point \\vx^* \\vx^* is a (unique) global minimizer of f(\\vx) f(\\vx) over the set \\mathcal{C} \\mathcal{C} if and only if \\nabla f(\\vx^*)\\trans (\\vz - \\vx^*) \\geq 0 \\nabla f(\\vx^*)\\trans (\\vz - \\vx^*) \\geq 0 for all \\vz \\in \\mathcal{C}. \\vz \\in \\mathcal{C}.","title":"Optimality conditions"},{"location":"notes/Convex_set/","text":"Convex Sets \u00b6 In this lecture, we define convex sets and set operations that preserve convexity. We will first look a affine sets, linear sets and subspaces. Affine sets, subspaces \u00b6 Affine set: A line through two distinct points \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n is the set \\{\\vz\\ |\\ \\theta\\vx + (1-\\theta)\\vy = z, \\theta \\in \\R\\} \\{\\vz\\ |\\ \\theta\\vx + (1-\\theta)\\vy = z, \\theta \\in \\R\\} . A set \\setS \\setS is affine if it contains all lines through any two distinct vectors in the set. That is, for every two vectors \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n and scalar \\theta \\in \\R \\theta \\in \\R , the vector \\theta \\vx + (1-\\theta) \\vy \\theta \\vx + (1-\\theta) \\vy is also in the set \\setS \\setS . Subspace: A set \\mathcal{S} \\subset \\R^n \\mathcal{S} \\subset \\R^n of vectors in \\R^n \\R^n is called a subspace of the following two properties are satisfied: (Closed under addition) For every two vectors \\vx \\vx , \\vy \\in \\setS \\vy \\in \\setS , their sum \\vx + \\vy \\vx + \\vy is also in \\setS \\setS . (closed under scalar multiplication) For every vector \\vx \\in \\setS \\vx \\in \\setS and scalar \\theta \\in \\R \\theta \\in \\R , the scalar multiple \\theta \\vx \\theta \\vx is also in \\setS \\setS . Examples of subspaces included the range and null space of a matrix. Convex sets \u00b6 The line segment between any two points \\vx \\vx and \\vy \\vy is the set \\{z | z = \\theta \\vx + (1-\\theta) \\vy,\\ \\theta \\in [0,1]\\} \\{z | z = \\theta \\vx + (1-\\theta) \\vy,\\ \\theta \\in [0,1]\\} . A set \\setC \\subseteq \\R^n \\setC \\subseteq \\R^n is convex if it contains the line segment between any two distinct points in the set, i.e. \\setC \\setC is convex if \\vx ,\\vy \\in \\setC \\text{ and } \\theta \\in [0,1] \\Rightarrow \\theta \\vx + (1-\\theta) \\vy \\in \\setC. \\vx ,\\vy \\in \\setC \\text{ and } \\theta \\in [0,1] \\Rightarrow \\theta \\vx + (1-\\theta) \\vy \\in \\setC. Another way to define convex sets is via the notion of convex combination. Convex combination of a set of points \\vx_1, \\dots, \\vx_k \\in \\R^n \\vx_1, \\dots, \\vx_k \\in \\R^n is the linear combination of those points \\vx = \\theta_1\\vx_1 + \\dots + \\theta_k\\vx_k \\vx = \\theta_1\\vx_1 + \\dots + \\theta_k\\vx_k with the coefficients \\theta_i \\theta_i satisfying \\sum_{i=1}^k \\theta_i = 1 \\sum_{i=1}^k \\theta_i = 1 and \\theta_i\\geq 0 \\theta_i\\geq 0 for all i i . So, a set \\setC \\setC is convex if every convex combination of points in the set is in the set. Some examples of convex sets are hyperplanes and half-spaces. Hyperplane is a set of the form \\{\\vx\\ |\\ \\va\\trans\\vx = b\\} \\{\\vx\\ |\\ \\va\\trans\\vx = b\\} , where \\va \\neq0 \\va \\neq0 and \\vb \\in \\R \\vb \\in \\R . Similarly, half-space is a set of the form \\{\\vx\\ |\\ \\va\\trans\\vx \\leq b\\} \\{\\vx\\ |\\ \\va\\trans\\vx \\leq b\\} , where \\va \\neq0 \\va \\neq0 and \\vb \\in \\R \\vb \\in \\R . Note that hyperplanes are affine and convex, but half-spaces are only convex. If a set is not convex, the simplest way to 'convexify' the set is by taking its convex hull. The convex hull of a set \\setD \\setD is the set that contains all convex combinations of points in the set \\setD \\setD . Formally, the convex hull of \\setD \\setD is \\text{conv}(\\setD) := \\{z \\ | \\ z = \\theta\\vx + (1-\\theta)\\vy,\\ \\vx,\\vy \\in \\setD,\\ \\theta\\in[0,1]\\}. \\text{conv}(\\setD) := \\{z \\ | \\ z = \\theta\\vx + (1-\\theta)\\vy,\\ \\vx,\\vy \\in \\setD,\\ \\theta\\in[0,1]\\}. Operations that preserve convexity \u00b6 Intersections : Let \\setC_i \\subseteq \\R^n \\setC_i \\subseteq \\R^n be a convex set for all i \\in \\setI i \\in \\setI . Then the intersection \\cap_{i \\in \\setI} \\setC_i \\cap_{i \\in \\setI} \\setC_i is itself a convex set. Some examples are: Convex Polytope: A set \\setP = \\{x \\in \\R^n\\ |\\ mA\\vx \\leq\\vb\\} \\setP = \\{x \\in \\R^n\\ |\\ mA\\vx \\leq\\vb\\} defined by a set of linear inequalities is convex. Let \\va_i \\va_i be the i i th column of \\mA\\trans \\mA\\trans . Then, we express \\setP \\setP as an intersection of half-spaces since \\begin{align*}\\setP &= \\{\\vx \\in \\R^n\\ |\\ \\mA\\vx \\leq\\vb\\}\\\\ & = \\cap_{i=1}^m \\{\\vx \\ | \\ \\va_i\\trans\\vx \\leq b_i\\}.\\end{align*} \\begin{align*}\\setP &= \\{\\vx \\in \\R^n\\ |\\ \\mA\\vx \\leq\\vb\\}\\\\ & = \\cap_{i=1}^m \\{\\vx \\ | \\ \\va_i\\trans\\vx \\leq b_i\\}.\\end{align*} Note that half-spaces are convex. Thus, the set \\setP \\setP is a convex set. n n -dimensional simplex: Consider the set \\setC = \\{\\vx \\in \\R^n\\ |\\ \\sum_{i=1}^n\\vx_i \\leq 1,\\ \\vx_i \\geq 0\\} \\setC = \\{\\vx \\in \\R^n\\ |\\ \\sum_{i=1}^n\\vx_i \\leq 1,\\ \\vx_i \\geq 0\\} . This set is called an n-dimensional simplex. We can express \\setC \\setC as \\setP = \\{\\vx \\ | \\ \\ve\\trans\\vx\\leq 1\\}\\cap\\{\\vx \\ | -\\ \\ve_1\\trans\\vx\\leq 0\\}\\cap\\dots\\cap\\{\\vx \\ | \\ -\\ve_n\\trans\\vx\\leq 0\\}. \\setP = \\{\\vx \\ | \\ \\ve\\trans\\vx\\leq 1\\}\\cap\\{\\vx \\ | -\\ \\ve_1\\trans\\vx\\leq 0\\}\\cap\\dots\\cap\\{\\vx \\ | \\ -\\ve_n\\trans\\vx\\leq 0\\}. Thus, an n n -dimensional simplex is the intersection of convex sets and is itself convex. Linear Mapping: Let \\setC \\subseteq \\R^n \\setC \\subseteq \\R^n be a convex set. For any m m by n n matrix \\mA \\mA , the image of \\setC \\setC under \\mA \\mA is convex. That is, the set \\mA(\\setC) = \\{\\mA\\vx | \\vx \\in \\setC\\} \\mA(\\setC) = \\{\\mA\\vx | \\vx \\in \\setC\\} is convex. Addition: Let \\setC_1,\\dots,\\setC_m \\subseteq \\R^n \\setC_1,\\dots,\\setC_m \\subseteq \\R^n be convex sets. For every scalars \\lambda_i \\lambda_i , the addition of the scaled convex sets is itself convex. That is, the set \\lambda_1\\setC_1 + \\dots + \\lambda_m\\setC_m = \\{\\sum_{i=1}^m\\lambda_i\\vx_i\\ |\\ \\vx_i\\in \\setC_i, i = 1,dots, m\\} \\lambda_1\\setC_1 + \\dots + \\lambda_m\\setC_m = \\{\\sum_{i=1}^m\\lambda_i\\vx_i\\ |\\ \\vx_i\\in \\setC_i, i = 1,dots, m\\} is convex. Convex cones \u00b6 A set \\setS \\subseteq \\R^n \\setS \\subseteq \\R^n is a cone if every positive scalar multiple of the vector \\vx \\in \\setS \\vx \\in \\setS is also in the set. That is, \\setS \\text{ is a cone if } \\vx\\in \\setS \\Leftrightarrow \\alpha \\vx \\in \\setS \\text{ for all } \\alpha\\geq 0 \\setS \\text{ is a cone if } \\vx\\in \\setS \\Leftrightarrow \\alpha \\vx \\in \\setS \\text{ for all } \\alpha\\geq 0 A convex cone is a cone that is convex. A convex cone satisfies: \\vx, \\vy \\in \\setS \\Leftrightarrow \\theta_1\\vx + \\theta_2 \\vy \\in \\setS,\\ \\text{for all } \\theta_1,\\theta_2 \\geq0. \\vx, \\vy \\in \\setS \\Leftrightarrow \\theta_1\\vx + \\theta_2 \\vy \\in \\setS,\\ \\text{for all } \\theta_1,\\theta_2 \\geq0. Some examples of convex cones are: Non-negative orthant, \\R^n_+ = \\{\\vx\\ |\\ \\vx_i\\geq 0\\ \\forall\\ i=1,\\dots,n\\} \\R^n_+ = \\{\\vx\\ |\\ \\vx_i\\geq 0\\ \\forall\\ i=1,\\dots,n\\} . Second-order cone, \\setL_+^n = \\{\\bmat \\vx\\\\t \\emat\\in\\R^{n+1}\\ |\\ \\|\\vx\\|_2\\leq t,\\ \\vx \\in\\R^n,\\ t\\in \\R_+ \\} \\setL_+^n = \\{\\bmat \\vx\\\\t \\emat\\in\\R^{n+1}\\ |\\ \\|\\vx\\|_2\\leq t,\\ \\vx \\in\\R^n,\\ t\\in \\R_+ \\} . Positive semi-definite cone, \\setS_+^n = \\{\\mX\\in\\R^{n\\times n}\\ |\\ \\vu\\trans\\mX\\vu\\geq,\\ \\forall \\vu\\in\\R^n,\\ \\mX = \\mX\\trans\\} \\setS_+^n = \\{\\mX\\in\\R^{n\\times n}\\ |\\ \\vu\\trans\\mX\\vu\\geq,\\ \\forall \\vu\\in\\R^n,\\ \\mX = \\mX\\trans\\} . The polytope \\setP =\\{\\vx\\ |\\ \\mA\\vx\\leq 0\\} \\setP =\\{\\vx\\ |\\ \\mA\\vx\\leq 0\\} is a convex cone.","title":"Convex sets"},{"location":"notes/Convex_set/#convex-sets","text":"In this lecture, we define convex sets and set operations that preserve convexity. We will first look a affine sets, linear sets and subspaces.","title":"Convex Sets"},{"location":"notes/Convex_set/#affine-sets-subspaces","text":"Affine set: A line through two distinct points \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n is the set \\{\\vz\\ |\\ \\theta\\vx + (1-\\theta)\\vy = z, \\theta \\in \\R\\} \\{\\vz\\ |\\ \\theta\\vx + (1-\\theta)\\vy = z, \\theta \\in \\R\\} . A set \\setS \\setS is affine if it contains all lines through any two distinct vectors in the set. That is, for every two vectors \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n and scalar \\theta \\in \\R \\theta \\in \\R , the vector \\theta \\vx + (1-\\theta) \\vy \\theta \\vx + (1-\\theta) \\vy is also in the set \\setS \\setS . Subspace: A set \\mathcal{S} \\subset \\R^n \\mathcal{S} \\subset \\R^n of vectors in \\R^n \\R^n is called a subspace of the following two properties are satisfied: (Closed under addition) For every two vectors \\vx \\vx , \\vy \\in \\setS \\vy \\in \\setS , their sum \\vx + \\vy \\vx + \\vy is also in \\setS \\setS . (closed under scalar multiplication) For every vector \\vx \\in \\setS \\vx \\in \\setS and scalar \\theta \\in \\R \\theta \\in \\R , the scalar multiple \\theta \\vx \\theta \\vx is also in \\setS \\setS . Examples of subspaces included the range and null space of a matrix.","title":"Affine sets, subspaces"},{"location":"notes/Convex_set/#convex-sets_1","text":"The line segment between any two points \\vx \\vx and \\vy \\vy is the set \\{z | z = \\theta \\vx + (1-\\theta) \\vy,\\ \\theta \\in [0,1]\\} \\{z | z = \\theta \\vx + (1-\\theta) \\vy,\\ \\theta \\in [0,1]\\} . A set \\setC \\subseteq \\R^n \\setC \\subseteq \\R^n is convex if it contains the line segment between any two distinct points in the set, i.e. \\setC \\setC is convex if \\vx ,\\vy \\in \\setC \\text{ and } \\theta \\in [0,1] \\Rightarrow \\theta \\vx + (1-\\theta) \\vy \\in \\setC. \\vx ,\\vy \\in \\setC \\text{ and } \\theta \\in [0,1] \\Rightarrow \\theta \\vx + (1-\\theta) \\vy \\in \\setC. Another way to define convex sets is via the notion of convex combination. Convex combination of a set of points \\vx_1, \\dots, \\vx_k \\in \\R^n \\vx_1, \\dots, \\vx_k \\in \\R^n is the linear combination of those points \\vx = \\theta_1\\vx_1 + \\dots + \\theta_k\\vx_k \\vx = \\theta_1\\vx_1 + \\dots + \\theta_k\\vx_k with the coefficients \\theta_i \\theta_i satisfying \\sum_{i=1}^k \\theta_i = 1 \\sum_{i=1}^k \\theta_i = 1 and \\theta_i\\geq 0 \\theta_i\\geq 0 for all i i . So, a set \\setC \\setC is convex if every convex combination of points in the set is in the set. Some examples of convex sets are hyperplanes and half-spaces. Hyperplane is a set of the form \\{\\vx\\ |\\ \\va\\trans\\vx = b\\} \\{\\vx\\ |\\ \\va\\trans\\vx = b\\} , where \\va \\neq0 \\va \\neq0 and \\vb \\in \\R \\vb \\in \\R . Similarly, half-space is a set of the form \\{\\vx\\ |\\ \\va\\trans\\vx \\leq b\\} \\{\\vx\\ |\\ \\va\\trans\\vx \\leq b\\} , where \\va \\neq0 \\va \\neq0 and \\vb \\in \\R \\vb \\in \\R . Note that hyperplanes are affine and convex, but half-spaces are only convex. If a set is not convex, the simplest way to 'convexify' the set is by taking its convex hull. The convex hull of a set \\setD \\setD is the set that contains all convex combinations of points in the set \\setD \\setD . Formally, the convex hull of \\setD \\setD is \\text{conv}(\\setD) := \\{z \\ | \\ z = \\theta\\vx + (1-\\theta)\\vy,\\ \\vx,\\vy \\in \\setD,\\ \\theta\\in[0,1]\\}. \\text{conv}(\\setD) := \\{z \\ | \\ z = \\theta\\vx + (1-\\theta)\\vy,\\ \\vx,\\vy \\in \\setD,\\ \\theta\\in[0,1]\\}.","title":"Convex sets"},{"location":"notes/Convex_set/#operations-that-preserve-convexity","text":"Intersections : Let \\setC_i \\subseteq \\R^n \\setC_i \\subseteq \\R^n be a convex set for all i \\in \\setI i \\in \\setI . Then the intersection \\cap_{i \\in \\setI} \\setC_i \\cap_{i \\in \\setI} \\setC_i is itself a convex set. Some examples are: Convex Polytope: A set \\setP = \\{x \\in \\R^n\\ |\\ mA\\vx \\leq\\vb\\} \\setP = \\{x \\in \\R^n\\ |\\ mA\\vx \\leq\\vb\\} defined by a set of linear inequalities is convex. Let \\va_i \\va_i be the i i th column of \\mA\\trans \\mA\\trans . Then, we express \\setP \\setP as an intersection of half-spaces since \\begin{align*}\\setP &= \\{\\vx \\in \\R^n\\ |\\ \\mA\\vx \\leq\\vb\\}\\\\ & = \\cap_{i=1}^m \\{\\vx \\ | \\ \\va_i\\trans\\vx \\leq b_i\\}.\\end{align*} \\begin{align*}\\setP &= \\{\\vx \\in \\R^n\\ |\\ \\mA\\vx \\leq\\vb\\}\\\\ & = \\cap_{i=1}^m \\{\\vx \\ | \\ \\va_i\\trans\\vx \\leq b_i\\}.\\end{align*} Note that half-spaces are convex. Thus, the set \\setP \\setP is a convex set. n n -dimensional simplex: Consider the set \\setC = \\{\\vx \\in \\R^n\\ |\\ \\sum_{i=1}^n\\vx_i \\leq 1,\\ \\vx_i \\geq 0\\} \\setC = \\{\\vx \\in \\R^n\\ |\\ \\sum_{i=1}^n\\vx_i \\leq 1,\\ \\vx_i \\geq 0\\} . This set is called an n-dimensional simplex. We can express \\setC \\setC as \\setP = \\{\\vx \\ | \\ \\ve\\trans\\vx\\leq 1\\}\\cap\\{\\vx \\ | -\\ \\ve_1\\trans\\vx\\leq 0\\}\\cap\\dots\\cap\\{\\vx \\ | \\ -\\ve_n\\trans\\vx\\leq 0\\}. \\setP = \\{\\vx \\ | \\ \\ve\\trans\\vx\\leq 1\\}\\cap\\{\\vx \\ | -\\ \\ve_1\\trans\\vx\\leq 0\\}\\cap\\dots\\cap\\{\\vx \\ | \\ -\\ve_n\\trans\\vx\\leq 0\\}. Thus, an n n -dimensional simplex is the intersection of convex sets and is itself convex. Linear Mapping: Let \\setC \\subseteq \\R^n \\setC \\subseteq \\R^n be a convex set. For any m m by n n matrix \\mA \\mA , the image of \\setC \\setC under \\mA \\mA is convex. That is, the set \\mA(\\setC) = \\{\\mA\\vx | \\vx \\in \\setC\\} \\mA(\\setC) = \\{\\mA\\vx | \\vx \\in \\setC\\} is convex. Addition: Let \\setC_1,\\dots,\\setC_m \\subseteq \\R^n \\setC_1,\\dots,\\setC_m \\subseteq \\R^n be convex sets. For every scalars \\lambda_i \\lambda_i , the addition of the scaled convex sets is itself convex. That is, the set \\lambda_1\\setC_1 + \\dots + \\lambda_m\\setC_m = \\{\\sum_{i=1}^m\\lambda_i\\vx_i\\ |\\ \\vx_i\\in \\setC_i, i = 1,dots, m\\} \\lambda_1\\setC_1 + \\dots + \\lambda_m\\setC_m = \\{\\sum_{i=1}^m\\lambda_i\\vx_i\\ |\\ \\vx_i\\in \\setC_i, i = 1,dots, m\\} is convex.","title":"Operations that preserve convexity"},{"location":"notes/Convex_set/#convex-cones","text":"A set \\setS \\subseteq \\R^n \\setS \\subseteq \\R^n is a cone if every positive scalar multiple of the vector \\vx \\in \\setS \\vx \\in \\setS is also in the set. That is, \\setS \\text{ is a cone if } \\vx\\in \\setS \\Leftrightarrow \\alpha \\vx \\in \\setS \\text{ for all } \\alpha\\geq 0 \\setS \\text{ is a cone if } \\vx\\in \\setS \\Leftrightarrow \\alpha \\vx \\in \\setS \\text{ for all } \\alpha\\geq 0 A convex cone is a cone that is convex. A convex cone satisfies: \\vx, \\vy \\in \\setS \\Leftrightarrow \\theta_1\\vx + \\theta_2 \\vy \\in \\setS,\\ \\text{for all } \\theta_1,\\theta_2 \\geq0. \\vx, \\vy \\in \\setS \\Leftrightarrow \\theta_1\\vx + \\theta_2 \\vy \\in \\setS,\\ \\text{for all } \\theta_1,\\theta_2 \\geq0. Some examples of convex cones are: Non-negative orthant, \\R^n_+ = \\{\\vx\\ |\\ \\vx_i\\geq 0\\ \\forall\\ i=1,\\dots,n\\} \\R^n_+ = \\{\\vx\\ |\\ \\vx_i\\geq 0\\ \\forall\\ i=1,\\dots,n\\} . Second-order cone, \\setL_+^n = \\{\\bmat \\vx\\\\t \\emat\\in\\R^{n+1}\\ |\\ \\|\\vx\\|_2\\leq t,\\ \\vx \\in\\R^n,\\ t\\in \\R_+ \\} \\setL_+^n = \\{\\bmat \\vx\\\\t \\emat\\in\\R^{n+1}\\ |\\ \\|\\vx\\|_2\\leq t,\\ \\vx \\in\\R^n,\\ t\\in \\R_+ \\} . Positive semi-definite cone, \\setS_+^n = \\{\\mX\\in\\R^{n\\times n}\\ |\\ \\vu\\trans\\mX\\vu\\geq,\\ \\forall \\vu\\in\\R^n,\\ \\mX = \\mX\\trans\\} \\setS_+^n = \\{\\mX\\in\\R^{n\\times n}\\ |\\ \\vu\\trans\\mX\\vu\\geq,\\ \\forall \\vu\\in\\R^n,\\ \\mX = \\mX\\trans\\} . The polytope \\setP =\\{\\vx\\ |\\ \\mA\\vx\\leq 0\\} \\setP =\\{\\vx\\ |\\ \\mA\\vx\\leq 0\\} is a convex cone.","title":"Convex cones"},{"location":"notes/Gradient_Descent/","text":"Gradient Descent \u00b6 Descent Directions \u00b6 In this lecture we will consider unconstrained nonlinear optimization of the form \\minimize_{\\vx\\in\\R^n} f(\\vx) \\minimize_{\\vx\\in\\R^n} f(\\vx) where f:\\R^n \\rightarrow \\R f:\\R^n \\rightarrow \\R is continuously differentiable and study an iterative algorithm that solves it. We will consider an iterative algorithm of the form \\vx^{k+1} = \\vx^k + \\alpha^k \\vd^k, \\quad k = 1,2,\\dots \\vx^{k+1} = \\vx^k + \\alpha^k \\vd^k, \\quad k = 1,2,\\dots where \\vd^k \\vd^k and \\alpha^k \\alpha^k are search direction and step length, respectively, at the k k th iteration of the algorithm. A seach direction \\vd \\neq 0 \\vd \\neq 0 is a descent direction for f f at \\vx \\vx if the directional derivative is negative, i.e., f'(\\vx;d) = \\nabla f(x)\\trans \\vd <0. f'(\\vx;d) = \\nabla f(x)\\trans \\vd <0. \\lemma{1} \\lemma{1} If f f is continously differentiable and \\vd \\vd is a decent direction at \\vx \\vx , then for some \\epsilon >0 \\epsilon >0 , we have f(\\vx+\\alpha\\vd) < f(\\vx), \\quad \\forall \\alpha\\in(0,\\epsilon]. f(\\vx+\\alpha\\vd) < f(\\vx), \\quad \\forall \\alpha\\in(0,\\epsilon]. \\proof \\proof (Idea) Because f'(\\vx;\\vd)<0 f'(\\vx;\\vd)<0 , we get \\lim_{\\alpha \\searrow 0} \\frac{f(\\vx+\\alpha\\vd-f(\\vx)}{\\alpha} \\equiv f'(\\vx;\\vd)<0 \\lim_{\\alpha \\searrow 0} \\frac{f(\\vx+\\alpha\\vd-f(\\vx)}{\\alpha} \\equiv f'(\\vx;\\vd)<0 So, there exists \\epsilon >0 \\epsilon >0 such that \\frac{f(\\vx+\\alpha\\vd-f(\\vx)}{\\alpha}<0 \\frac{f(\\vx+\\alpha\\vd-f(\\vx)}{\\alpha}<0 for all \\alpha \\in (0,\\epsilon] \\alpha \\in (0,\\epsilon] . The general outline of decent scheme is: Descent scheme outline Initialization: choose \\vx^0 \\in \\R^n \\vx^0 \\in \\R^n For $ k = 0,1,2,\\dots compute descent direction \\vd^k \\vd^k compute step size \\alpha^k \\alpha^k such that f(\\vx^k +\\alpha^k\\vd^k)< f(x^k) f(\\vx^k +\\alpha^k\\vd^k)< f(x^k) update \\vx^{k+1} = \\vx^k +\\alpha^k \\vd^k \\vx^{k+1} = \\vx^k +\\alpha^k \\vd^k check stopping criteria Each step in the above gradient descent scheme raises a few important question: How to determine a starting point? What are advantages/disadvantages of different directions \\vd^k \\vd^k ? How to compute a step length \\alpha^k \\alpha^k ? when to stop? Stepsize selection \u00b6 These are the selection rules most used in practice: Constant stepsize: Here, we fix an \\bar{\\alpha} \\bar{\\alpha} and choose $\\alpha^k = \\bar{\\alpha} \\bar{\\alpha} for all k k . Exact linesearch: In exact linesearch, we choose \\alpha^k \\alpha^k to minimize f f along a ray \\vd^k \\vd^k starting at \\vx^k \\vx^k , i.e. \\alpha^k = \\argmin_{\\alpha\\geq 0} f(\\vx^k+\\alpha \\vd^k) \\alpha^k = \\argmin_{\\alpha\\geq 0} f(\\vx^k+\\alpha \\vd^k) Backtracking \"Armijo\" linesearch: For some parameter \\mu \\in (0,1) \\mu \\in (0,1) , reduce \\alpha \\alpha (eg, \\alpha \\leftarrow \\alpha/2 \\alpha \\leftarrow \\alpha/2 begining with \\alpha = 1 \\alpha = 1 ) until the following sufficient decrease property is satisfied f(\\vx^k) - f(\\vx^k + \\alpha d^k) \\geq -\\mu \\alpha \\nabla f(\\vx^k)\\trans\\vd^k f(\\vx^k) - f(\\vx^k + \\alpha d^k) \\geq -\\mu \\alpha \\nabla f(\\vx^k)\\trans\\vd^k In the above figure, \\alpha_3 \\alpha_3 and \\alpha_4 \\alpha_4 satisfy the sufficient decrease property. \\exa{(Exact linesearch for quadratic functions)} \\exa{(Exact linesearch for quadratic functions)} An exact linesearch is typically only possible for quadratic functions: f(\\vx) = \\frac{1}{2}\\vx\\trans\\mA\\vx +\\vb\\trans\\vx + c f(\\vx) = \\frac{1}{2}\\vx\\trans\\mA\\vx +\\vb\\trans\\vx + c with \\mA \\succ 0 \\mA \\succ 0 . Exact line search solves the 1-dimensional optimization problem \\min_{\\alpha\\geq 0} f(\\vx + \\alpha \\vd) \\min_{\\alpha\\geq 0} f(\\vx + \\alpha \\vd) where \\vd \\vd is a descent direction, and both \\vx \\vx and \\vd \\vd are fixed. In the quadratic case, we have f(\\vx+\\alpha\\vd) = \\frac{1}{2}(\\vx+\\alpha \\vd)\\trans\\mA(\\vx+\\alpha \\vd) + \\vb\\trans(\\vx+\\alpha\\vd)+c. f(\\vx+\\alpha\\vd) = \\frac{1}{2}(\\vx+\\alpha \\vd)\\trans\\mA(\\vx+\\alpha \\vd) + \\vb\\trans(\\vx+\\alpha\\vd)+c. Since the gradient of f(\\vx+\\alpha\\vd) f(\\vx+\\alpha\\vd) w.r.t. \\alpha \\alpha is \\frac{d}{d\\alpha}f(\\vx+\\alpha\\vd) = \\alpha\\vd\\trans\\mA\\vd+\\vx\\trans\\mA\\vd+\\vb\\trans\\vd = \\alpha\\vd\\trans\\mA\\vd + \\nabla f(\\vx)\\trans\\vd, \\frac{d}{d\\alpha}f(\\vx+\\alpha\\vd) = \\alpha\\vd\\trans\\mA\\vd+\\vx\\trans\\mA\\vd+\\vb\\trans\\vd = \\alpha\\vd\\trans\\mA\\vd + \\nabla f(\\vx)\\trans\\vd, the optimal \\alpha = -\\frac{\\nabla f(\\vx)\\trans\\vd}{\\vd\\trans\\mA\\vd} \\alpha = -\\frac{\\nabla f(\\vx)\\trans\\vd}{\\vd\\trans\\mA\\vd} , which is always positive under the assumption that \\mA \\succ 0 \\mA \\succ 0 . Search Directions \u00b6 The simplest search direction provides the gradient descent algorithm. In gradient descent, the search direction \\vd^k : = -\\vg_k \\vd^k : = -\\vg_k , where \\vg_k = \\nabla f(\\vx^k) \\vg_k = \\nabla f(\\vx^k) . It is easy to see that the negative gradient descent direction -\\vg_k -\\vg_k provides a descent direction. To show that -\\vg_k -\\vg_k is a descent direction, consider f'(\\vx^k;\\vg_k) = -\\vg_k\\trans\\vg_k = -\\|\\vg\\|_2^2. f'(\\vx^k;\\vg_k) = -\\vg_k\\trans\\vg_k = -\\|\\vg\\|_2^2. If \\vg_k \\neq 0 \\vg_k \\neq 0 , i.e. \\vx^k \\vx^k is not a stationary point, then -\\|\\vg\\|_2^2 <0 -\\|\\vg\\|_2^2 <0 . The negative gradient is also called the steepest descent direction of f f at \\vx \\vx . We say \\vd \\vd is the steepest descent direction if it solves \\min\\{f'(\\vx;\\vd) : \\|\\vd\\| = 1\\}. \\min\\{f'(\\vx;\\vd) : \\|\\vd\\| = 1\\}. The gradent descent algorithm is: Gradient descent Input: \\epsilon >0 \\epsilon >0 (tolerance), \\vx_0 \\vx_0 (starting point) For k = 0,1,2,\\dots k = 0,1,2,\\dots evaluate gradient \\vg^k = \\nabla f(\\vx^k) \\vg^k = \\nabla f(\\vx^k) choose step length \\alpha^k \\alpha^k based on reducing the function \\phi(\\alpha) = f(\\vx^k-\\alpha\\vg^k) \\phi(\\alpha) = f(\\vx^k-\\alpha\\vg^k) set \\vx^{k+1} = \\vx^k -\\alpha^k \\vg^k \\vx^{k+1} = \\vx^k -\\alpha^k \\vg^k stop if \\|\\nabla f(\\vx^{k+1})\\|<\\epsilon \\|\\nabla f(\\vx^{k+1})\\|<\\epsilon . Below is a Julia implementation of gradient method with exact linesearch for minimizing quadratic functions of the form f(\\vx) = \\frac{1}{2}\\vx\\trans\\mA\\vx + \\vb\\trans\\vx f(\\vx) = \\frac{1}{2}\\vx\\trans\\mA\\vx + \\vb\\trans\\vx . We use gradient method with exact linesearch to minimize f(x,y) = x^2+2y^2 f(x,y) = x^2+2y^2 starting at (x_0,y_0) = (2,1) (x_0,y_0) = (2,1) . function grad_method_exact_ls ( A , b , x0 , \u03f5 = 1e-6 ) x = copy ( x0 ) \u2207f = A * x + b k = 0 xtrace = x ' while norm ( \u2207f ) > \u03f5 \u03b1 = dot ( \u2207f , \u2207f ) / dot ( \u2207f , A * \u2207f ) x = x - \u03b1 * \u2207f \u2207f = A * x + b f = ( 1 / 2 ) x '* A * x + b '* x # @printf \"it = %3d | |\u2207f| = %8.2e | f = %8.2e\\n\" k norm(\u2207f) f k += 1 xtrace = vcat ( xtrace , x ' ) end return xtrace end #Apply to f(x) = x^2+2y^2 A = [ 1 0 ; 0 2 ] b = [ 0 , 0 ] x0 = [ 2 , 1 ] xtrace = grad_method_exact_ls ( A , b , x0 ); # contour plot f ( x1 , x2 ) = ( 1 / 2 ) * [ x1 , x2 ] '* A * [ x1 , x2 ] + b '* [ x1 , x2 ] x1 = - 2 : 0.05 : 2 x2 = - 2 : 0.05 : 2 ; contour ( x1 , x2 , f , levels = 50 ) plot! ( xtrace [ : , 1 ], xtrace [ : , 2 ], marker = 3 , legend = false , title = \"Path of gradient method\" ) Convergence of gradient method with constant stepsize \u00b6 In gradient method with constant stepsize, we set \\alpha)k = \\bar{\\alpha} \\alpha)k = \\bar{\\alpha} for all iterations. Naturally, the convergence and convergence rate of gradient method with constant stepsize is depends on the size of the constant \\bar{\\alpha} \\bar{\\alpha} . If \\bar{\\alpha} \\bar{\\alpha} is small then gradient method will likely converge (assuming the function is well-behaved), however the convergence will be slow. On the other hand, if the \\bar{\\alpha} \\bar{\\alpha} is large, gradient method can diverge. So, we must choose a stepsize \\bar{\\alpha} \\in (0,\\alpha_{\\text{max}}) \\bar{\\alpha} \\in (0,\\alpha_{\\text{max}}) for the method to converge. This \\alpha_{\\text{max}} \\alpha_{\\text{max}} will depend on a property of \u2207f \u2207f called Lipschitz continuity. Definition (Lipschitz continuity of gradient) A continuously differentiable function f:\\R^n \\rightarrow \\R f:\\R^n \\rightarrow \\R has Lipschitz continuous gradient with parameter L L if \\|\\nabla f(\\vx) - \\nabla f(\\vy)\\|_2 \\leq L \\|\\vx- \\vy\\|_2 \\|\\nabla f(\\vx) - \\nabla f(\\vy)\\|_2 \\leq L \\|\\vx- \\vy\\|_2 for all vectors \\vx \\vx , \\vy \\vy and some constant L>0 L>0 . Example Let \\mA \\mA be a PSD matrix. Let f(\\vx) = \\frac{1}{2}\\vx\\trans\\mA\\vx+\\vb\\trans\\vx+c f(\\vx) = \\frac{1}{2}\\vx\\trans\\mA\\vx+\\vb\\trans\\vx+c . The gradient is \\nabla f(\\vx) = \\mA\\vx-\\vb \\nabla f(\\vx) = \\mA\\vx-\\vb and \\begin{align*} \\|\\nabla f(\\vx) -\\nabla f(\\vy)\\| =& \\|(\\mA\\vx-\\vb) - (\\mA\\vy-\\vb)\\|\\\\ =& \\|\\mA\\vx-\\mA\\vy\\|\\\\ =& \\|\\mA(\\vx-\\vy)\\|\\\\ \\leq & \\|\\mA\\|_2\\|\\vx-\\vy\\|_2\\\\ \\leq & \\lambda_{\\text{max}}(\\mA) \\|\\vx-\\vy\\|_2. \\end{align*} \\begin{align*} \\|\\nabla f(\\vx) -\\nabla f(\\vy)\\| =& \\|(\\mA\\vx-\\vb) - (\\mA\\vy-\\vb)\\|\\\\ =& \\|\\mA\\vx-\\mA\\vy\\|\\\\ =& \\|\\mA(\\vx-\\vy)\\|\\\\ \\leq & \\|\\mA\\|_2\\|\\vx-\\vy\\|_2\\\\ \\leq & \\lambda_{\\text{max}}(\\mA) \\|\\vx-\\vy\\|_2. \\end{align*} We now state a lemma that provides an upper bound on the stepsize for convergence of gradient method with constant stepsize. Lemma If f:\\R^n\\rightarrow \\R f:\\R^n\\rightarrow \\R has an L-Lipschitz continuous gradient and a minimizer exists, then the gradient method with constant stepsize converges if \\bar{\\alpha} \\in (0,2/L) \\bar{\\alpha} \\in (0,2/L) . Using the above lemma, we can show that minimizing the quadratic f(\\vx) = \\frac{1}{2}\\vx\\trans\\mA\\vx +\\vb\\trans\\vx +c f(\\vx) = \\frac{1}{2}\\vx\\trans\\mA\\vx +\\vb\\trans\\vx +c over \\R^n \\R^n with \\mA = \\bmat 1 & 0 \\\\0 & 2\\emat \\mA = \\bmat 1 & 0 \\\\0 & 2\\emat converges if the constant stepsize in the gradient method satisfies \\bar{\\alpha} \\in (0, 1) \\bar{\\alpha} \\in (0, 1) . This is because \\lambda_{\\text{max}}(\\mA) = 2 \\lambda_{\\text{max}}(\\mA) = 2 . Lemma (Convergence of the gradient method) For the minimization of f:\\R^n\\rightarrow\\R f:\\R^n\\rightarrow\\R bounded below with L L -Lipschitz gradient and one of the following linsearches constant stepsize \\bar{\\alpha}\\in(0,2?L), \\bar{\\alpha}\\in(0,2?L), exact linesearch, or backtracking linesearch with \\mu \\in (-0,1) \\mu \\in (-0,1) , the gradient method satisfies f(\\vx_{k+1})< f(\\vx_k) f(\\vx_{k+1})< f(\\vx_k) for all k =0,1,2,\\dots k =0,1,2,\\dots unless \\nabla f(\\vx_k) = 0 \\nabla f(\\vx_k) = 0 and \\|\\nabla f(\\vx_k)\\| \\rightarrow 0 \\|\\nabla f(\\vx_k)\\| \\rightarrow 0 as k \\rightarrow \\infty k \\rightarrow \\infty . Condition number of a matrix \u00b6 The condition number of a n\\times n n\\times n positive definite matrix \\mA \\mA is defined by \\kappa(\\mA) = \\frac{\\lambdamax(\\mA)}{\\lambdamin(\\mA)}. \\kappa(\\mA) = \\frac{\\lambdamax(\\mA)}{\\lambdamin(\\mA)}. An ill-conditioned matrix have large condition number and the condition number of the Hessian at the solution influences the speed at which the gradient method converges. Generally, if condition number of Hessian is small then gradient method converges quickly and, conversely, if condition number is large then gradient method converges slowly. Consider the Rosenbrock function f(x_1,x_2) = 100(x_1 - x_2^2)^2 + (1-x_1)^2 f(x_1,x_2) = 100(x_1 - x_2^2)^2 + (1-x_1)^2 . We can show that a stationary point of the Rosenbrock function is (x_1, x_2) = (1,1) (x_1, x_2) = (1,1) and the Hessian of f f at (1,1) (1,1) is \\bmat 802 & -400\\\\-400 & 200\\emat \\bmat 802 & -400\\\\-400 & 200\\emat . The Hessian has a large condition number and, as a result, any gradient method will have slow convergence. #Gradient method with backtracking function grad_method_backtracking ( fObj , gObj , x0 ; \u03f5 = 1e-6 , \u03bc = 1e-5 , maxits = 1000 ) x = copy ( x0 ) f = fObj ( x ) \u2207f = gObj ( x ) k = 0 xtrace = x ' while norm ( \u2207f ) > \u03f5 && k < maxits \u03b1 = 1.0 while (( f - fObj ( x - \u03b1 * \u2207f )) < \u03bc * \u03b1 * dot ( \u2207f , \u2207f ) ) \u03b1 /= 2 end x = x - \u03b1 * \u2207f f = fObj ( x ) \u2207f = gObj ( x ) k += 1 xtrace = vcat ( xtrace , x ' ) end @printf \"it = %3d | |\u2207f| = %8.2e | f = %8.2e \\n \" k norm ( \u2207f ) f return x , xtrace end #Apply gradient method with backtracking to Rosenbrock function f ( x ) = 100 ( x [ 2 ] - x [ 1 ] ^ 2 ) ^ 2 + ( 1 - x [ 1 ]) ^ 2 \u2207f ( x ) = ForwardDiff . gradient ( f , x ) x0 = [ 2 , 5 ] x , xtrace = grad_method_backtracking ( f , \u2207f , x0 , \u03bc = 1e-4 , maxits = 1000 ); it = 1000 | |\u2207f| = 1.56e+00 | f = 1.33e+00 #Contour plot f ( x1 , x2 ) = 100 ( x2 - x1 ^ 2 ) ^ 2 + ( 1 - x1 ) ^ 2 x1 = 0 : 0.05 : 3 x2 = 3 : 0.05 : 6 contour ( x1 , x2 , f , levels = 50 ) plot! ( xtrace [ : , 1 ], xtrace [ : , 2 ], marker = 3 , legend = false , title = \"Path of gradient method\" ) In the above plot, we see that the gradient method converges slowly. We can increase the convergence rate by transforming the minimization problem so that Hessian is well-behaved at the minimizer. Scaled gradient method \u00b6 Scaled gradient method applies a linear change of variable so that the resulting problem is well-behaved. Consider the minimization of a function f(\\vx) f(\\vx) where f:\\R^n \\rightarrow \\R f:\\R^n \\rightarrow \\R . In scaled gradient method, we fix a non-singular n\\times n n\\times n matrix \\mS \\mS and minimize \\minimize_{\\vy\\in\\R^n} g(\\vy), \\minimize_{\\vy\\in\\R^n} g(\\vy), where g(\\vy) := f(\\mS\\vx) g(\\vy) := f(\\mS\\vx) . That is, we make a linear change of variable with \\vx = \\mS\\vy \\vx = \\mS\\vy . Applying gradient method to the scaled problem yields \\vy_{k+1} = \\vy_k -\\alpha_k \\nabla g(\\vy_k) \\vy_{k+1} = \\vy_k -\\alpha_k \\nabla g(\\vy_k) with \\nabla g(\\vy) = \\mS\\trans \\nabla f(\\mS \\vy) \\nabla g(\\vy) = \\mS\\trans \\nabla f(\\mS \\vy) . Multiplying on the left by \\mS \\mS , we get \\vx_{k+1} = \\vx_k -\\alpha_k \\mS\\mS\\trans\\nabla f(\\vx_k) \\vx_{k+1} = \\vx_k -\\alpha_k \\mS\\mS\\trans\\nabla f(\\vx_k) . So, the scaled gradient method iterate, with \\mD:= \\mS\\mS\\trans \\mD:= \\mS\\mS\\trans is \\vx_{k+1} = \\vx_k -\\alpha_k\\mD\\nabla f(\\vx_k). \\vx_{k+1} = \\vx_k -\\alpha_k\\mD\\nabla f(\\vx_k). It is easy to show that the scaled gradient -D\\nabla f(\\vx) -D\\nabla f(\\vx) is a descent direction (check f'(\\vx;-\\mD\\nabla f(\\vx)) <0 f'(\\vx;-\\mD\\nabla f(\\vx)) <0 ). The scaled gradient method is Scaled Gradient method Input: \\epsilon >0 \\epsilon >0 (tolerance), \\vx_0 \\vx_0 (starting point) For k = 0,1,2,\\dots k = 0,1,2,\\dots Choose scaling matrix \\mD_k \\mD_k evaluate scaled gradient \\vd_k = \\mD_k \\nabla f(\\vx_k) \\vd_k = \\mD_k \\nabla f(\\vx_k) choose step length \\alpha_k \\alpha_k based on reducing the function \\phi(\\alpha) = f(\\vx_k-\\alpha\\vd_k) \\phi(\\alpha) = f(\\vx_k-\\alpha\\vd_k) set \\vx_{k+1} = \\vx_k -\\alpha_k \\vg_k \\vx_{k+1} = \\vx_k -\\alpha_k \\vg_k stop if \\|\\nabla f(\\vx_{k+1})\\|<\\epsilon \\|\\nabla f(\\vx_{k+1})\\|<\\epsilon . In the above algorithm, the choice of \\mD \\mD should make \\nabla^2 g(\\vy) = \\mD^{\\frac{1}{2}}\\nabla^2 f(\\mD^{\\frac{1}{2}}\\vy)\\mD^{\\frac{1}{2}} = \\mD^{\\frac{1}{2}}\\nabla^2 f(\\vx)\\mD^{\\frac{1}{2}} \\nabla^2 g(\\vy) = \\mD^{\\frac{1}{2}}\\nabla^2 f(\\mD^{\\frac{1}{2}}\\vy)\\mD^{\\frac{1}{2}} = \\mD^{\\frac{1}{2}}\\nabla^2 f(\\vx)\\mD^{\\frac{1}{2}} as well conditioned as possible. For example, in Newton method, we choose \\mD_k = (\\nabla^2 f(\\vx))^{-1} \\mD_k = (\\nabla^2 f(\\vx))^{-1} which results in \\mD^{\\frac{1}{2}}\\nabla^2 f(\\vx)\\mD^{\\frac{1}{2}} = \\mI \\mD^{\\frac{1}{2}}\\nabla^2 f(\\vx)\\mD^{\\frac{1}{2}} = \\mI , which has the condition number of 1. Similarly, in damped-Newton, we choose \\mD_k = (\\nabla^2 f(\\vx)+\\lambda\\mI)^{-1} \\mD_k = (\\nabla^2 f(\\vx)+\\lambda\\mI)^{-1} , where condition number approaches 1 as \\lambda \\rightarrow \\infty \\lambda \\rightarrow \\infty","title":"Gradient descent"},{"location":"notes/Gradient_Descent/#gradient-descent","text":"","title":"Gradient Descent"},{"location":"notes/Gradient_Descent/#descent-directions","text":"In this lecture we will consider unconstrained nonlinear optimization of the form \\minimize_{\\vx\\in\\R^n} f(\\vx) \\minimize_{\\vx\\in\\R^n} f(\\vx) where f:\\R^n \\rightarrow \\R f:\\R^n \\rightarrow \\R is continuously differentiable and study an iterative algorithm that solves it. We will consider an iterative algorithm of the form \\vx^{k+1} = \\vx^k + \\alpha^k \\vd^k, \\quad k = 1,2,\\dots \\vx^{k+1} = \\vx^k + \\alpha^k \\vd^k, \\quad k = 1,2,\\dots where \\vd^k \\vd^k and \\alpha^k \\alpha^k are search direction and step length, respectively, at the k k th iteration of the algorithm. A seach direction \\vd \\neq 0 \\vd \\neq 0 is a descent direction for f f at \\vx \\vx if the directional derivative is negative, i.e., f'(\\vx;d) = \\nabla f(x)\\trans \\vd <0. f'(\\vx;d) = \\nabla f(x)\\trans \\vd <0. \\lemma{1} \\lemma{1} If f f is continously differentiable and \\vd \\vd is a decent direction at \\vx \\vx , then for some \\epsilon >0 \\epsilon >0 , we have f(\\vx+\\alpha\\vd) < f(\\vx), \\quad \\forall \\alpha\\in(0,\\epsilon]. f(\\vx+\\alpha\\vd) < f(\\vx), \\quad \\forall \\alpha\\in(0,\\epsilon]. \\proof \\proof (Idea) Because f'(\\vx;\\vd)<0 f'(\\vx;\\vd)<0 , we get \\lim_{\\alpha \\searrow 0} \\frac{f(\\vx+\\alpha\\vd-f(\\vx)}{\\alpha} \\equiv f'(\\vx;\\vd)<0 \\lim_{\\alpha \\searrow 0} \\frac{f(\\vx+\\alpha\\vd-f(\\vx)}{\\alpha} \\equiv f'(\\vx;\\vd)<0 So, there exists \\epsilon >0 \\epsilon >0 such that \\frac{f(\\vx+\\alpha\\vd-f(\\vx)}{\\alpha}<0 \\frac{f(\\vx+\\alpha\\vd-f(\\vx)}{\\alpha}<0 for all \\alpha \\in (0,\\epsilon] \\alpha \\in (0,\\epsilon] . The general outline of decent scheme is: Descent scheme outline Initialization: choose \\vx^0 \\in \\R^n \\vx^0 \\in \\R^n For $ k = 0,1,2,\\dots compute descent direction \\vd^k \\vd^k compute step size \\alpha^k \\alpha^k such that f(\\vx^k +\\alpha^k\\vd^k)< f(x^k) f(\\vx^k +\\alpha^k\\vd^k)< f(x^k) update \\vx^{k+1} = \\vx^k +\\alpha^k \\vd^k \\vx^{k+1} = \\vx^k +\\alpha^k \\vd^k check stopping criteria Each step in the above gradient descent scheme raises a few important question: How to determine a starting point? What are advantages/disadvantages of different directions \\vd^k \\vd^k ? How to compute a step length \\alpha^k \\alpha^k ? when to stop?","title":"Descent Directions"},{"location":"notes/Gradient_Descent/#stepsize-selection","text":"These are the selection rules most used in practice: Constant stepsize: Here, we fix an \\bar{\\alpha} \\bar{\\alpha} and choose $\\alpha^k = \\bar{\\alpha} \\bar{\\alpha} for all k k . Exact linesearch: In exact linesearch, we choose \\alpha^k \\alpha^k to minimize f f along a ray \\vd^k \\vd^k starting at \\vx^k \\vx^k , i.e. \\alpha^k = \\argmin_{\\alpha\\geq 0} f(\\vx^k+\\alpha \\vd^k) \\alpha^k = \\argmin_{\\alpha\\geq 0} f(\\vx^k+\\alpha \\vd^k) Backtracking \"Armijo\" linesearch: For some parameter \\mu \\in (0,1) \\mu \\in (0,1) , reduce \\alpha \\alpha (eg, \\alpha \\leftarrow \\alpha/2 \\alpha \\leftarrow \\alpha/2 begining with \\alpha = 1 \\alpha = 1 ) until the following sufficient decrease property is satisfied f(\\vx^k) - f(\\vx^k + \\alpha d^k) \\geq -\\mu \\alpha \\nabla f(\\vx^k)\\trans\\vd^k f(\\vx^k) - f(\\vx^k + \\alpha d^k) \\geq -\\mu \\alpha \\nabla f(\\vx^k)\\trans\\vd^k In the above figure, \\alpha_3 \\alpha_3 and \\alpha_4 \\alpha_4 satisfy the sufficient decrease property. \\exa{(Exact linesearch for quadratic functions)} \\exa{(Exact linesearch for quadratic functions)} An exact linesearch is typically only possible for quadratic functions: f(\\vx) = \\frac{1}{2}\\vx\\trans\\mA\\vx +\\vb\\trans\\vx + c f(\\vx) = \\frac{1}{2}\\vx\\trans\\mA\\vx +\\vb\\trans\\vx + c with \\mA \\succ 0 \\mA \\succ 0 . Exact line search solves the 1-dimensional optimization problem \\min_{\\alpha\\geq 0} f(\\vx + \\alpha \\vd) \\min_{\\alpha\\geq 0} f(\\vx + \\alpha \\vd) where \\vd \\vd is a descent direction, and both \\vx \\vx and \\vd \\vd are fixed. In the quadratic case, we have f(\\vx+\\alpha\\vd) = \\frac{1}{2}(\\vx+\\alpha \\vd)\\trans\\mA(\\vx+\\alpha \\vd) + \\vb\\trans(\\vx+\\alpha\\vd)+c. f(\\vx+\\alpha\\vd) = \\frac{1}{2}(\\vx+\\alpha \\vd)\\trans\\mA(\\vx+\\alpha \\vd) + \\vb\\trans(\\vx+\\alpha\\vd)+c. Since the gradient of f(\\vx+\\alpha\\vd) f(\\vx+\\alpha\\vd) w.r.t. \\alpha \\alpha is \\frac{d}{d\\alpha}f(\\vx+\\alpha\\vd) = \\alpha\\vd\\trans\\mA\\vd+\\vx\\trans\\mA\\vd+\\vb\\trans\\vd = \\alpha\\vd\\trans\\mA\\vd + \\nabla f(\\vx)\\trans\\vd, \\frac{d}{d\\alpha}f(\\vx+\\alpha\\vd) = \\alpha\\vd\\trans\\mA\\vd+\\vx\\trans\\mA\\vd+\\vb\\trans\\vd = \\alpha\\vd\\trans\\mA\\vd + \\nabla f(\\vx)\\trans\\vd, the optimal \\alpha = -\\frac{\\nabla f(\\vx)\\trans\\vd}{\\vd\\trans\\mA\\vd} \\alpha = -\\frac{\\nabla f(\\vx)\\trans\\vd}{\\vd\\trans\\mA\\vd} , which is always positive under the assumption that \\mA \\succ 0 \\mA \\succ 0 .","title":"Stepsize selection"},{"location":"notes/Gradient_Descent/#search-directions","text":"The simplest search direction provides the gradient descent algorithm. In gradient descent, the search direction \\vd^k : = -\\vg_k \\vd^k : = -\\vg_k , where \\vg_k = \\nabla f(\\vx^k) \\vg_k = \\nabla f(\\vx^k) . It is easy to see that the negative gradient descent direction -\\vg_k -\\vg_k provides a descent direction. To show that -\\vg_k -\\vg_k is a descent direction, consider f'(\\vx^k;\\vg_k) = -\\vg_k\\trans\\vg_k = -\\|\\vg\\|_2^2. f'(\\vx^k;\\vg_k) = -\\vg_k\\trans\\vg_k = -\\|\\vg\\|_2^2. If \\vg_k \\neq 0 \\vg_k \\neq 0 , i.e. \\vx^k \\vx^k is not a stationary point, then -\\|\\vg\\|_2^2 <0 -\\|\\vg\\|_2^2 <0 . The negative gradient is also called the steepest descent direction of f f at \\vx \\vx . We say \\vd \\vd is the steepest descent direction if it solves \\min\\{f'(\\vx;\\vd) : \\|\\vd\\| = 1\\}. \\min\\{f'(\\vx;\\vd) : \\|\\vd\\| = 1\\}. The gradent descent algorithm is: Gradient descent Input: \\epsilon >0 \\epsilon >0 (tolerance), \\vx_0 \\vx_0 (starting point) For k = 0,1,2,\\dots k = 0,1,2,\\dots evaluate gradient \\vg^k = \\nabla f(\\vx^k) \\vg^k = \\nabla f(\\vx^k) choose step length \\alpha^k \\alpha^k based on reducing the function \\phi(\\alpha) = f(\\vx^k-\\alpha\\vg^k) \\phi(\\alpha) = f(\\vx^k-\\alpha\\vg^k) set \\vx^{k+1} = \\vx^k -\\alpha^k \\vg^k \\vx^{k+1} = \\vx^k -\\alpha^k \\vg^k stop if \\|\\nabla f(\\vx^{k+1})\\|<\\epsilon \\|\\nabla f(\\vx^{k+1})\\|<\\epsilon . Below is a Julia implementation of gradient method with exact linesearch for minimizing quadratic functions of the form f(\\vx) = \\frac{1}{2}\\vx\\trans\\mA\\vx + \\vb\\trans\\vx f(\\vx) = \\frac{1}{2}\\vx\\trans\\mA\\vx + \\vb\\trans\\vx . We use gradient method with exact linesearch to minimize f(x,y) = x^2+2y^2 f(x,y) = x^2+2y^2 starting at (x_0,y_0) = (2,1) (x_0,y_0) = (2,1) . function grad_method_exact_ls ( A , b , x0 , \u03f5 = 1e-6 ) x = copy ( x0 ) \u2207f = A * x + b k = 0 xtrace = x ' while norm ( \u2207f ) > \u03f5 \u03b1 = dot ( \u2207f , \u2207f ) / dot ( \u2207f , A * \u2207f ) x = x - \u03b1 * \u2207f \u2207f = A * x + b f = ( 1 / 2 ) x '* A * x + b '* x # @printf \"it = %3d | |\u2207f| = %8.2e | f = %8.2e\\n\" k norm(\u2207f) f k += 1 xtrace = vcat ( xtrace , x ' ) end return xtrace end #Apply to f(x) = x^2+2y^2 A = [ 1 0 ; 0 2 ] b = [ 0 , 0 ] x0 = [ 2 , 1 ] xtrace = grad_method_exact_ls ( A , b , x0 ); # contour plot f ( x1 , x2 ) = ( 1 / 2 ) * [ x1 , x2 ] '* A * [ x1 , x2 ] + b '* [ x1 , x2 ] x1 = - 2 : 0.05 : 2 x2 = - 2 : 0.05 : 2 ; contour ( x1 , x2 , f , levels = 50 ) plot! ( xtrace [ : , 1 ], xtrace [ : , 2 ], marker = 3 , legend = false , title = \"Path of gradient method\" )","title":"Search Directions"},{"location":"notes/Gradient_Descent/#convergence-of-gradient-method-with-constant-stepsize","text":"In gradient method with constant stepsize, we set \\alpha)k = \\bar{\\alpha} \\alpha)k = \\bar{\\alpha} for all iterations. Naturally, the convergence and convergence rate of gradient method with constant stepsize is depends on the size of the constant \\bar{\\alpha} \\bar{\\alpha} . If \\bar{\\alpha} \\bar{\\alpha} is small then gradient method will likely converge (assuming the function is well-behaved), however the convergence will be slow. On the other hand, if the \\bar{\\alpha} \\bar{\\alpha} is large, gradient method can diverge. So, we must choose a stepsize \\bar{\\alpha} \\in (0,\\alpha_{\\text{max}}) \\bar{\\alpha} \\in (0,\\alpha_{\\text{max}}) for the method to converge. This \\alpha_{\\text{max}} \\alpha_{\\text{max}} will depend on a property of \u2207f \u2207f called Lipschitz continuity. Definition (Lipschitz continuity of gradient) A continuously differentiable function f:\\R^n \\rightarrow \\R f:\\R^n \\rightarrow \\R has Lipschitz continuous gradient with parameter L L if \\|\\nabla f(\\vx) - \\nabla f(\\vy)\\|_2 \\leq L \\|\\vx- \\vy\\|_2 \\|\\nabla f(\\vx) - \\nabla f(\\vy)\\|_2 \\leq L \\|\\vx- \\vy\\|_2 for all vectors \\vx \\vx , \\vy \\vy and some constant L>0 L>0 . Example Let \\mA \\mA be a PSD matrix. Let f(\\vx) = \\frac{1}{2}\\vx\\trans\\mA\\vx+\\vb\\trans\\vx+c f(\\vx) = \\frac{1}{2}\\vx\\trans\\mA\\vx+\\vb\\trans\\vx+c . The gradient is \\nabla f(\\vx) = \\mA\\vx-\\vb \\nabla f(\\vx) = \\mA\\vx-\\vb and \\begin{align*} \\|\\nabla f(\\vx) -\\nabla f(\\vy)\\| =& \\|(\\mA\\vx-\\vb) - (\\mA\\vy-\\vb)\\|\\\\ =& \\|\\mA\\vx-\\mA\\vy\\|\\\\ =& \\|\\mA(\\vx-\\vy)\\|\\\\ \\leq & \\|\\mA\\|_2\\|\\vx-\\vy\\|_2\\\\ \\leq & \\lambda_{\\text{max}}(\\mA) \\|\\vx-\\vy\\|_2. \\end{align*} \\begin{align*} \\|\\nabla f(\\vx) -\\nabla f(\\vy)\\| =& \\|(\\mA\\vx-\\vb) - (\\mA\\vy-\\vb)\\|\\\\ =& \\|\\mA\\vx-\\mA\\vy\\|\\\\ =& \\|\\mA(\\vx-\\vy)\\|\\\\ \\leq & \\|\\mA\\|_2\\|\\vx-\\vy\\|_2\\\\ \\leq & \\lambda_{\\text{max}}(\\mA) \\|\\vx-\\vy\\|_2. \\end{align*} We now state a lemma that provides an upper bound on the stepsize for convergence of gradient method with constant stepsize. Lemma If f:\\R^n\\rightarrow \\R f:\\R^n\\rightarrow \\R has an L-Lipschitz continuous gradient and a minimizer exists, then the gradient method with constant stepsize converges if \\bar{\\alpha} \\in (0,2/L) \\bar{\\alpha} \\in (0,2/L) . Using the above lemma, we can show that minimizing the quadratic f(\\vx) = \\frac{1}{2}\\vx\\trans\\mA\\vx +\\vb\\trans\\vx +c f(\\vx) = \\frac{1}{2}\\vx\\trans\\mA\\vx +\\vb\\trans\\vx +c over \\R^n \\R^n with \\mA = \\bmat 1 & 0 \\\\0 & 2\\emat \\mA = \\bmat 1 & 0 \\\\0 & 2\\emat converges if the constant stepsize in the gradient method satisfies \\bar{\\alpha} \\in (0, 1) \\bar{\\alpha} \\in (0, 1) . This is because \\lambda_{\\text{max}}(\\mA) = 2 \\lambda_{\\text{max}}(\\mA) = 2 . Lemma (Convergence of the gradient method) For the minimization of f:\\R^n\\rightarrow\\R f:\\R^n\\rightarrow\\R bounded below with L L -Lipschitz gradient and one of the following linsearches constant stepsize \\bar{\\alpha}\\in(0,2?L), \\bar{\\alpha}\\in(0,2?L), exact linesearch, or backtracking linesearch with \\mu \\in (-0,1) \\mu \\in (-0,1) , the gradient method satisfies f(\\vx_{k+1})< f(\\vx_k) f(\\vx_{k+1})< f(\\vx_k) for all k =0,1,2,\\dots k =0,1,2,\\dots unless \\nabla f(\\vx_k) = 0 \\nabla f(\\vx_k) = 0 and \\|\\nabla f(\\vx_k)\\| \\rightarrow 0 \\|\\nabla f(\\vx_k)\\| \\rightarrow 0 as k \\rightarrow \\infty k \\rightarrow \\infty .","title":"Convergence of gradient method with constant stepsize"},{"location":"notes/Gradient_Descent/#condition-number-of-a-matrix","text":"The condition number of a n\\times n n\\times n positive definite matrix \\mA \\mA is defined by \\kappa(\\mA) = \\frac{\\lambdamax(\\mA)}{\\lambdamin(\\mA)}. \\kappa(\\mA) = \\frac{\\lambdamax(\\mA)}{\\lambdamin(\\mA)}. An ill-conditioned matrix have large condition number and the condition number of the Hessian at the solution influences the speed at which the gradient method converges. Generally, if condition number of Hessian is small then gradient method converges quickly and, conversely, if condition number is large then gradient method converges slowly. Consider the Rosenbrock function f(x_1,x_2) = 100(x_1 - x_2^2)^2 + (1-x_1)^2 f(x_1,x_2) = 100(x_1 - x_2^2)^2 + (1-x_1)^2 . We can show that a stationary point of the Rosenbrock function is (x_1, x_2) = (1,1) (x_1, x_2) = (1,1) and the Hessian of f f at (1,1) (1,1) is \\bmat 802 & -400\\\\-400 & 200\\emat \\bmat 802 & -400\\\\-400 & 200\\emat . The Hessian has a large condition number and, as a result, any gradient method will have slow convergence. #Gradient method with backtracking function grad_method_backtracking ( fObj , gObj , x0 ; \u03f5 = 1e-6 , \u03bc = 1e-5 , maxits = 1000 ) x = copy ( x0 ) f = fObj ( x ) \u2207f = gObj ( x ) k = 0 xtrace = x ' while norm ( \u2207f ) > \u03f5 && k < maxits \u03b1 = 1.0 while (( f - fObj ( x - \u03b1 * \u2207f )) < \u03bc * \u03b1 * dot ( \u2207f , \u2207f ) ) \u03b1 /= 2 end x = x - \u03b1 * \u2207f f = fObj ( x ) \u2207f = gObj ( x ) k += 1 xtrace = vcat ( xtrace , x ' ) end @printf \"it = %3d | |\u2207f| = %8.2e | f = %8.2e \\n \" k norm ( \u2207f ) f return x , xtrace end #Apply gradient method with backtracking to Rosenbrock function f ( x ) = 100 ( x [ 2 ] - x [ 1 ] ^ 2 ) ^ 2 + ( 1 - x [ 1 ]) ^ 2 \u2207f ( x ) = ForwardDiff . gradient ( f , x ) x0 = [ 2 , 5 ] x , xtrace = grad_method_backtracking ( f , \u2207f , x0 , \u03bc = 1e-4 , maxits = 1000 ); it = 1000 | |\u2207f| = 1.56e+00 | f = 1.33e+00 #Contour plot f ( x1 , x2 ) = 100 ( x2 - x1 ^ 2 ) ^ 2 + ( 1 - x1 ) ^ 2 x1 = 0 : 0.05 : 3 x2 = 3 : 0.05 : 6 contour ( x1 , x2 , f , levels = 50 ) plot! ( xtrace [ : , 1 ], xtrace [ : , 2 ], marker = 3 , legend = false , title = \"Path of gradient method\" ) In the above plot, we see that the gradient method converges slowly. We can increase the convergence rate by transforming the minimization problem so that Hessian is well-behaved at the minimizer.","title":"Condition number of a matrix"},{"location":"notes/Gradient_Descent/#scaled-gradient-method","text":"Scaled gradient method applies a linear change of variable so that the resulting problem is well-behaved. Consider the minimization of a function f(\\vx) f(\\vx) where f:\\R^n \\rightarrow \\R f:\\R^n \\rightarrow \\R . In scaled gradient method, we fix a non-singular n\\times n n\\times n matrix \\mS \\mS and minimize \\minimize_{\\vy\\in\\R^n} g(\\vy), \\minimize_{\\vy\\in\\R^n} g(\\vy), where g(\\vy) := f(\\mS\\vx) g(\\vy) := f(\\mS\\vx) . That is, we make a linear change of variable with \\vx = \\mS\\vy \\vx = \\mS\\vy . Applying gradient method to the scaled problem yields \\vy_{k+1} = \\vy_k -\\alpha_k \\nabla g(\\vy_k) \\vy_{k+1} = \\vy_k -\\alpha_k \\nabla g(\\vy_k) with \\nabla g(\\vy) = \\mS\\trans \\nabla f(\\mS \\vy) \\nabla g(\\vy) = \\mS\\trans \\nabla f(\\mS \\vy) . Multiplying on the left by \\mS \\mS , we get \\vx_{k+1} = \\vx_k -\\alpha_k \\mS\\mS\\trans\\nabla f(\\vx_k) \\vx_{k+1} = \\vx_k -\\alpha_k \\mS\\mS\\trans\\nabla f(\\vx_k) . So, the scaled gradient method iterate, with \\mD:= \\mS\\mS\\trans \\mD:= \\mS\\mS\\trans is \\vx_{k+1} = \\vx_k -\\alpha_k\\mD\\nabla f(\\vx_k). \\vx_{k+1} = \\vx_k -\\alpha_k\\mD\\nabla f(\\vx_k). It is easy to show that the scaled gradient -D\\nabla f(\\vx) -D\\nabla f(\\vx) is a descent direction (check f'(\\vx;-\\mD\\nabla f(\\vx)) <0 f'(\\vx;-\\mD\\nabla f(\\vx)) <0 ). The scaled gradient method is Scaled Gradient method Input: \\epsilon >0 \\epsilon >0 (tolerance), \\vx_0 \\vx_0 (starting point) For k = 0,1,2,\\dots k = 0,1,2,\\dots Choose scaling matrix \\mD_k \\mD_k evaluate scaled gradient \\vd_k = \\mD_k \\nabla f(\\vx_k) \\vd_k = \\mD_k \\nabla f(\\vx_k) choose step length \\alpha_k \\alpha_k based on reducing the function \\phi(\\alpha) = f(\\vx_k-\\alpha\\vd_k) \\phi(\\alpha) = f(\\vx_k-\\alpha\\vd_k) set \\vx_{k+1} = \\vx_k -\\alpha_k \\vg_k \\vx_{k+1} = \\vx_k -\\alpha_k \\vg_k stop if \\|\\nabla f(\\vx_{k+1})\\|<\\epsilon \\|\\nabla f(\\vx_{k+1})\\|<\\epsilon . In the above algorithm, the choice of \\mD \\mD should make \\nabla^2 g(\\vy) = \\mD^{\\frac{1}{2}}\\nabla^2 f(\\mD^{\\frac{1}{2}}\\vy)\\mD^{\\frac{1}{2}} = \\mD^{\\frac{1}{2}}\\nabla^2 f(\\vx)\\mD^{\\frac{1}{2}} \\nabla^2 g(\\vy) = \\mD^{\\frac{1}{2}}\\nabla^2 f(\\mD^{\\frac{1}{2}}\\vy)\\mD^{\\frac{1}{2}} = \\mD^{\\frac{1}{2}}\\nabla^2 f(\\vx)\\mD^{\\frac{1}{2}} as well conditioned as possible. For example, in Newton method, we choose \\mD_k = (\\nabla^2 f(\\vx))^{-1} \\mD_k = (\\nabla^2 f(\\vx))^{-1} which results in \\mD^{\\frac{1}{2}}\\nabla^2 f(\\vx)\\mD^{\\frac{1}{2}} = \\mI \\mD^{\\frac{1}{2}}\\nabla^2 f(\\vx)\\mD^{\\frac{1}{2}} = \\mI , which has the condition number of 1. Similarly, in damped-Newton, we choose \\mD_k = (\\nabla^2 f(\\vx)+\\lambda\\mI)^{-1} \\mD_k = (\\nabla^2 f(\\vx)+\\lambda\\mI)^{-1} , where condition number approaches 1 as \\lambda \\rightarrow \\infty \\lambda \\rightarrow \\infty","title":"Scaled gradient method"},{"location":"notes/Least_squares/","text":"Least Squares \u00b6 In this lecture, we will cover least squares for data fitting, linear systems, properties of least squares and QR factorization. Least squares for data fitting \u00b6 Consider the problem of fitting a line to observations y_i y_i gven input z_i z_i for i = 1,\\dots, n i = 1,\\dots, n . In the figure above, the data points seem to follow a linear trend. One way to find the parameters c,s \\in \\R c,s \\in \\R of the linear model f(z) = s\\cdot z + c f(z) = s\\cdot z + c that coresponds to a line of best fit is to minimize the following squared distance subject to a linear constraint: \\begin{align*} \\min_{s,\\,c,\\,\\hat{y}_i \\in \\R} &\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\\\ \\text{subject to } &s\\cdot z_i+c = \\hat{y}_i \\end{align*} \\begin{align*} \\min_{s,\\,c,\\,\\hat{y}_i \\in \\R} &\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\\\ \\text{subject to } &s\\cdot z_i+c = \\hat{y}_i \\end{align*} The above minimization program can be reformulated as a linear least squares problem: \\min_{\\vx} \\|\\mA \\vx - \\vb\\|_2^2 = \\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2, \\min_{\\vx} \\|\\mA \\vx - \\vb\\|_2^2 = \\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2, where \\mA = \\begin{bmatrix}z_1 & 1\\\\ z_2 & 1\\\\ \\vdots&\\vdots\\\\z_n & 1\\end{bmatrix}, \\ \\vb = \\begin{bmatrix}y_1\\\\y_2\\\\\\vdots \\\\y_n\\end{bmatrix}, \\text{ and } \\vx = \\begin{bmatrix}s\\\\c\\end{bmatrix}. \\mA = \\begin{bmatrix}z_1 & 1\\\\ z_2 & 1\\\\ \\vdots&\\vdots\\\\z_n & 1\\end{bmatrix}, \\ \\vb = \\begin{bmatrix}y_1\\\\y_2\\\\\\vdots \\\\y_n\\end{bmatrix}, \\text{ and } \\vx = \\begin{bmatrix}s\\\\c\\end{bmatrix}. Let \\begin{equation}\\label{least_squares_problem} \\func{f}(\\vx):=\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 = \\frac{1}{2}\\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2. \\end{equation} \\begin{equation}\\label{least_squares_problem} \\func{f}(\\vx):=\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 = \\frac{1}{2}\\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2. \\end{equation} The gradient and hessian of \\func{f}(\\vx) \\func{f}(\\vx) are: \\nabla\\func{f}(\\vx) = \\mA^\\intercal(\\mA\\vx-\\vb), \\text{ and } \\nabla^2\\func{f}(\\vx) = \\mA^\\intercal\\mA, \\nabla\\func{f}(\\vx) = \\mA^\\intercal(\\mA\\vx-\\vb), \\text{ and } \\nabla^2\\func{f}(\\vx) = \\mA^\\intercal\\mA, respectively. The Hessian is positive semidefinite for every \\vx \\vx (and is positive definite if \\mA \\mA has full row rank). This implies that the function \\func{f}(\\vx) \\func{f}(\\vx) is convex. Additionally, \\vx = \\vx^{*} \\vx = \\vx^{*} is a critical point if \\begin{equation}\\label{least_square_normal_eqn} \\mA^\\intercal(\\mA\\vx^*-b) = \\vzero. \\end{equation} \\begin{equation}\\label{least_square_normal_eqn} \\mA^\\intercal(\\mA\\vx^*-b) = \\vzero. \\end{equation} Since \\func{f}(\\vx) \\func{f}(\\vx) is convex, \\vx^{*} \\vx^{*} is a global minimizer. Equation \\eqref{least_square_normal_eqn} is called the normal equations of the least squares problem \\eqref{least_squares_problem}. Solving the normal equations, we get the following line of best fit. Linear systems \u00b6 Consider the problem of solving a linear system of equations. For \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} and \\vb\\in\\R^{m} \\vb\\in\\R^{m} , the linear system of equations \\mA\\vx = \\vb \\mA\\vx = \\vb is: overdetermined if m>n m>n , underdetermined if m< n m< n , or square if m = n m = n . A linear system can have exactly one solution, many solutions, or no solutions: In general, a linear system \\mA\\vx=\\vb \\mA\\vx=\\vb has a solution if \\vb \\in \\text{range} (\\mA) \\vb \\in \\text{range} (\\mA) . Properties of linear least squares \u00b6 Recall that the minimizer \\vx^* \\vx^* to the linear least squares poblem satisfies the normal equations: \\mA^\\intercal \\mA \\vx^* = \\mA^\\intercal\\vb \\mA^\\intercal \\mA \\vx^* = \\mA^\\intercal\\vb with the residual \\vr^* = \\mA\\vx^* -\\vb, \\vr^* = \\mA\\vx^* -\\vb, satisfying \\mA^\\intercal\\vr^* = \\vzero. \\mA^\\intercal\\vr^* = \\vzero. Here, \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} . The minimzer of the linear least squares problem is unique if \\mA^\\intercal\\mA \\mA^\\intercal\\mA is invertible. However, the vector in the range of \\mA \\mA closest to \\vb \\vb is unique, i.e. \\vb^* = \\mA\\vx* \\vb^* = \\mA\\vx* is unique. Recall that range space of \\mA \\mA and the null space of \\mA^\\intercal \\mA^\\intercal is: \\text{range}(\\mA) = \\set{R}(\\mA) = \\{\\vy : \\vy = \\mA \\vx \\text{ for some } \\vx \\}\\\\ \\text{null}(\\mA^\\intercal)= \\set{N}(\\mA^\\intercal) = \\{\\vz: \\mA^\\intercal\\vz = \\vzero\\} \\text{range}(\\mA) = \\set{R}(\\mA) = \\{\\vy : \\vy = \\mA \\vx \\text{ for some } \\vx \\}\\\\ \\text{null}(\\mA^\\intercal)= \\set{N}(\\mA^\\intercal) = \\{\\vz: \\mA^\\intercal\\vz = \\vzero\\} By fundamental theorem of linear algebra, we have $$ \\begin{equation}\\label{least_squares_FTLA} \\set{R}(\\mA) \\oplus \\set{N}(\\mA^\\intercal) = \\R^m. \\end{equation} $$ Thus, for all \\vx \\in \\R^m \\vx \\in \\R^m , we have \\vx = \\vu + \\vv, \\quad \\vu \\in \\set{R}(\\mA), \\quad \\vv\\in\\set{N}(\\mA^\\intercal) \\vx = \\vu + \\vv, \\quad \\vu \\in \\set{R}(\\mA), \\quad \\vv\\in\\set{N}(\\mA^\\intercal) with \\vu \\vu and \\vv \\vv uniquely determined. This is illustrated in the figure below: Here, \\vx_{LS} \\vx_{LS} is the least squares solution, \\mA = \\begin{bmatrix}\\va_1&\\va_2&\\dots&\\va_n\\end{bmatrix} \\mA = \\begin{bmatrix}\\va_1&\\va_2&\\dots&\\va_n\\end{bmatrix} , with \\va_i \\in \\R^m \\va_i \\in \\R^m for all i i . Comparing with \\eqref{least_squares_FTLA}, we get \\vb = \\mA\\vx_{LS}+r \\text{ with } \\mA\\vx_{LS} \\in \\set{R}(\\mA) \\text{ and } \\vr \\in \\set{N}(\\mA^\\intercal) \\vb = \\mA\\vx_{LS}+r \\text{ with } \\mA\\vx_{LS} \\in \\set{R}(\\mA) \\text{ and } \\vr \\in \\set{N}(\\mA^\\intercal) \\exa{1} \\exa{1} What is the least-squares solution \\vx^* \\vx^* for the problem \\min_{\\vx} \\frac{1}{2} \\|\\mA\\vx - \\vb\\|_2^2, \\min_{\\vx} \\frac{1}{2} \\|\\mA\\vx - \\vb\\|_2^2, where \\mA = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = \\ve \\in \\R^m \\quad \\text{ and } \\quad \\vb = \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix}. \\mA = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = \\ve \\in \\R^m \\quad \\text{ and } \\quad \\vb = \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix}. \\text{Solution:} \\text{Solution:} First setup the normal equations: \\mA^\\intercal\\mA\\vx = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\vx = m\\vx \\quad \\text {and} \\quad \\mA^\\intercal\\vb = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix} =\\ve^\\intercal\\vb \\mA^\\intercal\\mA\\vx = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\vx = m\\vx \\quad \\text {and} \\quad \\mA^\\intercal\\vb = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix} =\\ve^\\intercal\\vb Solving the normal equations, we get \\mA^\\intercal\\mA\\vx^* = \\mA^\\intercal\\vb \\iff m\\vx^* = \\ve^\\intercal\\vb \\iff \\vx^* = \\frac{1}{m}\\ve^\\intercal\\vb \\mA^\\intercal\\mA\\vx^* = \\mA^\\intercal\\vb \\iff m\\vx^* = \\ve^\\intercal\\vb \\iff \\vx^* = \\frac{1}{m}\\ve^\\intercal\\vb So, the least squares solution \\vx^* \\vx^* is the mean value of the elements in \\vb \\vb .","title":"Least squares"},{"location":"notes/Least_squares/#least-squares","text":"In this lecture, we will cover least squares for data fitting, linear systems, properties of least squares and QR factorization.","title":"Least Squares"},{"location":"notes/Least_squares/#least-squares-for-data-fitting","text":"Consider the problem of fitting a line to observations y_i y_i gven input z_i z_i for i = 1,\\dots, n i = 1,\\dots, n . In the figure above, the data points seem to follow a linear trend. One way to find the parameters c,s \\in \\R c,s \\in \\R of the linear model f(z) = s\\cdot z + c f(z) = s\\cdot z + c that coresponds to a line of best fit is to minimize the following squared distance subject to a linear constraint: \\begin{align*} \\min_{s,\\,c,\\,\\hat{y}_i \\in \\R} &\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\\\ \\text{subject to } &s\\cdot z_i+c = \\hat{y}_i \\end{align*} \\begin{align*} \\min_{s,\\,c,\\,\\hat{y}_i \\in \\R} &\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\\\ \\text{subject to } &s\\cdot z_i+c = \\hat{y}_i \\end{align*} The above minimization program can be reformulated as a linear least squares problem: \\min_{\\vx} \\|\\mA \\vx - \\vb\\|_2^2 = \\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2, \\min_{\\vx} \\|\\mA \\vx - \\vb\\|_2^2 = \\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2, where \\mA = \\begin{bmatrix}z_1 & 1\\\\ z_2 & 1\\\\ \\vdots&\\vdots\\\\z_n & 1\\end{bmatrix}, \\ \\vb = \\begin{bmatrix}y_1\\\\y_2\\\\\\vdots \\\\y_n\\end{bmatrix}, \\text{ and } \\vx = \\begin{bmatrix}s\\\\c\\end{bmatrix}. \\mA = \\begin{bmatrix}z_1 & 1\\\\ z_2 & 1\\\\ \\vdots&\\vdots\\\\z_n & 1\\end{bmatrix}, \\ \\vb = \\begin{bmatrix}y_1\\\\y_2\\\\\\vdots \\\\y_n\\end{bmatrix}, \\text{ and } \\vx = \\begin{bmatrix}s\\\\c\\end{bmatrix}. Let \\begin{equation}\\label{least_squares_problem} \\func{f}(\\vx):=\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 = \\frac{1}{2}\\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2. \\end{equation} \\begin{equation}\\label{least_squares_problem} \\func{f}(\\vx):=\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 = \\frac{1}{2}\\sum_{i=1}^n(\\va_{i}^\\intercal\\vx-b_i)^2. \\end{equation} The gradient and hessian of \\func{f}(\\vx) \\func{f}(\\vx) are: \\nabla\\func{f}(\\vx) = \\mA^\\intercal(\\mA\\vx-\\vb), \\text{ and } \\nabla^2\\func{f}(\\vx) = \\mA^\\intercal\\mA, \\nabla\\func{f}(\\vx) = \\mA^\\intercal(\\mA\\vx-\\vb), \\text{ and } \\nabla^2\\func{f}(\\vx) = \\mA^\\intercal\\mA, respectively. The Hessian is positive semidefinite for every \\vx \\vx (and is positive definite if \\mA \\mA has full row rank). This implies that the function \\func{f}(\\vx) \\func{f}(\\vx) is convex. Additionally, \\vx = \\vx^{*} \\vx = \\vx^{*} is a critical point if \\begin{equation}\\label{least_square_normal_eqn} \\mA^\\intercal(\\mA\\vx^*-b) = \\vzero. \\end{equation} \\begin{equation}\\label{least_square_normal_eqn} \\mA^\\intercal(\\mA\\vx^*-b) = \\vzero. \\end{equation} Since \\func{f}(\\vx) \\func{f}(\\vx) is convex, \\vx^{*} \\vx^{*} is a global minimizer. Equation \\eqref{least_square_normal_eqn} is called the normal equations of the least squares problem \\eqref{least_squares_problem}. Solving the normal equations, we get the following line of best fit.","title":"Least squares for data fitting"},{"location":"notes/Least_squares/#linear-systems","text":"Consider the problem of solving a linear system of equations. For \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} and \\vb\\in\\R^{m} \\vb\\in\\R^{m} , the linear system of equations \\mA\\vx = \\vb \\mA\\vx = \\vb is: overdetermined if m>n m>n , underdetermined if m< n m< n , or square if m = n m = n . A linear system can have exactly one solution, many solutions, or no solutions: In general, a linear system \\mA\\vx=\\vb \\mA\\vx=\\vb has a solution if \\vb \\in \\text{range} (\\mA) \\vb \\in \\text{range} (\\mA) .","title":"Linear systems"},{"location":"notes/Least_squares/#properties-of-linear-least-squares","text":"Recall that the minimizer \\vx^* \\vx^* to the linear least squares poblem satisfies the normal equations: \\mA^\\intercal \\mA \\vx^* = \\mA^\\intercal\\vb \\mA^\\intercal \\mA \\vx^* = \\mA^\\intercal\\vb with the residual \\vr^* = \\mA\\vx^* -\\vb, \\vr^* = \\mA\\vx^* -\\vb, satisfying \\mA^\\intercal\\vr^* = \\vzero. \\mA^\\intercal\\vr^* = \\vzero. Here, \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} . The minimzer of the linear least squares problem is unique if \\mA^\\intercal\\mA \\mA^\\intercal\\mA is invertible. However, the vector in the range of \\mA \\mA closest to \\vb \\vb is unique, i.e. \\vb^* = \\mA\\vx* \\vb^* = \\mA\\vx* is unique. Recall that range space of \\mA \\mA and the null space of \\mA^\\intercal \\mA^\\intercal is: \\text{range}(\\mA) = \\set{R}(\\mA) = \\{\\vy : \\vy = \\mA \\vx \\text{ for some } \\vx \\}\\\\ \\text{null}(\\mA^\\intercal)= \\set{N}(\\mA^\\intercal) = \\{\\vz: \\mA^\\intercal\\vz = \\vzero\\} \\text{range}(\\mA) = \\set{R}(\\mA) = \\{\\vy : \\vy = \\mA \\vx \\text{ for some } \\vx \\}\\\\ \\text{null}(\\mA^\\intercal)= \\set{N}(\\mA^\\intercal) = \\{\\vz: \\mA^\\intercal\\vz = \\vzero\\} By fundamental theorem of linear algebra, we have $$ \\begin{equation}\\label{least_squares_FTLA} \\set{R}(\\mA) \\oplus \\set{N}(\\mA^\\intercal) = \\R^m. \\end{equation} $$ Thus, for all \\vx \\in \\R^m \\vx \\in \\R^m , we have \\vx = \\vu + \\vv, \\quad \\vu \\in \\set{R}(\\mA), \\quad \\vv\\in\\set{N}(\\mA^\\intercal) \\vx = \\vu + \\vv, \\quad \\vu \\in \\set{R}(\\mA), \\quad \\vv\\in\\set{N}(\\mA^\\intercal) with \\vu \\vu and \\vv \\vv uniquely determined. This is illustrated in the figure below: Here, \\vx_{LS} \\vx_{LS} is the least squares solution, \\mA = \\begin{bmatrix}\\va_1&\\va_2&\\dots&\\va_n\\end{bmatrix} \\mA = \\begin{bmatrix}\\va_1&\\va_2&\\dots&\\va_n\\end{bmatrix} , with \\va_i \\in \\R^m \\va_i \\in \\R^m for all i i . Comparing with \\eqref{least_squares_FTLA}, we get \\vb = \\mA\\vx_{LS}+r \\text{ with } \\mA\\vx_{LS} \\in \\set{R}(\\mA) \\text{ and } \\vr \\in \\set{N}(\\mA^\\intercal) \\vb = \\mA\\vx_{LS}+r \\text{ with } \\mA\\vx_{LS} \\in \\set{R}(\\mA) \\text{ and } \\vr \\in \\set{N}(\\mA^\\intercal) \\exa{1} \\exa{1} What is the least-squares solution \\vx^* \\vx^* for the problem \\min_{\\vx} \\frac{1}{2} \\|\\mA\\vx - \\vb\\|_2^2, \\min_{\\vx} \\frac{1}{2} \\|\\mA\\vx - \\vb\\|_2^2, where \\mA = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = \\ve \\in \\R^m \\quad \\text{ and } \\quad \\vb = \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix}. \\mA = \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = \\ve \\in \\R^m \\quad \\text{ and } \\quad \\vb = \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix}. \\text{Solution:} \\text{Solution:} First setup the normal equations: \\mA^\\intercal\\mA\\vx = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\vx = m\\vx \\quad \\text {and} \\quad \\mA^\\intercal\\vb = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix} =\\ve^\\intercal\\vb \\mA^\\intercal\\mA\\vx = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\\vx = m\\vx \\quad \\text {and} \\quad \\mA^\\intercal\\vb = \\begin{bmatrix}1&\\dots&1\\end{bmatrix} \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{bmatrix} =\\ve^\\intercal\\vb Solving the normal equations, we get \\mA^\\intercal\\mA\\vx^* = \\mA^\\intercal\\vb \\iff m\\vx^* = \\ve^\\intercal\\vb \\iff \\vx^* = \\frac{1}{m}\\ve^\\intercal\\vb \\mA^\\intercal\\mA\\vx^* = \\mA^\\intercal\\vb \\iff m\\vx^* = \\ve^\\intercal\\vb \\iff \\vx^* = \\frac{1}{m}\\ve^\\intercal\\vb So, the least squares solution \\vx^* \\vx^* is the mean value of the elements in \\vb \\vb .","title":"Properties of linear least squares"},{"location":"notes/Linear_constraint/","text":"Linear Constraint \u00b6 In this lecture, we consider optimization problem where the unknown variable satisfies a linear constraint. Let f:\\R^n \u2192 \\R f:\\R^n \u2192 \\R be a continuously differentiable real valued function and \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} be a matrix with m\\le n m\\le n . The linearly-constrained optimization problem is \\begin{equation}\\label{linear_constraint} \\minimize_{\\vx\\in\\R^n} f(\\vx) \\text{ subject to } \\mA\\vx = \\vb, \\end{equation} \\begin{equation}\\label{linear_constraint} \\minimize_{\\vx\\in\\R^n} f(\\vx) \\text{ subject to } \\mA\\vx = \\vb, \\end{equation} where \\vb \\in \\R^m \\vb \\in \\R^m is a fixed vector. We assume that the matrix \\mA \\mA has full row rank and the minimum value is finite and attained. The figure below shows interaction between the objective function f f and the feasible set \\mathcal{F} \\mathcal{F} . The feasible set is the set of point that satisfy the constraint. Reduced unconstrained problem \u00b6 In the case of linear constraints as in \\eqref{linear_constraint}, we can reformulate the problem into an unconstrained problem. In order to write the unconstrained problem, we note that the feasible set \\mathcal{F} = \\{\\vx \\in \\R^n| \\mA\\vx=\\vb\\} \\mathcal{F} = \\{\\vx \\in \\R^n| \\mA\\vx=\\vb\\} can we equivalent expressed as the set \\{\\bar{\\vx} + \\mZ \\vp |\\vp \\in \\R^{n-m}\\} \\{\\bar{\\vx} + \\mZ \\vp |\\vp \\in \\R^{n-m}\\} , where \\bar{x} \\bar{x} is any particular feasible solution , i.e. \\bar{\\vx} \\bar{\\vx} satisfies \\mA\\bar{\\vx} = \\vb \\mA\\bar{\\vx} = \\vb , and \\mZ \\mZ is a basis for \\vnull(\\mA) \\vnull(\\mA) . So, any \\vx \\vx that satisfies the linear constraint in \\eqref{linear_constraint} can be expressed as \\bar{\\vx}+\\mZ\\vp \\bar{\\vx}+\\mZ\\vp for some \\vp \\in \\R^{n-m} \\vp \\in \\R^{n-m} . This reduces the linear constrained problem \\eqref{linear_constraint} to an unconstrained problem in n-m n-m variables: \\begin{equation}\\label{reduced_unconstrained} \\minimize_{\\vp\\in\\R^{n-m}} f(\\bar{\\vx} + \\mZ\\vp). \\end{equation} \\begin{equation}\\label{reduced_unconstrained} \\minimize_{\\vp\\in\\R^{n-m}} f(\\bar{\\vx} + \\mZ\\vp). \\end{equation} The above unconstrained problem can be solved to get an optimal \\vp^* \\vp^* via any unconstrained method. The corresponding optimal solution to the constrained problem is then \\vx^* = \\bar{\\vx}+\\mZ\\vp^* \\vx^* = \\bar{\\vx}+\\mZ\\vp^* . Optimality Conditions \u00b6 Define \"reduced\" objective function for any particular solution \\bar{\\vx} \\bar{\\vx} and basis for null space \\mZ \\mZ as f_\\vz(\\vp): = f(\\bar{\\vx} + \\mZ\\vp) f_\\vz(\\vp): = f(\\bar{\\vx} + \\mZ\\vp) . THe gradient of f_\\vx f_\\vx at \\vp \\vp is \\nabla f_\\vz(\\vp) = \\mZ\\trans\\nabla f(\\bar{\\vx} + \\mZ+\\vp). \\nabla f_\\vz(\\vp) = \\mZ\\trans\\nabla f(\\bar{\\vx} + \\mZ+\\vp). The quantity \\nabla f_\\vz \\nabla f_\\vz is called the reduced gradient. Let \\vp^* \\vp^* be an arbitrary point and set \\vx^* = \\bar{\\vx}+\\mZ\\vp^* \\vx^* = \\bar{\\vx}+\\mZ\\vp^* . Then \\vp^* \\vp^* is optimal only if \\nabla f_\\vz(\\vp^*) = 0 \u21d4 \\mZ\\trans\\nabla f(\\vx^*) = 0 \\Leftrightarrow \\nabla f(\\vx^*) \\in \\vnull(\\mZ\\trans). \\nabla f_\\vz(\\vp^*) = 0 \u21d4 \\mZ\\trans\\nabla f(\\vx^*) = 0 \\Leftrightarrow \\nabla f(\\vx^*) \\in \\vnull(\\mZ\\trans). By construction of \\mZ \\mZ , we have \\vnull(\\mZ) = \\range(\\mA) \\vnull(\\mZ) = \\range(\\mA) . Also, note that the fundamental subspaces in \\R^n \\R^n associated with \\mA \\mA and \\mZ \\mZ satisfy \\begin{align*} & \\range(\\mA\\trans)\\oplus \\vnull(\\mA) = \\R^n,\\text{ and}\\\\ & \\vnull(\\mZ\\trans)\\oplus \\range(\\mZ) = \\R^n. \\end{align*} \\begin{align*} & \\range(\\mA\\trans)\\oplus \\vnull(\\mA) = \\R^n,\\text{ and}\\\\ & \\vnull(\\mZ\\trans)\\oplus \\range(\\mZ) = \\R^n. \\end{align*} Thus, \\nabla f(\\vx^*) \\in \\vnull(\\mZ\\trans) \\nabla f(\\vx^*) \\in \\vnull(\\mZ\\trans) if and only if \\nabla f(\\vx^*) \\in \\range(\\mA\\trans) \\nabla f(\\vx^*) \\in \\range(\\mA\\trans) . Further, \\nabla f(\\vx^*) \\in \\range(\\mA\\trans) \\nabla f(\\vx^*) \\in \\range(\\mA\\trans) if and only if there exist a \\vy \\vy such that \\nabla f(\\vx^*) = \\mA\\trans\\vy \\nabla f(\\vx^*) = \\mA\\trans\\vy . These relations lead to the following first-order necessary conditions. Lemma (First order necessary condition) A point \\vx^* \\vx^* is a local minimizer of \\eqref{linear_constraint} only if there exists an m m -vector \\vy \\vy such that (Optimality) \\nabla f(\\vx^*) = \\mA\\trans\\vy = \\sum_{i=1}^m \\va_i y_i \\nabla f(\\vx^*) = \\mA\\trans\\vy = \\sum_{i=1}^m \\va_i y_i . Here, \\va_i \\va_i is the i i th row of \\mA \\mA . (Feasibility) \\mA\\vx^* = \\vb \\mA\\vx^* = \\vb . Note that the optimality statement in the above lemma can be equivalently expressed as \\mZ\\trans\\nabla f(\\vx^*) = 0 \\mZ\\trans\\nabla f(\\vx^*) = 0 , which is further equivalent to \\nabla f(\\vx^*)\\trans\\vp = 0 \\nabla f(\\vx^*)\\trans\\vp = 0 for all \\vp \\in\\vnull(A) \\vp \\in\\vnull(A) . The vector y y is sometimes referred to as \"Lagrange multipliers\". Note that the Hessian of f_\\vz f_\\vz at \\vp \\vp is \\nabla^2 f_\\vz(\\vp) = \\mZ\\trans\\nabla^2f(\\bar{\\vx}+\\mZ\\vp)\\mZ \\nabla^2 f_\\vz(\\vp) = \\mZ\\trans\\nabla^2f(\\bar{\\vx}+\\mZ\\vp)\\mZ . We now state the necessary second-order condition. Lemma (Second-order necessary condition) A point \\vx^* \\vx^* is a local minimizer only if \\mA\\vx^* = \\vb \\mA\\vx^* = \\vb and \\nabla^2f_\\vz(\\vx^*)\u2ab0 0 \\nabla^2f_\\vz(\\vx^*)\u2ab0 0 . Equivalently, \\vx^* \\vx^* is a local minimizer only if \\mA\\vx^* = \\vb \\mA\\vx^* = \\vb and \\mZ\\trans\\nabla^2f(\\vx^*)\\mZ \\succeq 0 \\mZ\\trans\\nabla^2f(\\vx^*)\\mZ \\succeq 0 . We now state the sufficient second-order optimality condition. Lemma (Second-order sufficient condition) A point \\vx^* \\vx^* is a local minimizer if it satisfies the following conditions: (Feasibility) \\mA\\vx^* = \\vb \\mA\\vx^* = \\vb , (Stationary) \\nabla f_\\vz(\\vx^*) = 0 \\nabla f_\\vz(\\vx^*) = 0 , and (Positivity) \\nabla^2f_\\vz(\\vx^*) \\succ 0 \\nabla^2f_\\vz(\\vx^*) \\succ 0 . Equivalently, \\vx^* \\vx^* is a local minimizer if it satisfies the following conditions: (Feasibility) \\mA\\vx^* = \\vb \\mA\\vx^* = \\vb , (Stationary) \\nabla f(\\vx^*) = \\mA\\trans\\vy \\nabla f(\\vx^*) = \\mA\\trans\\vy for some \\vy \\vy , and (Positivity) \\vp\\trans\\nabla^2f(\\vx^*)\\vp>0 \\vp\\trans\\nabla^2f(\\vx^*)\\vp>0 for all \\vp \\in \\vnull(A)\\backslash \\{\\vzero\\} \\vp \\in \\vnull(A)\\backslash \\{\\vzero\\} . Example \u00b6 (Least norm solutions) Consider the following problem: \\minimize_{\\vx} \\|\\vx\\|_2 \\text{ subject to } \\mA\\vx=\\vb. \\minimize_{\\vx} \\|\\vx\\|_2 \\text{ subject to } \\mA\\vx=\\vb. To find the least norm solution, we first state the corresponding first-order optimality condition. Without loss of generality, we may assume that the objective is f(\\vx) = \\frac{1}{2}\\|\\vx\\|_2^2 f(\\vx) = \\frac{1}{2}\\|\\vx\\|_2^2 . Since \\nabla f(\\vx) = \\vx \\nabla f(\\vx) = \\vx , first-order optimality is \\left. \\begin{array}{c} &\\vx = \\mA\\trans\\vy \\text{ for some } \\vy\\\\ &\\mA\\vx = \\vb \\end{array} \\right\\} \u21d4 \\bmat-\\mI & \\mA\\trans\\\\ \\mA & \\vzero\\emat\\bmat\\vx\\\\ \\vy\\emat = \\bmat\\vzero\\\\ \\vb\\emat. \\left. \\begin{array}{c} &\\vx = \\mA\\trans\\vy \\text{ for some } \\vy\\\\ &\\mA\\vx = \\vb \\end{array} \\right\\} \u21d4 \\bmat-\\mI & \\mA\\trans\\\\ \\mA & \\vzero\\emat\\bmat\\vx\\\\ \\vy\\emat = \\bmat\\vzero\\\\ \\vb\\emat. So, if the constraint set \\mA\\vx = \\vb \\mA\\vx = \\vb is x_1 + x_2 + \\dots + x_n = 1 x_1 + x_2 + \\dots + x_n = 1 , then the first order optimality condition yields \\bmat-\\mI & \\ve \\trans\\\\ \\ve\\trans & 0\\emat\\bmat\\vx\\\\ y\\emat = \\bmat 0\\\\ 1\\emat \\Rightarrow \\left\\{\\begin{array}{c} &-vx +\\ve y = 0\\\\ &\\ve\\trans\\vx = 1 \\end{array}\\right\\} \\Rightarrow \\ve\\trans\\ve\\vy = 1 \u21d2 y = \\frac{1}{n}. \\bmat-\\mI & \\ve \\trans\\\\ \\ve\\trans & 0\\emat\\bmat\\vx\\\\ y\\emat = \\bmat 0\\\\ 1\\emat \\Rightarrow \\left\\{\\begin{array}{c} &-vx +\\ve y = 0\\\\ &\\ve\\trans\\vx = 1 \\end{array}\\right\\} \\Rightarrow \\ve\\trans\\ve\\vy = 1 \u21d2 y = \\frac{1}{n}. Thus, the minimizer is \\vx = \\frac{1}{n} \\ve \\vx = \\frac{1}{n} \\ve . Algorithm \u00b6 The reduced gradient method for solving linear constraint optimization problem is: Algorithm (Reduced gradient method) Given \\vx_0 \\vx_0 feasible and \\mZ \\mZ a basis for \\vnull(\\mA) \\vnull(\\mA) For k = 0, 1, 2, \\dots k = 0, 1, 2, \\dots Compute \\vg := \\nabla f(\\vx_k) \\vg := \\nabla f(\\vx_k) . Compute \\mH := \\nabla^2f(\\vx_k) \\mH := \\nabla^2f(\\vx_k) . Solve \\mZ\\trans\\mH\\mZ\\vp = -\\mZ\\trans\\vg \\mZ\\trans\\mH\\mZ\\vp = -\\mZ\\trans\\vg to get \\vp_k \\vp_k . Linesearch on f(\\vx_k + \\alpha \\mZ\\vp_k) f(\\vx_k + \\alpha \\mZ\\vp_k) . \\vx_{k+1} = \\vx_k + \\alpha_k\\mZ\\vp_k \\vx_{k+1} = \\vx_k + \\alpha_k\\mZ\\vp_k . Repeat until convergence. The algorithm requires an initial feasible point and a basis for the null space of \\mA \\mA . An initial feasible point is easy to obtain (for example, the least squares solution). One approach to obtain a basis for the null space is by considering the QR decomposition of \\mA\\trans \\mA\\trans . We will present an alternate way to construct the basis for the null space. Permute the columns of \\mA \\mA so that the matrix \\mA \\mA can be represented as \\mA = \\bmat \\mB & \\mN\\emat, \\mA = \\bmat \\mB & \\mN\\emat, where \\mB \\mB is non-singular. Such a matrix exists because \\mA \\mA is full rank. The matrix \\mB \\mB is called a basic matrix and \\mN \\mN is called the non-basic matrix. Then feasibility requires \\mA\\vx =\\vb \u21d4 \\bmat\\mB &\\mN\\emat\\bmat\\vx_B\\\\ \\vx_N\\emat \\Leftrightarrow \\mB\\vx_B + \\mN\\vx_N = \\vb. \\mA\\vx =\\vb \u21d4 \\bmat\\mB &\\mN\\emat\\bmat\\vx_B\\\\ \\vx_N\\emat \\Leftrightarrow \\mB\\vx_B + \\mN\\vx_N = \\vb. The variables \\vx_B \\vx_B and \\vx_N \\vx_N are called basic variable and non-basic variable respectively. Note that \\vx_B \\vx_B is uniquely determined by \\vx_N \\vx_N , i.e. \\vx_B = \\mB^{-1}(b-\\mN\\vx_N) \\vx_B = \\mB^{-1}(b-\\mN\\vx_N) , and \\vx_N \\vx_N is free to move. By construction, the matrix \\mZ = \\bmat-\\mB^{-1}\\mN\\\\ \\mI\\emat \\mZ = \\bmat-\\mB^{-1}\\mN\\\\ \\mI\\emat is a basis for the null space of \\mA \\mA . This is because \\mA\\mZ = \\bmat\\mB&\\mN\\emat\\bmat-\\mB^{-1}\\mN \\\\ \\mI\\emat = 0 \\mA\\mZ = \\bmat\\mB&\\mN\\emat\\bmat-\\mB^{-1}\\mN \\\\ \\mI\\emat = 0 .","title":"Linear constraint"},{"location":"notes/Linear_constraint/#linear-constraint","text":"In this lecture, we consider optimization problem where the unknown variable satisfies a linear constraint. Let f:\\R^n \u2192 \\R f:\\R^n \u2192 \\R be a continuously differentiable real valued function and \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} be a matrix with m\\le n m\\le n . The linearly-constrained optimization problem is \\begin{equation}\\label{linear_constraint} \\minimize_{\\vx\\in\\R^n} f(\\vx) \\text{ subject to } \\mA\\vx = \\vb, \\end{equation} \\begin{equation}\\label{linear_constraint} \\minimize_{\\vx\\in\\R^n} f(\\vx) \\text{ subject to } \\mA\\vx = \\vb, \\end{equation} where \\vb \\in \\R^m \\vb \\in \\R^m is a fixed vector. We assume that the matrix \\mA \\mA has full row rank and the minimum value is finite and attained. The figure below shows interaction between the objective function f f and the feasible set \\mathcal{F} \\mathcal{F} . The feasible set is the set of point that satisfy the constraint.","title":"Linear Constraint"},{"location":"notes/Linear_constraint/#reduced-unconstrained-problem","text":"In the case of linear constraints as in \\eqref{linear_constraint}, we can reformulate the problem into an unconstrained problem. In order to write the unconstrained problem, we note that the feasible set \\mathcal{F} = \\{\\vx \\in \\R^n| \\mA\\vx=\\vb\\} \\mathcal{F} = \\{\\vx \\in \\R^n| \\mA\\vx=\\vb\\} can we equivalent expressed as the set \\{\\bar{\\vx} + \\mZ \\vp |\\vp \\in \\R^{n-m}\\} \\{\\bar{\\vx} + \\mZ \\vp |\\vp \\in \\R^{n-m}\\} , where \\bar{x} \\bar{x} is any particular feasible solution , i.e. \\bar{\\vx} \\bar{\\vx} satisfies \\mA\\bar{\\vx} = \\vb \\mA\\bar{\\vx} = \\vb , and \\mZ \\mZ is a basis for \\vnull(\\mA) \\vnull(\\mA) . So, any \\vx \\vx that satisfies the linear constraint in \\eqref{linear_constraint} can be expressed as \\bar{\\vx}+\\mZ\\vp \\bar{\\vx}+\\mZ\\vp for some \\vp \\in \\R^{n-m} \\vp \\in \\R^{n-m} . This reduces the linear constrained problem \\eqref{linear_constraint} to an unconstrained problem in n-m n-m variables: \\begin{equation}\\label{reduced_unconstrained} \\minimize_{\\vp\\in\\R^{n-m}} f(\\bar{\\vx} + \\mZ\\vp). \\end{equation} \\begin{equation}\\label{reduced_unconstrained} \\minimize_{\\vp\\in\\R^{n-m}} f(\\bar{\\vx} + \\mZ\\vp). \\end{equation} The above unconstrained problem can be solved to get an optimal \\vp^* \\vp^* via any unconstrained method. The corresponding optimal solution to the constrained problem is then \\vx^* = \\bar{\\vx}+\\mZ\\vp^* \\vx^* = \\bar{\\vx}+\\mZ\\vp^* .","title":"Reduced unconstrained problem"},{"location":"notes/Linear_constraint/#optimality-conditions","text":"Define \"reduced\" objective function for any particular solution \\bar{\\vx} \\bar{\\vx} and basis for null space \\mZ \\mZ as f_\\vz(\\vp): = f(\\bar{\\vx} + \\mZ\\vp) f_\\vz(\\vp): = f(\\bar{\\vx} + \\mZ\\vp) . THe gradient of f_\\vx f_\\vx at \\vp \\vp is \\nabla f_\\vz(\\vp) = \\mZ\\trans\\nabla f(\\bar{\\vx} + \\mZ+\\vp). \\nabla f_\\vz(\\vp) = \\mZ\\trans\\nabla f(\\bar{\\vx} + \\mZ+\\vp). The quantity \\nabla f_\\vz \\nabla f_\\vz is called the reduced gradient. Let \\vp^* \\vp^* be an arbitrary point and set \\vx^* = \\bar{\\vx}+\\mZ\\vp^* \\vx^* = \\bar{\\vx}+\\mZ\\vp^* . Then \\vp^* \\vp^* is optimal only if \\nabla f_\\vz(\\vp^*) = 0 \u21d4 \\mZ\\trans\\nabla f(\\vx^*) = 0 \\Leftrightarrow \\nabla f(\\vx^*) \\in \\vnull(\\mZ\\trans). \\nabla f_\\vz(\\vp^*) = 0 \u21d4 \\mZ\\trans\\nabla f(\\vx^*) = 0 \\Leftrightarrow \\nabla f(\\vx^*) \\in \\vnull(\\mZ\\trans). By construction of \\mZ \\mZ , we have \\vnull(\\mZ) = \\range(\\mA) \\vnull(\\mZ) = \\range(\\mA) . Also, note that the fundamental subspaces in \\R^n \\R^n associated with \\mA \\mA and \\mZ \\mZ satisfy \\begin{align*} & \\range(\\mA\\trans)\\oplus \\vnull(\\mA) = \\R^n,\\text{ and}\\\\ & \\vnull(\\mZ\\trans)\\oplus \\range(\\mZ) = \\R^n. \\end{align*} \\begin{align*} & \\range(\\mA\\trans)\\oplus \\vnull(\\mA) = \\R^n,\\text{ and}\\\\ & \\vnull(\\mZ\\trans)\\oplus \\range(\\mZ) = \\R^n. \\end{align*} Thus, \\nabla f(\\vx^*) \\in \\vnull(\\mZ\\trans) \\nabla f(\\vx^*) \\in \\vnull(\\mZ\\trans) if and only if \\nabla f(\\vx^*) \\in \\range(\\mA\\trans) \\nabla f(\\vx^*) \\in \\range(\\mA\\trans) . Further, \\nabla f(\\vx^*) \\in \\range(\\mA\\trans) \\nabla f(\\vx^*) \\in \\range(\\mA\\trans) if and only if there exist a \\vy \\vy such that \\nabla f(\\vx^*) = \\mA\\trans\\vy \\nabla f(\\vx^*) = \\mA\\trans\\vy . These relations lead to the following first-order necessary conditions. Lemma (First order necessary condition) A point \\vx^* \\vx^* is a local minimizer of \\eqref{linear_constraint} only if there exists an m m -vector \\vy \\vy such that (Optimality) \\nabla f(\\vx^*) = \\mA\\trans\\vy = \\sum_{i=1}^m \\va_i y_i \\nabla f(\\vx^*) = \\mA\\trans\\vy = \\sum_{i=1}^m \\va_i y_i . Here, \\va_i \\va_i is the i i th row of \\mA \\mA . (Feasibility) \\mA\\vx^* = \\vb \\mA\\vx^* = \\vb . Note that the optimality statement in the above lemma can be equivalently expressed as \\mZ\\trans\\nabla f(\\vx^*) = 0 \\mZ\\trans\\nabla f(\\vx^*) = 0 , which is further equivalent to \\nabla f(\\vx^*)\\trans\\vp = 0 \\nabla f(\\vx^*)\\trans\\vp = 0 for all \\vp \\in\\vnull(A) \\vp \\in\\vnull(A) . The vector y y is sometimes referred to as \"Lagrange multipliers\". Note that the Hessian of f_\\vz f_\\vz at \\vp \\vp is \\nabla^2 f_\\vz(\\vp) = \\mZ\\trans\\nabla^2f(\\bar{\\vx}+\\mZ\\vp)\\mZ \\nabla^2 f_\\vz(\\vp) = \\mZ\\trans\\nabla^2f(\\bar{\\vx}+\\mZ\\vp)\\mZ . We now state the necessary second-order condition. Lemma (Second-order necessary condition) A point \\vx^* \\vx^* is a local minimizer only if \\mA\\vx^* = \\vb \\mA\\vx^* = \\vb and \\nabla^2f_\\vz(\\vx^*)\u2ab0 0 \\nabla^2f_\\vz(\\vx^*)\u2ab0 0 . Equivalently, \\vx^* \\vx^* is a local minimizer only if \\mA\\vx^* = \\vb \\mA\\vx^* = \\vb and \\mZ\\trans\\nabla^2f(\\vx^*)\\mZ \\succeq 0 \\mZ\\trans\\nabla^2f(\\vx^*)\\mZ \\succeq 0 . We now state the sufficient second-order optimality condition. Lemma (Second-order sufficient condition) A point \\vx^* \\vx^* is a local minimizer if it satisfies the following conditions: (Feasibility) \\mA\\vx^* = \\vb \\mA\\vx^* = \\vb , (Stationary) \\nabla f_\\vz(\\vx^*) = 0 \\nabla f_\\vz(\\vx^*) = 0 , and (Positivity) \\nabla^2f_\\vz(\\vx^*) \\succ 0 \\nabla^2f_\\vz(\\vx^*) \\succ 0 . Equivalently, \\vx^* \\vx^* is a local minimizer if it satisfies the following conditions: (Feasibility) \\mA\\vx^* = \\vb \\mA\\vx^* = \\vb , (Stationary) \\nabla f(\\vx^*) = \\mA\\trans\\vy \\nabla f(\\vx^*) = \\mA\\trans\\vy for some \\vy \\vy , and (Positivity) \\vp\\trans\\nabla^2f(\\vx^*)\\vp>0 \\vp\\trans\\nabla^2f(\\vx^*)\\vp>0 for all \\vp \\in \\vnull(A)\\backslash \\{\\vzero\\} \\vp \\in \\vnull(A)\\backslash \\{\\vzero\\} .","title":"Optimality Conditions"},{"location":"notes/Linear_constraint/#example","text":"(Least norm solutions) Consider the following problem: \\minimize_{\\vx} \\|\\vx\\|_2 \\text{ subject to } \\mA\\vx=\\vb. \\minimize_{\\vx} \\|\\vx\\|_2 \\text{ subject to } \\mA\\vx=\\vb. To find the least norm solution, we first state the corresponding first-order optimality condition. Without loss of generality, we may assume that the objective is f(\\vx) = \\frac{1}{2}\\|\\vx\\|_2^2 f(\\vx) = \\frac{1}{2}\\|\\vx\\|_2^2 . Since \\nabla f(\\vx) = \\vx \\nabla f(\\vx) = \\vx , first-order optimality is \\left. \\begin{array}{c} &\\vx = \\mA\\trans\\vy \\text{ for some } \\vy\\\\ &\\mA\\vx = \\vb \\end{array} \\right\\} \u21d4 \\bmat-\\mI & \\mA\\trans\\\\ \\mA & \\vzero\\emat\\bmat\\vx\\\\ \\vy\\emat = \\bmat\\vzero\\\\ \\vb\\emat. \\left. \\begin{array}{c} &\\vx = \\mA\\trans\\vy \\text{ for some } \\vy\\\\ &\\mA\\vx = \\vb \\end{array} \\right\\} \u21d4 \\bmat-\\mI & \\mA\\trans\\\\ \\mA & \\vzero\\emat\\bmat\\vx\\\\ \\vy\\emat = \\bmat\\vzero\\\\ \\vb\\emat. So, if the constraint set \\mA\\vx = \\vb \\mA\\vx = \\vb is x_1 + x_2 + \\dots + x_n = 1 x_1 + x_2 + \\dots + x_n = 1 , then the first order optimality condition yields \\bmat-\\mI & \\ve \\trans\\\\ \\ve\\trans & 0\\emat\\bmat\\vx\\\\ y\\emat = \\bmat 0\\\\ 1\\emat \\Rightarrow \\left\\{\\begin{array}{c} &-vx +\\ve y = 0\\\\ &\\ve\\trans\\vx = 1 \\end{array}\\right\\} \\Rightarrow \\ve\\trans\\ve\\vy = 1 \u21d2 y = \\frac{1}{n}. \\bmat-\\mI & \\ve \\trans\\\\ \\ve\\trans & 0\\emat\\bmat\\vx\\\\ y\\emat = \\bmat 0\\\\ 1\\emat \\Rightarrow \\left\\{\\begin{array}{c} &-vx +\\ve y = 0\\\\ &\\ve\\trans\\vx = 1 \\end{array}\\right\\} \\Rightarrow \\ve\\trans\\ve\\vy = 1 \u21d2 y = \\frac{1}{n}. Thus, the minimizer is \\vx = \\frac{1}{n} \\ve \\vx = \\frac{1}{n} \\ve .","title":"Example"},{"location":"notes/Linear_constraint/#algorithm","text":"The reduced gradient method for solving linear constraint optimization problem is: Algorithm (Reduced gradient method) Given \\vx_0 \\vx_0 feasible and \\mZ \\mZ a basis for \\vnull(\\mA) \\vnull(\\mA) For k = 0, 1, 2, \\dots k = 0, 1, 2, \\dots Compute \\vg := \\nabla f(\\vx_k) \\vg := \\nabla f(\\vx_k) . Compute \\mH := \\nabla^2f(\\vx_k) \\mH := \\nabla^2f(\\vx_k) . Solve \\mZ\\trans\\mH\\mZ\\vp = -\\mZ\\trans\\vg \\mZ\\trans\\mH\\mZ\\vp = -\\mZ\\trans\\vg to get \\vp_k \\vp_k . Linesearch on f(\\vx_k + \\alpha \\mZ\\vp_k) f(\\vx_k + \\alpha \\mZ\\vp_k) . \\vx_{k+1} = \\vx_k + \\alpha_k\\mZ\\vp_k \\vx_{k+1} = \\vx_k + \\alpha_k\\mZ\\vp_k . Repeat until convergence. The algorithm requires an initial feasible point and a basis for the null space of \\mA \\mA . An initial feasible point is easy to obtain (for example, the least squares solution). One approach to obtain a basis for the null space is by considering the QR decomposition of \\mA\\trans \\mA\\trans . We will present an alternate way to construct the basis for the null space. Permute the columns of \\mA \\mA so that the matrix \\mA \\mA can be represented as \\mA = \\bmat \\mB & \\mN\\emat, \\mA = \\bmat \\mB & \\mN\\emat, where \\mB \\mB is non-singular. Such a matrix exists because \\mA \\mA is full rank. The matrix \\mB \\mB is called a basic matrix and \\mN \\mN is called the non-basic matrix. Then feasibility requires \\mA\\vx =\\vb \u21d4 \\bmat\\mB &\\mN\\emat\\bmat\\vx_B\\\\ \\vx_N\\emat \\Leftrightarrow \\mB\\vx_B + \\mN\\vx_N = \\vb. \\mA\\vx =\\vb \u21d4 \\bmat\\mB &\\mN\\emat\\bmat\\vx_B\\\\ \\vx_N\\emat \\Leftrightarrow \\mB\\vx_B + \\mN\\vx_N = \\vb. The variables \\vx_B \\vx_B and \\vx_N \\vx_N are called basic variable and non-basic variable respectively. Note that \\vx_B \\vx_B is uniquely determined by \\vx_N \\vx_N , i.e. \\vx_B = \\mB^{-1}(b-\\mN\\vx_N) \\vx_B = \\mB^{-1}(b-\\mN\\vx_N) , and \\vx_N \\vx_N is free to move. By construction, the matrix \\mZ = \\bmat-\\mB^{-1}\\mN\\\\ \\mI\\emat \\mZ = \\bmat-\\mB^{-1}\\mN\\\\ \\mI\\emat is a basis for the null space of \\mA \\mA . This is because \\mA\\mZ = \\bmat\\mB&\\mN\\emat\\bmat-\\mB^{-1}\\mN \\\\ \\mI\\emat = 0 \\mA\\mZ = \\bmat\\mB&\\mN\\emat\\bmat-\\mB^{-1}\\mN \\\\ \\mI\\emat = 0 .","title":"Algorithm"},{"location":"notes/Linear_programming/","text":"Linear Programming \u00b6 Linear programming solves the following minimization program \\begin{equation}\\label{non_stand_LP} \\min_{\\vx\\in\\R^n} \\vc\\trans\\vx \\text{ subject to } \\mA\\vx\\leq \\vb, \\end{equation} \\begin{equation}\\label{non_stand_LP} \\min_{\\vx\\in\\R^n} \\vc\\trans\\vx \\text{ subject to } \\mA\\vx\\leq \\vb, \\end{equation} where \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} , \\vc \\in \\R^n \\vc \\in \\R^n , and \\vb \\in \\R^m \\vb \\in \\R^m are known a priori. In general, the optimization problem \\eqref{non_stand_LP} has three possibilities: The problem is infeasible, i.e. there exist no such \\vx \\in \\R^n \\vx \\in \\R^n such that \\mA\\vx \\leq \\vb \\mA\\vx \\leq \\vb . The problem is unbounded from below, i.e. there exist some \\vx = \\vx_0 +\\alpha \\vd \\vx = \\vx_0 +\\alpha \\vd where \\mA\\vx \\leq \\vb \\mA\\vx \\leq \\vb for all \\alpha\\geq 0 \\alpha\\geq 0 , and \\vc\\trans \\vx < 0 \\vc\\trans \\vx < 0 . In this case \\vd \\vd is a direction that pushes the objective value to -\u221e -\u221e while maintaining feasibility. The problem is feasible and has a finite minimum, i.e. there exist a point \\vx \\vx such that \\mA\\vx\\leq \\vb \\mA\\vx\\leq \\vb and the optimal value of \\eqref{non_stand_LP} p^* = \\min\\{\\vc\\trans\\vx | \\mA\\vx\\leq \\vb\\} p^* = \\min\\{\\vc\\trans\\vx | \\mA\\vx\\leq \\vb\\} satisfies p^*>-\u221e p^*>-\u221e . In this lecture, we will assume the problem is always feasible and characterize the optimal points of \\eqref{non_stand_LP}. Extreme points \u00b6 The constraint set in the linear program \\eqref{non_stand_LP} is \\setP := \\{x\\in \\R^n | \\va_i\\trans\\vx \\leq b_i,\\ i = 1\\,\\dots, m\\} \\setP := \\{x\\in \\R^n | \\va_i\\trans\\vx \\leq b_i,\\ i = 1\\,\\dots, m\\} . This set is the intersection of m m half-spaces given by \\va_i\\trans\\vx \\leq b_i \\va_i\\trans\\vx \\leq b_i and form a polyhedron (see figure below). Extreme points can be studied in many equivalent ways. A definition of extreme point is: Definition A point \\vx \\in \\setP \\vx \\in \\setP is an extreme point of \\setP \\setP if there does not exist two vectors \\vy,\\vz \\in \\setP \\vy,\\vz \\in \\setP such that \\vx = \\lambda \\vy + (1-\\lambda)\\vz \\vx = \\lambda \\vy + (1-\\lambda)\\vz for any \\lambda \\in (0,1) \\lambda \\in (0,1) . This definition states that an extreme point can not be in the interior of any line segment contained in the polyhedron. Another geometric way to view extreme points is via vertices of a polyhedron. A point \\bar{\\vx} \\bar{\\vx} is an extreme point of a polyhedron if and only if \\bar{\\vx} \\bar{\\vx} is an vertex. The vertex of a polyhedron is defined as: Definition A point \\vx \\in \\setP \\vx \\in \\setP is a vertex of \\setP \\setP if there exists a vector \\vc \\vc such that \\vc\\trans\\vx <\\vc\\trans\\vy \\vc\\trans\\vx <\\vc\\trans\\vy for all \\vy\\in\\setP \\vy\\in\\setP with \\vy\\neq \\vx \\vy\\neq \\vx . We will consider yet another view of the extreme point that depends number of half-space constraints active as a strict equality. This will be useful when studying the simplex algorithm, which can be used to solve \\eqref{non_stand_LP}. Let \\setB \\subset \\{1,\\dots,m\\} \\setB \\subset \\{1,\\dots,m\\} be a set of row indices of the matrix \\mA \\mA . Let \\mA_\\setB \\mA_\\setB be the sub-matrix of \\mA \\mA containing the rows indexed in \\setB \\setB . Consider a point \\vx \\in \\setP \\vx \\in \\setP . Note that there exists an index set \\setB(\\vx)\\subset \\{1,\\dots,m\\} \\setB(\\vx)\\subset \\{1,\\dots,m\\} such that \\begin{align} &\\va_i\\trans\\vx = b_i \\text{ for all } i\\in \\setB(\\vx), \\text{ and }\\label{eq:boundary}\\\\ &\\va_i\\trans\\vx < b_i \\text{ for all } i\\in\\setN :=\\{1,\\dots,m\\}\\backslash \\setB(\\vx). \\label{eq:interior} \\end{align} \\begin{align} &\\va_i\\trans\\vx = b_i \\text{ for all } i\\in \\setB(\\vx), \\text{ and }\\label{eq:boundary}\\\\ &\\va_i\\trans\\vx < b_i \\text{ for all } i\\in\\setN :=\\{1,\\dots,m\\}\\backslash \\setB(\\vx). \\label{eq:interior} \\end{align} The index set \\setB \\setB is called the active set and \\setN \\setN is called the inactive set (if \\setB(\\vx) \\setB(\\vx) is empty, then \\vx \\vx is in the interior of the polyhedron). Definition A point \\vx \\in \\setP \\vx \\in \\setP is called a basic feasible solution if the vectors \\va_i \\va_i for i \\in \\setB(\\vx) i \\in \\setB(\\vx) are linearly independent and rank( \\mA_{\\setB(\\bar{\\vx})}) = n \\mA_{\\setB(\\bar{\\vx})}) = n . We now state a theorem that relates extreme points, vertices and basic feasible solutions. Theorem The following are equivalent: \\vx^* \\vx^* is an extreme point. \\vx^* \\vx^* is a vertex. \\vx^* \\vx^* is a basic feasible solution. We now state an interesting property of every feasible Linear program that characterizes its optimal points. Theorem Let p^* p^* be the optimal value of the LP \\eqref{non_stand_LP}. There exists a feasible extreme point \\vx^* \\vx^* where \\vc\\trans\\vx^* = p^* \\vc\\trans\\vx^* = p^* . Proof Suppose \\vc\\trans\\hat{\\vx} = p^* \\vc\\trans\\hat{\\vx} = p^* , but \\hat{\\vx} \\hat{\\vx} is not an extreme point. Then the basic set \\setB(\\hat{\\vx}) = \\{ i \\ |\\ \\va_i\\trans\\hat{\\vx} = b_i\\} \\setB(\\hat{\\vx}) = \\{ i \\ |\\ \\va_i\\trans\\hat{\\vx} = b_i\\} has few than n n indices. So, the sub-matrix \\mA_{\\setB(\\hat{\\vx})} \\mA_{\\setB(\\hat{\\vx})} has a non-trivial null-space. Pick some \\vv \\vv in that null-space. Then either \\vc\\trans\\vv = 0 \\vc\\trans\\vv = 0 or \\vc\\trans\\vv \\neq 0 \\vc\\trans\\vv \\neq 0 . We will now proceed by considering possible cases. Suppose \\vc\\trans\\vv <0 \\vc\\trans\\vv <0 and consider \\tilde{\\vx} = \\hat{\\vx} + \\alpha \\vv \\tilde{\\vx} = \\hat{\\vx} + \\alpha \\vv . Note that for all \\alpha>0 \\alpha>0 , we have \\vc\\trans\\hat{\\vx} >\\vc\\trans\\tilde{\\vx} \\vc\\trans\\hat{\\vx} >\\vc\\trans\\tilde{\\vx} and \\mA_{\\setB(\\hat{\\vx})}\\hat{\\vx} = \\mA_{\\setB(\\hat{\\vx})}\\tilde{\\vx} \\mA_{\\setB(\\hat{\\vx})}\\hat{\\vx} = \\mA_{\\setB(\\hat{\\vx})}\\tilde{\\vx} . Additionally, since we have \\mA_{\\setN(\\hat{\\vx})}\\hat{\\vx} < \\vb_{\\setN(\\hat{\\vx})} \\mA_{\\setN(\\hat{\\vx})}\\hat{\\vx} < \\vb_{\\setN(\\hat{\\vx})} , there exists an \\alpha >0 \\alpha >0 such that \\mA_{\\setN(\\hat{\\vx})}\\tilde{\\vx} \\leq \\vb_{\\setN(\\hat{\\vx})} \\mA_{\\setN(\\hat{\\vx})}\\tilde{\\vx} \\leq \\vb_{\\setN(\\hat{\\vx})} . Thus, \\hat{\\vx} \\hat{\\vx} is not an optimal point, which is a contradiction. The case with \\vc\\trans\\vv >0 \\vc\\trans\\vv >0 is similar. Lastly, suppose \\vc\\trans\\vv = 0 \\vc\\trans\\vv = 0 . Then any adjacent extreme point is equally optimal (why?). Standard-form polyhedra \u00b6 In this section, we will outline the steps for converting a generic polyhedorn \\{\\tilde{\\vx}\\ | \\ \\tilde{\\mA}\\tilde{\\vx} = \\tilde{\\vb},\\ \\tilde{\\mC}\\tilde{\\vx} \\leq \\tilde{\\vd}\\} \\{\\tilde{\\vx}\\ | \\ \\tilde{\\mA}\\tilde{\\vx} = \\tilde{\\vb},\\ \\tilde{\\mC}\\tilde{\\vx} \\leq \\tilde{\\vd}\\} into the standard-form. The standard-form of a polyhedron is \\begin{equation}\\label{standard_form} \\setP = \\{\\vx\\ | \\ \\mA\\vx = \\vb, \\vx\\geq 0\\}, \\text{ where } \\vb \\geq 0. \\end{equation} \\begin{equation}\\label{standard_form} \\setP = \\{\\vx\\ | \\ \\mA\\vx = \\vb, \\vx\\geq 0\\}, \\text{ where } \\vb \\geq 0. \\end{equation} The steps for converting to \\setP \\setP , where \\mA \\mA is some m m by n n matrix with m\\leq n m\\leq n , are elementary and stated below: Free variable: A variable \\tilde{\\vx}_i \\tilde{\\vx}_i is called a free variable if it has no constraints. Since every variable must be nonnegative in standard form, and there are no free variable, these variables must be converted. Every free variable \\tilde{\\vx}_i \\tilde{\\vx}_i is replaced with two new nonnegative variables \\tilde{\\vx}^{'}_i \\tilde{\\vx}^{'}_i and \\tilde{\\vx}^{''}_i \\tilde{\\vx}^{''}_i with \\begin{equation} \\tilde{\\vx}_i = \\tilde{\\vx}^{'}_i - \\tilde{\\vx}^{''}_i. \\end{equation} \\begin{equation} \\tilde{\\vx}_i = \\tilde{\\vx}^{'}_i - \\tilde{\\vx}^{''}_i. \\end{equation} Here, \\tilde{\\vx}^{'}_i \\tilde{\\vx}^{'}_i encodes the positive part of \\tilde{\\vx}_i \\tilde{\\vx}_i and \\tilde{\\vx}^{''}_i \\tilde{\\vx}^{''}_i encodes the negative part of \\tilde{\\vx}_i \\tilde{\\vx}_i . For \\tilde{b}_i < 0 \\tilde{b}_i < 0 , we replace \\tilde{a}_i\\trans\\tilde{\\vx} = \\tilde{b}_i \\tilde{a}_i\\trans\\tilde{\\vx} = \\tilde{b}_i with (-\\tilde{a}_i)\\trans\\tilde{\\vx} = -\\tilde{b}_i (-\\tilde{a}_i)\\trans\\tilde{\\vx} = -\\tilde{b}_i . Similarly, For \\tilde{d}_i < 0 \\tilde{d}_i < 0 , we replace \\tilde{c}_i\\trans\\tilde{\\vx} \\leq \\tilde{d}_i \\tilde{c}_i\\trans\\tilde{\\vx} \\leq \\tilde{d}_i with (-\\tilde{c}_i)\\trans\\tilde{\\vx} \\geq -\\tilde{d}_i (-\\tilde{c}_i)\\trans\\tilde{\\vx} \\geq -\\tilde{d}_i and we replace \\tilde{c}_i\\trans\\tilde{\\vx}\\geq \\tilde{d}_i \\tilde{c}_i\\trans\\tilde{\\vx}\\geq \\tilde{d}_i with (-\\tilde{c}_i)\\trans\\tilde{\\vx} \\leq -\\tilde{d}_i (-\\tilde{c}_i)\\trans\\tilde{\\vx} \\leq -\\tilde{d}_i . Surplus and Slack: After the right hand side of the inequality constarint satisfy nonnegativity constraint, these need to be converted to equality constraints. This is done by adding a surplus or slack variable. For inequality constrains of the form \\tilde{\\vc}_i\\trans\\tilde{\\vx}_i \\leq \\tilde{\\vd}_i \\tilde{\\vc}_i\\trans\\tilde{\\vx}_i \\leq \\tilde{\\vd}_i , we introduce a new slack variable s_i s_i and replace the inequality with the following two constraints: \\tilde{\\vc}_i\\trans\\tilde{\\vx}_i + s_i = \\tilde{\\vd}_i \\tilde{\\vc}_i\\trans\\tilde{\\vx}_i + s_i = \\tilde{\\vd}_i and s_i\\geq 0 s_i\\geq 0 . Similarly, if \\tilde{\\vc}_i\\trans\\tilde{\\vx}_i \\geq \\tilde{\\vd}_i \\tilde{\\vc}_i\\trans\\tilde{\\vx}_i \\geq \\tilde{\\vd}_i , we introduce a surplus variable. Basic solution in standard form \u00b6 Recall that \\vx^* \\in \\R^n \\vx^* \\in \\R^n is a basic solution if the vectors \\va_i \\va_i for i \\in \\setB(\\vx^*) i \\in \\setB(\\vx^*) are linearly indenpendent and rank (\\mA_{\\setB(\\vx^*)}) = n (\\mA_{\\setB(\\vx^*)}) = n . In standard form, there are n n variables, i.e. x_1,\\dots,x_n x_1,\\dots,x_n , m m equality constraints, and n n inequality constraints. Since the basic set \\setB(\\vx^*) \\setB(\\vx^*) for any basic solution \\vx^* \\vx^* must have exactly n elements, exactly n of the m+n m+n constraints are active at \\vx^* \\vx^* . The m m equality constraints are always satisfied, thus exactly n-m n-m of the n n inequality constraints x_i\\geq 0 x_i\\geq 0 should be active at \\vx^* \\vx^* . This corresponds to eleminating n-m n-m columns of \\mA \\mA and choosing the remaining m m columnns. Additionally, these m m columns corresponding to a basic solution \\vx^* \\vx^* are linearly independent. So, there exists a permutation matrix \\mP \\mP such that \\begin{equation} \\mA\\mP = \\begin{bmatrix}\\mB &\\mN \\end{bmatrix}, \\end{equation} \\begin{equation} \\mA\\mP = \\begin{bmatrix}\\mB &\\mN \\end{bmatrix}, \\end{equation} where \\mB \\mB is a nonsingular matrix. Let \\bar{\\mA} = \\begin{bmatrix}\\mB & \\mN\\\\ & \\mI\\end{bmatrix} \\bar{\\mA} = \\begin{bmatrix}\\mB & \\mN\\\\ & \\mI\\end{bmatrix} and \\bar{\\vx} = \\mP\\trans\\vx^* \\bar{\\vx} = \\mP\\trans\\vx^* . So, \\begin{equation} \\bar{\\mA} \\bar{\\vx} = \\begin{bmatrix}\\mB & \\mN\\\\ & \\mI\\end{bmatrix}\\begin{bmatrix}\\bar{\\vx}_B \\\\ \\bar{\\vx}_N\\end{bmatrix} = \\begin{bmatrix} \\vb \\\\ \\vzero \\end{bmatrix}, \\end{equation} \\begin{equation} \\bar{\\mA} \\bar{\\vx} = \\begin{bmatrix}\\mB & \\mN\\\\ & \\mI\\end{bmatrix}\\begin{bmatrix}\\bar{\\vx}_B \\\\ \\bar{\\vx}_N\\end{bmatrix} = \\begin{bmatrix} \\vb \\\\ \\vzero \\end{bmatrix}, \\end{equation} which implies that the basic solution satifes \\mB\\bar{\\vx}_B = \\vb \\mB\\bar{\\vx}_B = \\vb and \\bar{\\vx}_N = \\vzero \\bar{\\vx}_N = \\vzero . Simplex method \u00b6 In this section, we will develop the simplex algorithm for a Linear Program in standard-form \\eqref{standard_form}, where \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} . Throughout this section. we will assume that \\mA \\mA has full row rank (no redundant rows). the LP is feasible all basic feasible solution (i.e. extreme points) are nondegenerate. Let \\vx \\in \\setP = \\{\\vx\\ |\\ \\mA\\vx = \\vb, \\vx\\geq \\vzero\\} \\vx \\in \\setP = \\{\\vx\\ |\\ \\mA\\vx = \\vb, \\vx\\geq \\vzero\\} be a feasible point. We say a direction \\vd \\vd is feasible at \\vx \\vx if there exists a scalar \\alpha>0 \\alpha>0 such that \\vx+\\alpha\\vd \\in \\setP \\vx+\\alpha\\vd \\in \\setP . The following subsections will outline the process for choosing a feasible direction and appropriate stepsize \\alpha \\alpha . The simple algorithm initializes at a basic feasible solution. We will also look at how to choose such an initialization. Construction feasible direction \u00b6 The goal of finding a feasible direction is to find a vector \\vd \\vd such that given \\vx \\in \\setP \\vx \\in \\setP , we have that \\vx+\\alpha\\vd \\vx+\\alpha\\vd is also in the polyhedron \\setP \\setP for some \\alpha> 0 \\alpha> 0 . Precisely, this implies \\begin{equation} \\vb = \\mA(\\vx+\\alpha\\vd) = \\mA\\vx + \\alpha\\mA\\vd = b +\\alpha \\mA\\vd. \\end{equation} \\begin{equation} \\vb = \\mA(\\vx+\\alpha\\vd) = \\mA\\vx + \\alpha\\mA\\vd = b +\\alpha \\mA\\vd. \\end{equation} Thus, the feasible direction must be in the null space of \\mA \\mA and satisfy \\mA\\vd =\\vzero \\mA\\vd =\\vzero . Let \\vx \\vx be a basic feasible solution. So, after some permutation, the following relation holds: \\mA\\vx = \\begin{bmatrix}\\mB&\\mN\\end{bmatrix}\\begin{bmatrix}\\vx_B\\\\ \\vx_N\\end{bmatrix} = \\vb \\mA\\vx = \\begin{bmatrix}\\mB&\\mN\\end{bmatrix}\\begin{bmatrix}\\vx_B\\\\ \\vx_N\\end{bmatrix} = \\vb . Correspondingly, we also have \\begin{equation} \\vzero = \\mA \\vd = \\begin{bmatrix}\\mB&\\mN\\end{bmatrix}\\begin{bmatrix}\\vd_B\\\\ \\vd_N\\end{bmatrix} = \\mB\\vd_B + \\mN\\vd_N. \\end{equation} \\begin{equation} \\vzero = \\mA \\vd = \\begin{bmatrix}\\mB&\\mN\\end{bmatrix}\\begin{bmatrix}\\vd_B\\\\ \\vd_N\\end{bmatrix} = \\mB\\vd_B + \\mN\\vd_N. \\end{equation} Thus, we have \\begin{equation}\\label{feasible_direction} \\mB\\vd_B = -\\mN\\vd_N. \\end{equation} \\begin{equation}\\label{feasible_direction} \\mB\\vd_B = -\\mN\\vd_N. \\end{equation} Note that here \\mB \\in \\R^{m\\times m} \\mB \\in \\R^{m\\times m} is non-singular and N \\in \\R^{m\\times (n-m)} N \\in \\R^{m\\times (n-m)} . We can now choose a feaible direction by fixing an index \\eta_k \\in N \\eta_k \\in N , which determines a single nonbasic direction to move along. So, the feasible direction \\vd \\vd corresponding to a nonbasic variable with index \\eta_k \\eta_k satisfies: \\begin{equation} \\vd_N = \\ve_k \\quad\\text{ and } \\quad\\mB\\vd_b = -\\mN\\vd_N = -\\va_{\\eta_k}, \\end{equation} \\begin{equation} \\vd_N = \\ve_k \\quad\\text{ and } \\quad\\mB\\vd_b = -\\mN\\vd_N = -\\va_{\\eta_k}, \\end{equation} where \\va_{\\eta_k} \\va_{\\eta_k} is a column of \\mA = \\begin{bmatrix}\\mB & \\mN\\end{bmatrix} \\mA = \\begin{bmatrix}\\mB & \\mN\\end{bmatrix} corresponding to index \\eta_k \\in N \\eta_k \\in N . Reduced cost \u00b6 The goal is to choose a feasible direction \\vd \\vd in such a way that it reduces the cost function (objective) \\phi(\\vx) : = \\vc\\trans(\\vx) \\phi(\\vx) : = \\vc\\trans(\\vx) . Let \\bar{\\vx} = \\vx +\\alpha \\vd \\bar{\\vx} = \\vx +\\alpha \\vd , where \\vx \\vx is some basic feasible solution and \\alpha>0 \\alpha>0 . Consider the objective value at the next iterate \\vx + \\alpha \\vd \\vx + \\alpha \\vd given by \\begin{align} \\phi(\\vx + \\alpha \\vx) & = \\vc\\trans\\bar{\\vx}\\nonumber\\\\ & = \\vc\\trans(\\vx + \\alpha \\vd)\\nonumber\\\\ & = \\vc\\trans\\vx + \\alpha \\vc\\trans\\vd\\nonumber\\\\ & = \\phi(\\vx) + \\alpha (\\underbrace{\\vc_B\\trans\\vd_B + \\vc_N\\trans \\vd_N}_{=: I})\\label{obj_decrease} \\end{align}. \\begin{align} \\phi(\\vx + \\alpha \\vx) & = \\vc\\trans\\bar{\\vx}\\nonumber\\\\ & = \\vc\\trans(\\vx + \\alpha \\vd)\\nonumber\\\\ & = \\vc\\trans\\vx + \\alpha \\vc\\trans\\vd\\nonumber\\\\ & = \\phi(\\vx) + \\alpha (\\underbrace{\\vc_B\\trans\\vd_B + \\vc_N\\trans \\vd_N}_{=: I})\\label{obj_decrease} \\end{align}. The term I I in the above equation is related to the reduced cost. Recall that the feasible direction satfies \\eqref{feasible_direction}. So if feasible direction does note move along an index in the nonbasic set, i.e. \\vd_N = \\vzero \\vd_N = \\vzero then this necessarily means that \\vd_B = 0 \\vd_B = 0 . This is beacause \\vd_N = \\vzero \\vd_N = \\vzero implies \\mB\\vd_B = 0 \\mB\\vd_B = 0 . Since \\mB \\mB is invertible, the null space of \\mB \\mB is trivial and, consequently, \\mB\\vd_B = 0 \\mB\\vd_B = 0 necessarily imples \\vd \\vd is the zero vector. So, the term I= \\vc_B\\trans\\vd_B + \\vc_N\\trans \\vd_N I= \\vc_B\\trans\\vd_B + \\vc_N\\trans \\vd_N is zero for any feasible direction not aligned with a nonbasic variable. This leads to the following notion of reduced cost where we only consider directions along the nonbasic variable. For j \\in N j \\in N , let z_j = \\vc_B\\trans\\vd_B + \\vc_N\\trans \\ve_j = \\vc_B\\trans\\vd_B + c_j. z_j = \\vc_B\\trans\\vd_B + \\vc_N\\trans \\ve_j = \\vc_B\\trans\\vd_B + c_j. Additionally, \\eqref{feasible_direction} implies that \\vd_B = -\\mB^{-1} \\va_j \\vd_B = -\\mB^{-1} \\va_j . So, we have z_j = -\\vc_B \\mB^{-1} \\va_j + c_j z_j = -\\vc_B \\mB^{-1} \\va_j + c_j . Recall from \\eqref{obj_decrease}, the objective function decrease with \\vd = \\begin{bmatrix}\\vd_B\\\\ \\vd_N\\end{bmatrix} = \\begin{bmatrix}\\vzero\\\\ \\ve_j\\end{bmatrix} \\vd = \\begin{bmatrix}\\vd_B\\\\ \\vd_N\\end{bmatrix} = \\begin{bmatrix}\\vzero\\\\ \\ve_j\\end{bmatrix} as a descent direction if z_j < 0 z_j < 0 . We gather z_j z_j for all j \\in N j \\in N and call this vector the reduced cost. So the reduced cost vector is \\vz \\in \\R^{|N|} \\vz \\in \\R^{|N|} with z_j = -\\vc_B \\mB^{-1} \\va_j + c_j z_j = -\\vc_B \\mB^{-1} \\va_j + c_j and |N| |N| is the cardinality of the nonbasic set. We now state the Theorem for optimality of a basic feasible point. Theorem Consider a basic feasible solution \\vx \\vx with a reduced cost \\vz \\vz . The following holds true: If \\vz \\geq 0 \\vz \\geq 0 then \\vx \\vx is optimal. If \\vx \\vx is optimal and nondegenerate then \\vz \\geq \\vzero \\vz \\geq \\vzero . Choosing a stepsize \u00b6 Let \\vz \\vz be the reduced cost at a basic feasible solution \\vx \\vx . Note that the change objective for moving in the p p the nonbasic variable with index \\eta_p\\in N \\eta_p\\in N is \\bar{\\phi} = \\phi + \\alpha z_{\\eta_p}, \\bar{\\phi} = \\phi + \\alpha z_{\\eta_p}, where \\bar{\\phi} \\bar{\\phi} is the objective value at the next iterate and \\phi \\phi is the objective value at the current iterate. We will assume atleast one of the z_i z_i 's satisfy z_i <0 z_i <0 (otherwise, the basic feasible solution is the optimal point). The idea for choosing a stepsize is to find \\alpha^* = \\max\\{\\alpha\\geq 0 \\ | \\vx + \\alpha\\vd \\geq 0\\} \\alpha^* = \\max\\{\\alpha\\geq 0 \\ | \\vx + \\alpha\\vd \\geq 0\\} There are two possible cases: If \\vd\\geq \\vzero \\vd\\geq \\vzero , then \\vd \\vd is an unbounded feasible direction of descent, i.e. x+\\alpha\\vd \\geq \\text{ for all } \\alpha\\geq 0. x+\\alpha\\vd \\geq \\text{ for all } \\alpha\\geq 0. In this case, the optimal value for the LP is unbounded. If d_j \\leq 0 d_j \\leq 0 for some j j , then \\vx+\\alpha\\vd \\geq \\vzero \\vx+\\alpha\\vd \\geq \\vzero only if \\alpha -\\frac{x_j}{d_j} \\text{ for every } d_j <0. \\alpha -\\frac{x_j}{d_j} \\text{ for every } d_j <0. If we choose \\alpha^* \\alpha^* acording to the following ratio test \\alpha^* = \\min_{j\\in \\{j\\in B| d_j<0\\}} -\\frac{x_j}{d_j}, \\alpha^* = \\min_{j\\in \\{j\\in B| d_j<0\\}} -\\frac{x_j}{d_j}, then \\vx+\\alpha^*\\vd \\geq \\vzero \\vx+\\alpha^*\\vd \\geq \\vzero is satisfed and \\alpha^* \\alpha^* is the largest stepsize that can be used without violating the feasiblity condition.","title":"Linear programming"},{"location":"notes/Linear_programming/#linear-programming","text":"Linear programming solves the following minimization program \\begin{equation}\\label{non_stand_LP} \\min_{\\vx\\in\\R^n} \\vc\\trans\\vx \\text{ subject to } \\mA\\vx\\leq \\vb, \\end{equation} \\begin{equation}\\label{non_stand_LP} \\min_{\\vx\\in\\R^n} \\vc\\trans\\vx \\text{ subject to } \\mA\\vx\\leq \\vb, \\end{equation} where \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} , \\vc \\in \\R^n \\vc \\in \\R^n , and \\vb \\in \\R^m \\vb \\in \\R^m are known a priori. In general, the optimization problem \\eqref{non_stand_LP} has three possibilities: The problem is infeasible, i.e. there exist no such \\vx \\in \\R^n \\vx \\in \\R^n such that \\mA\\vx \\leq \\vb \\mA\\vx \\leq \\vb . The problem is unbounded from below, i.e. there exist some \\vx = \\vx_0 +\\alpha \\vd \\vx = \\vx_0 +\\alpha \\vd where \\mA\\vx \\leq \\vb \\mA\\vx \\leq \\vb for all \\alpha\\geq 0 \\alpha\\geq 0 , and \\vc\\trans \\vx < 0 \\vc\\trans \\vx < 0 . In this case \\vd \\vd is a direction that pushes the objective value to -\u221e -\u221e while maintaining feasibility. The problem is feasible and has a finite minimum, i.e. there exist a point \\vx \\vx such that \\mA\\vx\\leq \\vb \\mA\\vx\\leq \\vb and the optimal value of \\eqref{non_stand_LP} p^* = \\min\\{\\vc\\trans\\vx | \\mA\\vx\\leq \\vb\\} p^* = \\min\\{\\vc\\trans\\vx | \\mA\\vx\\leq \\vb\\} satisfies p^*>-\u221e p^*>-\u221e . In this lecture, we will assume the problem is always feasible and characterize the optimal points of \\eqref{non_stand_LP}.","title":"Linear Programming"},{"location":"notes/Linear_programming/#extreme-points","text":"The constraint set in the linear program \\eqref{non_stand_LP} is \\setP := \\{x\\in \\R^n | \\va_i\\trans\\vx \\leq b_i,\\ i = 1\\,\\dots, m\\} \\setP := \\{x\\in \\R^n | \\va_i\\trans\\vx \\leq b_i,\\ i = 1\\,\\dots, m\\} . This set is the intersection of m m half-spaces given by \\va_i\\trans\\vx \\leq b_i \\va_i\\trans\\vx \\leq b_i and form a polyhedron (see figure below). Extreme points can be studied in many equivalent ways. A definition of extreme point is: Definition A point \\vx \\in \\setP \\vx \\in \\setP is an extreme point of \\setP \\setP if there does not exist two vectors \\vy,\\vz \\in \\setP \\vy,\\vz \\in \\setP such that \\vx = \\lambda \\vy + (1-\\lambda)\\vz \\vx = \\lambda \\vy + (1-\\lambda)\\vz for any \\lambda \\in (0,1) \\lambda \\in (0,1) . This definition states that an extreme point can not be in the interior of any line segment contained in the polyhedron. Another geometric way to view extreme points is via vertices of a polyhedron. A point \\bar{\\vx} \\bar{\\vx} is an extreme point of a polyhedron if and only if \\bar{\\vx} \\bar{\\vx} is an vertex. The vertex of a polyhedron is defined as: Definition A point \\vx \\in \\setP \\vx \\in \\setP is a vertex of \\setP \\setP if there exists a vector \\vc \\vc such that \\vc\\trans\\vx <\\vc\\trans\\vy \\vc\\trans\\vx <\\vc\\trans\\vy for all \\vy\\in\\setP \\vy\\in\\setP with \\vy\\neq \\vx \\vy\\neq \\vx . We will consider yet another view of the extreme point that depends number of half-space constraints active as a strict equality. This will be useful when studying the simplex algorithm, which can be used to solve \\eqref{non_stand_LP}. Let \\setB \\subset \\{1,\\dots,m\\} \\setB \\subset \\{1,\\dots,m\\} be a set of row indices of the matrix \\mA \\mA . Let \\mA_\\setB \\mA_\\setB be the sub-matrix of \\mA \\mA containing the rows indexed in \\setB \\setB . Consider a point \\vx \\in \\setP \\vx \\in \\setP . Note that there exists an index set \\setB(\\vx)\\subset \\{1,\\dots,m\\} \\setB(\\vx)\\subset \\{1,\\dots,m\\} such that \\begin{align} &\\va_i\\trans\\vx = b_i \\text{ for all } i\\in \\setB(\\vx), \\text{ and }\\label{eq:boundary}\\\\ &\\va_i\\trans\\vx < b_i \\text{ for all } i\\in\\setN :=\\{1,\\dots,m\\}\\backslash \\setB(\\vx). \\label{eq:interior} \\end{align} \\begin{align} &\\va_i\\trans\\vx = b_i \\text{ for all } i\\in \\setB(\\vx), \\text{ and }\\label{eq:boundary}\\\\ &\\va_i\\trans\\vx < b_i \\text{ for all } i\\in\\setN :=\\{1,\\dots,m\\}\\backslash \\setB(\\vx). \\label{eq:interior} \\end{align} The index set \\setB \\setB is called the active set and \\setN \\setN is called the inactive set (if \\setB(\\vx) \\setB(\\vx) is empty, then \\vx \\vx is in the interior of the polyhedron). Definition A point \\vx \\in \\setP \\vx \\in \\setP is called a basic feasible solution if the vectors \\va_i \\va_i for i \\in \\setB(\\vx) i \\in \\setB(\\vx) are linearly independent and rank( \\mA_{\\setB(\\bar{\\vx})}) = n \\mA_{\\setB(\\bar{\\vx})}) = n . We now state a theorem that relates extreme points, vertices and basic feasible solutions. Theorem The following are equivalent: \\vx^* \\vx^* is an extreme point. \\vx^* \\vx^* is a vertex. \\vx^* \\vx^* is a basic feasible solution. We now state an interesting property of every feasible Linear program that characterizes its optimal points. Theorem Let p^* p^* be the optimal value of the LP \\eqref{non_stand_LP}. There exists a feasible extreme point \\vx^* \\vx^* where \\vc\\trans\\vx^* = p^* \\vc\\trans\\vx^* = p^* . Proof Suppose \\vc\\trans\\hat{\\vx} = p^* \\vc\\trans\\hat{\\vx} = p^* , but \\hat{\\vx} \\hat{\\vx} is not an extreme point. Then the basic set \\setB(\\hat{\\vx}) = \\{ i \\ |\\ \\va_i\\trans\\hat{\\vx} = b_i\\} \\setB(\\hat{\\vx}) = \\{ i \\ |\\ \\va_i\\trans\\hat{\\vx} = b_i\\} has few than n n indices. So, the sub-matrix \\mA_{\\setB(\\hat{\\vx})} \\mA_{\\setB(\\hat{\\vx})} has a non-trivial null-space. Pick some \\vv \\vv in that null-space. Then either \\vc\\trans\\vv = 0 \\vc\\trans\\vv = 0 or \\vc\\trans\\vv \\neq 0 \\vc\\trans\\vv \\neq 0 . We will now proceed by considering possible cases. Suppose \\vc\\trans\\vv <0 \\vc\\trans\\vv <0 and consider \\tilde{\\vx} = \\hat{\\vx} + \\alpha \\vv \\tilde{\\vx} = \\hat{\\vx} + \\alpha \\vv . Note that for all \\alpha>0 \\alpha>0 , we have \\vc\\trans\\hat{\\vx} >\\vc\\trans\\tilde{\\vx} \\vc\\trans\\hat{\\vx} >\\vc\\trans\\tilde{\\vx} and \\mA_{\\setB(\\hat{\\vx})}\\hat{\\vx} = \\mA_{\\setB(\\hat{\\vx})}\\tilde{\\vx} \\mA_{\\setB(\\hat{\\vx})}\\hat{\\vx} = \\mA_{\\setB(\\hat{\\vx})}\\tilde{\\vx} . Additionally, since we have \\mA_{\\setN(\\hat{\\vx})}\\hat{\\vx} < \\vb_{\\setN(\\hat{\\vx})} \\mA_{\\setN(\\hat{\\vx})}\\hat{\\vx} < \\vb_{\\setN(\\hat{\\vx})} , there exists an \\alpha >0 \\alpha >0 such that \\mA_{\\setN(\\hat{\\vx})}\\tilde{\\vx} \\leq \\vb_{\\setN(\\hat{\\vx})} \\mA_{\\setN(\\hat{\\vx})}\\tilde{\\vx} \\leq \\vb_{\\setN(\\hat{\\vx})} . Thus, \\hat{\\vx} \\hat{\\vx} is not an optimal point, which is a contradiction. The case with \\vc\\trans\\vv >0 \\vc\\trans\\vv >0 is similar. Lastly, suppose \\vc\\trans\\vv = 0 \\vc\\trans\\vv = 0 . Then any adjacent extreme point is equally optimal (why?).","title":"Extreme points"},{"location":"notes/Linear_programming/#standard-form-polyhedra","text":"In this section, we will outline the steps for converting a generic polyhedorn \\{\\tilde{\\vx}\\ | \\ \\tilde{\\mA}\\tilde{\\vx} = \\tilde{\\vb},\\ \\tilde{\\mC}\\tilde{\\vx} \\leq \\tilde{\\vd}\\} \\{\\tilde{\\vx}\\ | \\ \\tilde{\\mA}\\tilde{\\vx} = \\tilde{\\vb},\\ \\tilde{\\mC}\\tilde{\\vx} \\leq \\tilde{\\vd}\\} into the standard-form. The standard-form of a polyhedron is \\begin{equation}\\label{standard_form} \\setP = \\{\\vx\\ | \\ \\mA\\vx = \\vb, \\vx\\geq 0\\}, \\text{ where } \\vb \\geq 0. \\end{equation} \\begin{equation}\\label{standard_form} \\setP = \\{\\vx\\ | \\ \\mA\\vx = \\vb, \\vx\\geq 0\\}, \\text{ where } \\vb \\geq 0. \\end{equation} The steps for converting to \\setP \\setP , where \\mA \\mA is some m m by n n matrix with m\\leq n m\\leq n , are elementary and stated below: Free variable: A variable \\tilde{\\vx}_i \\tilde{\\vx}_i is called a free variable if it has no constraints. Since every variable must be nonnegative in standard form, and there are no free variable, these variables must be converted. Every free variable \\tilde{\\vx}_i \\tilde{\\vx}_i is replaced with two new nonnegative variables \\tilde{\\vx}^{'}_i \\tilde{\\vx}^{'}_i and \\tilde{\\vx}^{''}_i \\tilde{\\vx}^{''}_i with \\begin{equation} \\tilde{\\vx}_i = \\tilde{\\vx}^{'}_i - \\tilde{\\vx}^{''}_i. \\end{equation} \\begin{equation} \\tilde{\\vx}_i = \\tilde{\\vx}^{'}_i - \\tilde{\\vx}^{''}_i. \\end{equation} Here, \\tilde{\\vx}^{'}_i \\tilde{\\vx}^{'}_i encodes the positive part of \\tilde{\\vx}_i \\tilde{\\vx}_i and \\tilde{\\vx}^{''}_i \\tilde{\\vx}^{''}_i encodes the negative part of \\tilde{\\vx}_i \\tilde{\\vx}_i . For \\tilde{b}_i < 0 \\tilde{b}_i < 0 , we replace \\tilde{a}_i\\trans\\tilde{\\vx} = \\tilde{b}_i \\tilde{a}_i\\trans\\tilde{\\vx} = \\tilde{b}_i with (-\\tilde{a}_i)\\trans\\tilde{\\vx} = -\\tilde{b}_i (-\\tilde{a}_i)\\trans\\tilde{\\vx} = -\\tilde{b}_i . Similarly, For \\tilde{d}_i < 0 \\tilde{d}_i < 0 , we replace \\tilde{c}_i\\trans\\tilde{\\vx} \\leq \\tilde{d}_i \\tilde{c}_i\\trans\\tilde{\\vx} \\leq \\tilde{d}_i with (-\\tilde{c}_i)\\trans\\tilde{\\vx} \\geq -\\tilde{d}_i (-\\tilde{c}_i)\\trans\\tilde{\\vx} \\geq -\\tilde{d}_i and we replace \\tilde{c}_i\\trans\\tilde{\\vx}\\geq \\tilde{d}_i \\tilde{c}_i\\trans\\tilde{\\vx}\\geq \\tilde{d}_i with (-\\tilde{c}_i)\\trans\\tilde{\\vx} \\leq -\\tilde{d}_i (-\\tilde{c}_i)\\trans\\tilde{\\vx} \\leq -\\tilde{d}_i . Surplus and Slack: After the right hand side of the inequality constarint satisfy nonnegativity constraint, these need to be converted to equality constraints. This is done by adding a surplus or slack variable. For inequality constrains of the form \\tilde{\\vc}_i\\trans\\tilde{\\vx}_i \\leq \\tilde{\\vd}_i \\tilde{\\vc}_i\\trans\\tilde{\\vx}_i \\leq \\tilde{\\vd}_i , we introduce a new slack variable s_i s_i and replace the inequality with the following two constraints: \\tilde{\\vc}_i\\trans\\tilde{\\vx}_i + s_i = \\tilde{\\vd}_i \\tilde{\\vc}_i\\trans\\tilde{\\vx}_i + s_i = \\tilde{\\vd}_i and s_i\\geq 0 s_i\\geq 0 . Similarly, if \\tilde{\\vc}_i\\trans\\tilde{\\vx}_i \\geq \\tilde{\\vd}_i \\tilde{\\vc}_i\\trans\\tilde{\\vx}_i \\geq \\tilde{\\vd}_i , we introduce a surplus variable.","title":"Standard-form polyhedra"},{"location":"notes/Linear_programming/#basic-solution-in-standard-form","text":"Recall that \\vx^* \\in \\R^n \\vx^* \\in \\R^n is a basic solution if the vectors \\va_i \\va_i for i \\in \\setB(\\vx^*) i \\in \\setB(\\vx^*) are linearly indenpendent and rank (\\mA_{\\setB(\\vx^*)}) = n (\\mA_{\\setB(\\vx^*)}) = n . In standard form, there are n n variables, i.e. x_1,\\dots,x_n x_1,\\dots,x_n , m m equality constraints, and n n inequality constraints. Since the basic set \\setB(\\vx^*) \\setB(\\vx^*) for any basic solution \\vx^* \\vx^* must have exactly n elements, exactly n of the m+n m+n constraints are active at \\vx^* \\vx^* . The m m equality constraints are always satisfied, thus exactly n-m n-m of the n n inequality constraints x_i\\geq 0 x_i\\geq 0 should be active at \\vx^* \\vx^* . This corresponds to eleminating n-m n-m columns of \\mA \\mA and choosing the remaining m m columnns. Additionally, these m m columns corresponding to a basic solution \\vx^* \\vx^* are linearly independent. So, there exists a permutation matrix \\mP \\mP such that \\begin{equation} \\mA\\mP = \\begin{bmatrix}\\mB &\\mN \\end{bmatrix}, \\end{equation} \\begin{equation} \\mA\\mP = \\begin{bmatrix}\\mB &\\mN \\end{bmatrix}, \\end{equation} where \\mB \\mB is a nonsingular matrix. Let \\bar{\\mA} = \\begin{bmatrix}\\mB & \\mN\\\\ & \\mI\\end{bmatrix} \\bar{\\mA} = \\begin{bmatrix}\\mB & \\mN\\\\ & \\mI\\end{bmatrix} and \\bar{\\vx} = \\mP\\trans\\vx^* \\bar{\\vx} = \\mP\\trans\\vx^* . So, \\begin{equation} \\bar{\\mA} \\bar{\\vx} = \\begin{bmatrix}\\mB & \\mN\\\\ & \\mI\\end{bmatrix}\\begin{bmatrix}\\bar{\\vx}_B \\\\ \\bar{\\vx}_N\\end{bmatrix} = \\begin{bmatrix} \\vb \\\\ \\vzero \\end{bmatrix}, \\end{equation} \\begin{equation} \\bar{\\mA} \\bar{\\vx} = \\begin{bmatrix}\\mB & \\mN\\\\ & \\mI\\end{bmatrix}\\begin{bmatrix}\\bar{\\vx}_B \\\\ \\bar{\\vx}_N\\end{bmatrix} = \\begin{bmatrix} \\vb \\\\ \\vzero \\end{bmatrix}, \\end{equation} which implies that the basic solution satifes \\mB\\bar{\\vx}_B = \\vb \\mB\\bar{\\vx}_B = \\vb and \\bar{\\vx}_N = \\vzero \\bar{\\vx}_N = \\vzero .","title":"Basic solution in standard form"},{"location":"notes/Linear_programming/#simplex-method","text":"In this section, we will develop the simplex algorithm for a Linear Program in standard-form \\eqref{standard_form}, where \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} . Throughout this section. we will assume that \\mA \\mA has full row rank (no redundant rows). the LP is feasible all basic feasible solution (i.e. extreme points) are nondegenerate. Let \\vx \\in \\setP = \\{\\vx\\ |\\ \\mA\\vx = \\vb, \\vx\\geq \\vzero\\} \\vx \\in \\setP = \\{\\vx\\ |\\ \\mA\\vx = \\vb, \\vx\\geq \\vzero\\} be a feasible point. We say a direction \\vd \\vd is feasible at \\vx \\vx if there exists a scalar \\alpha>0 \\alpha>0 such that \\vx+\\alpha\\vd \\in \\setP \\vx+\\alpha\\vd \\in \\setP . The following subsections will outline the process for choosing a feasible direction and appropriate stepsize \\alpha \\alpha . The simple algorithm initializes at a basic feasible solution. We will also look at how to choose such an initialization.","title":"Simplex method"},{"location":"notes/Linear_programming/#construction-feasible-direction","text":"The goal of finding a feasible direction is to find a vector \\vd \\vd such that given \\vx \\in \\setP \\vx \\in \\setP , we have that \\vx+\\alpha\\vd \\vx+\\alpha\\vd is also in the polyhedron \\setP \\setP for some \\alpha> 0 \\alpha> 0 . Precisely, this implies \\begin{equation} \\vb = \\mA(\\vx+\\alpha\\vd) = \\mA\\vx + \\alpha\\mA\\vd = b +\\alpha \\mA\\vd. \\end{equation} \\begin{equation} \\vb = \\mA(\\vx+\\alpha\\vd) = \\mA\\vx + \\alpha\\mA\\vd = b +\\alpha \\mA\\vd. \\end{equation} Thus, the feasible direction must be in the null space of \\mA \\mA and satisfy \\mA\\vd =\\vzero \\mA\\vd =\\vzero . Let \\vx \\vx be a basic feasible solution. So, after some permutation, the following relation holds: \\mA\\vx = \\begin{bmatrix}\\mB&\\mN\\end{bmatrix}\\begin{bmatrix}\\vx_B\\\\ \\vx_N\\end{bmatrix} = \\vb \\mA\\vx = \\begin{bmatrix}\\mB&\\mN\\end{bmatrix}\\begin{bmatrix}\\vx_B\\\\ \\vx_N\\end{bmatrix} = \\vb . Correspondingly, we also have \\begin{equation} \\vzero = \\mA \\vd = \\begin{bmatrix}\\mB&\\mN\\end{bmatrix}\\begin{bmatrix}\\vd_B\\\\ \\vd_N\\end{bmatrix} = \\mB\\vd_B + \\mN\\vd_N. \\end{equation} \\begin{equation} \\vzero = \\mA \\vd = \\begin{bmatrix}\\mB&\\mN\\end{bmatrix}\\begin{bmatrix}\\vd_B\\\\ \\vd_N\\end{bmatrix} = \\mB\\vd_B + \\mN\\vd_N. \\end{equation} Thus, we have \\begin{equation}\\label{feasible_direction} \\mB\\vd_B = -\\mN\\vd_N. \\end{equation} \\begin{equation}\\label{feasible_direction} \\mB\\vd_B = -\\mN\\vd_N. \\end{equation} Note that here \\mB \\in \\R^{m\\times m} \\mB \\in \\R^{m\\times m} is non-singular and N \\in \\R^{m\\times (n-m)} N \\in \\R^{m\\times (n-m)} . We can now choose a feaible direction by fixing an index \\eta_k \\in N \\eta_k \\in N , which determines a single nonbasic direction to move along. So, the feasible direction \\vd \\vd corresponding to a nonbasic variable with index \\eta_k \\eta_k satisfies: \\begin{equation} \\vd_N = \\ve_k \\quad\\text{ and } \\quad\\mB\\vd_b = -\\mN\\vd_N = -\\va_{\\eta_k}, \\end{equation} \\begin{equation} \\vd_N = \\ve_k \\quad\\text{ and } \\quad\\mB\\vd_b = -\\mN\\vd_N = -\\va_{\\eta_k}, \\end{equation} where \\va_{\\eta_k} \\va_{\\eta_k} is a column of \\mA = \\begin{bmatrix}\\mB & \\mN\\end{bmatrix} \\mA = \\begin{bmatrix}\\mB & \\mN\\end{bmatrix} corresponding to index \\eta_k \\in N \\eta_k \\in N .","title":"Construction feasible direction"},{"location":"notes/Linear_programming/#reduced-cost","text":"The goal is to choose a feasible direction \\vd \\vd in such a way that it reduces the cost function (objective) \\phi(\\vx) : = \\vc\\trans(\\vx) \\phi(\\vx) : = \\vc\\trans(\\vx) . Let \\bar{\\vx} = \\vx +\\alpha \\vd \\bar{\\vx} = \\vx +\\alpha \\vd , where \\vx \\vx is some basic feasible solution and \\alpha>0 \\alpha>0 . Consider the objective value at the next iterate \\vx + \\alpha \\vd \\vx + \\alpha \\vd given by \\begin{align} \\phi(\\vx + \\alpha \\vx) & = \\vc\\trans\\bar{\\vx}\\nonumber\\\\ & = \\vc\\trans(\\vx + \\alpha \\vd)\\nonumber\\\\ & = \\vc\\trans\\vx + \\alpha \\vc\\trans\\vd\\nonumber\\\\ & = \\phi(\\vx) + \\alpha (\\underbrace{\\vc_B\\trans\\vd_B + \\vc_N\\trans \\vd_N}_{=: I})\\label{obj_decrease} \\end{align}. \\begin{align} \\phi(\\vx + \\alpha \\vx) & = \\vc\\trans\\bar{\\vx}\\nonumber\\\\ & = \\vc\\trans(\\vx + \\alpha \\vd)\\nonumber\\\\ & = \\vc\\trans\\vx + \\alpha \\vc\\trans\\vd\\nonumber\\\\ & = \\phi(\\vx) + \\alpha (\\underbrace{\\vc_B\\trans\\vd_B + \\vc_N\\trans \\vd_N}_{=: I})\\label{obj_decrease} \\end{align}. The term I I in the above equation is related to the reduced cost. Recall that the feasible direction satfies \\eqref{feasible_direction}. So if feasible direction does note move along an index in the nonbasic set, i.e. \\vd_N = \\vzero \\vd_N = \\vzero then this necessarily means that \\vd_B = 0 \\vd_B = 0 . This is beacause \\vd_N = \\vzero \\vd_N = \\vzero implies \\mB\\vd_B = 0 \\mB\\vd_B = 0 . Since \\mB \\mB is invertible, the null space of \\mB \\mB is trivial and, consequently, \\mB\\vd_B = 0 \\mB\\vd_B = 0 necessarily imples \\vd \\vd is the zero vector. So, the term I= \\vc_B\\trans\\vd_B + \\vc_N\\trans \\vd_N I= \\vc_B\\trans\\vd_B + \\vc_N\\trans \\vd_N is zero for any feasible direction not aligned with a nonbasic variable. This leads to the following notion of reduced cost where we only consider directions along the nonbasic variable. For j \\in N j \\in N , let z_j = \\vc_B\\trans\\vd_B + \\vc_N\\trans \\ve_j = \\vc_B\\trans\\vd_B + c_j. z_j = \\vc_B\\trans\\vd_B + \\vc_N\\trans \\ve_j = \\vc_B\\trans\\vd_B + c_j. Additionally, \\eqref{feasible_direction} implies that \\vd_B = -\\mB^{-1} \\va_j \\vd_B = -\\mB^{-1} \\va_j . So, we have z_j = -\\vc_B \\mB^{-1} \\va_j + c_j z_j = -\\vc_B \\mB^{-1} \\va_j + c_j . Recall from \\eqref{obj_decrease}, the objective function decrease with \\vd = \\begin{bmatrix}\\vd_B\\\\ \\vd_N\\end{bmatrix} = \\begin{bmatrix}\\vzero\\\\ \\ve_j\\end{bmatrix} \\vd = \\begin{bmatrix}\\vd_B\\\\ \\vd_N\\end{bmatrix} = \\begin{bmatrix}\\vzero\\\\ \\ve_j\\end{bmatrix} as a descent direction if z_j < 0 z_j < 0 . We gather z_j z_j for all j \\in N j \\in N and call this vector the reduced cost. So the reduced cost vector is \\vz \\in \\R^{|N|} \\vz \\in \\R^{|N|} with z_j = -\\vc_B \\mB^{-1} \\va_j + c_j z_j = -\\vc_B \\mB^{-1} \\va_j + c_j and |N| |N| is the cardinality of the nonbasic set. We now state the Theorem for optimality of a basic feasible point. Theorem Consider a basic feasible solution \\vx \\vx with a reduced cost \\vz \\vz . The following holds true: If \\vz \\geq 0 \\vz \\geq 0 then \\vx \\vx is optimal. If \\vx \\vx is optimal and nondegenerate then \\vz \\geq \\vzero \\vz \\geq \\vzero .","title":"Reduced cost"},{"location":"notes/Linear_programming/#choosing-a-stepsize","text":"Let \\vz \\vz be the reduced cost at a basic feasible solution \\vx \\vx . Note that the change objective for moving in the p p the nonbasic variable with index \\eta_p\\in N \\eta_p\\in N is \\bar{\\phi} = \\phi + \\alpha z_{\\eta_p}, \\bar{\\phi} = \\phi + \\alpha z_{\\eta_p}, where \\bar{\\phi} \\bar{\\phi} is the objective value at the next iterate and \\phi \\phi is the objective value at the current iterate. We will assume atleast one of the z_i z_i 's satisfy z_i <0 z_i <0 (otherwise, the basic feasible solution is the optimal point). The idea for choosing a stepsize is to find \\alpha^* = \\max\\{\\alpha\\geq 0 \\ | \\vx + \\alpha\\vd \\geq 0\\} \\alpha^* = \\max\\{\\alpha\\geq 0 \\ | \\vx + \\alpha\\vd \\geq 0\\} There are two possible cases: If \\vd\\geq \\vzero \\vd\\geq \\vzero , then \\vd \\vd is an unbounded feasible direction of descent, i.e. x+\\alpha\\vd \\geq \\text{ for all } \\alpha\\geq 0. x+\\alpha\\vd \\geq \\text{ for all } \\alpha\\geq 0. In this case, the optimal value for the LP is unbounded. If d_j \\leq 0 d_j \\leq 0 for some j j , then \\vx+\\alpha\\vd \\geq \\vzero \\vx+\\alpha\\vd \\geq \\vzero only if \\alpha -\\frac{x_j}{d_j} \\text{ for every } d_j <0. \\alpha -\\frac{x_j}{d_j} \\text{ for every } d_j <0. If we choose \\alpha^* \\alpha^* acording to the following ratio test \\alpha^* = \\min_{j\\in \\{j\\in B| d_j<0\\}} -\\frac{x_j}{d_j}, \\alpha^* = \\min_{j\\in \\{j\\in B| d_j<0\\}} -\\frac{x_j}{d_j}, then \\vx+\\alpha^*\\vd \\geq \\vzero \\vx+\\alpha^*\\vd \\geq \\vzero is satisfed and \\alpha^* \\alpha^* is the largest stepsize that can be used without violating the feasiblity condition.","title":"Choosing a stepsize"},{"location":"notes/Newtons_method/","text":"Newton's Method \u00b6 In Newton's method, we minimize a quadratic approximation of the function at each iterate. Consider the problem \\minimize_{\\vx \\in \\R^n} f(\\vx) \\minimize_{\\vx \\in \\R^n} f(\\vx) where f:\\R^n\\rightarrow \\R f:\\R^n\\rightarrow \\R is a twice continuously differentiable function. Given an iterate \\vx_k \\vx_k , define \\vx_{k+1} \\vx_{k+1} as the minimizer of the quadratic program given by \\vx_{k+1} = \\argmin_{\\vx} \\{f_k + \\nabla f_k\\trans(\\vx - \\vx_k) - \\frac{1}{2}(\\vx-\\vx_k)\\trans\\nabla^2f_k(\\vx-\\vx_k)\\}. \\vx_{k+1} = \\argmin_{\\vx} \\{f_k + \\nabla f_k\\trans(\\vx - \\vx_k) - \\frac{1}{2}(\\vx-\\vx_k)\\trans\\nabla^2f_k(\\vx-\\vx_k)\\}. In the above equation, f_k := f(\\vx_k) f_k := f(\\vx_k) . The minimizer of the quadratic program is well-defined if \\nabla^2 f_k \u227b 0 \\nabla^2 f_k \u227b 0 . In that case, \\vx_{k+1} = \\vx_{k} - (\\nabla^2 f_k)^{-1} \\nabla f_k = \\vx_k + \\vd_k, \\vx_{k+1} = \\vx_{k} - (\\nabla^2 f_k)^{-1} \\nabla f_k = \\vx_k + \\vd_k, where \\vd_k \\vd_k solves \\nabla^2 f_k d = -\\nabla f_k \\nabla^2 f_k d = -\\nabla f_k . The pure Newton's method is given by the following algorithm: Pure Newton's method Given: \\vx_0 \\vx_0 , tol > 0 for k = 0, 1, 2,\\dots k = 0, 1, 2,\\dots \\vg_k \u2190 \u2207f(\\vx_k) \\vg_k \u2190 \u2207f(\\vx_k) (compute gradient) \\mH_k \u2190 \u2207^2f(\\vx_k) \\mH_k \u2190 \u2207^2f(\\vx_k) (compute Hessian) \\vd_k \\vd_k solves \\mH_k \\vd = -\\vg_k \\mH_k \\vd = -\\vg_k (compute Newton step) \\vx_{k+1} \\leftarrow \\vx_k + \\vd_k \\vx_{k+1} \\leftarrow \\vx_k + \\vd_k STOP if \\|\\nabla f(\\vx_{k+1})\\|_2 \\leq \\|\\nabla f(\\vx_{k+1})\\|_2 \\leq tol Convergence of (pure) Newton's method \u00b6 Convergence of (pure) Newton's method requires the Hessian at every iteration to be positive definite, i.e. \\nabla^2f_k \\succ 0 \\nabla^2f_k \\succ 0 for all iterations k. This positive definiteness of the Hessian ensures that the Newton step is a descent direction since \\nabla^2f_k \\vd = -\\nabla f_k \\Rightarrow \\vd\\trans\\nabla^2f_k\\vd = -\\vd\\trans\\nabla f_k \\Rightarrow \\nabla f_k\\trans\\vd <.0 \\nabla^2f_k \\vd = -\\nabla f_k \\Rightarrow \\vd\\trans\\nabla^2f_k\\vd = -\\vd\\trans\\nabla f_k \\Rightarrow \\nabla f_k\\trans\\vd <.0 However, positive definiteness is not sufficient for convergence, i.e. the pure Newton method may still diverge even if \\nabla^2 f_k \\succ 0 \\nabla^2 f_k \\succ 0 . For example, consider the function f(x) = \\sqrt{1+x^2} f(x) = \\sqrt{1+x^2} . The derivative of f f is f'(x) = x(1+x^2)^{-\\frac{1}{2}} f'(x) = x(1+x^2)^{-\\frac{1}{2}} and the second-derivative is f''(x) = (1+x^2)^{-\\frac{3}{2}} f''(x) = (1+x^2)^{-\\frac{3}{2}} . So the Newton iteration is x_{k+1} = x_k - \\frac{f'(x_k)}{f''(x_k)} = -x_k^3. x_{k+1} = x_k - \\frac{f'(x_k)}{f''(x_k)} = -x_k^3. Note that x_{k} \\rightarrow \\left\\{\\begin{array}{ll}0 & \\text{if } |x_0|<1\\\\ \\pm 1 & \\text{if } |x_0|=1\\\\ \\pm\\infty & \\text{if } |x_0|>1\\end{array}\\right. x_{k} \\rightarrow \\left\\{\\begin{array}{ll}0 & \\text{if } |x_0|<1\\\\ \\pm 1 & \\text{if } |x_0|=1\\\\ \\pm\\infty & \\text{if } |x_0|>1\\end{array}\\right. Lemma (Convergence of pure Newton's method) Suppose f:\\R^n\\rightarrow \\R f:\\R^n\\rightarrow \\R is twice continuously differentiable and there exists an \\epsilon >0 \\epsilon >0 such that \\nabla^2 f(x)\\succeq \\epsilon \\mI \\nabla^2 f(x)\\succeq \\epsilon \\mI for all \\vx \\vx , there exists some L>0 L>0 such that \\|\\nabla^2 f(\\vx) -\\nabla^2 f(\\vy)\\|_2 \\leq L \\|\\vx-\\vy\\|_2 \\|\\nabla^2 f(\\vx) -\\nabla^2 f(\\vy)\\|_2 \\leq L \\|\\vx-\\vy\\|_2 for all \\vx \\vx and \\vy \\vy . If the minimizer \\vx_* \\vx_* of f f is unique, then the pure Newton iterations satisfies \\|\\vx_{k+1} - \\vx_*\\|_2 \\leq \\frac{L}{2\\epsilon} \\|\\vx-\\vx_*\\|_2^2. \\|\\vx_{k+1} - \\vx_*\\|_2 \\leq \\frac{L}{2\\epsilon} \\|\\vx-\\vx_*\\|_2^2. In addition, if \\|\\vx_0 - \\vx_*\\|_2 \\leq \\frac{\\epsilon}{L} \\|\\vx_0 - \\vx_*\\|_2 \\leq \\frac{\\epsilon}{L} , then each iterate k k satisfies \\|\\vx_{k} - \\vx_*\\|_2 \\leq \\left(\\frac{L}{2\\epsilon}\\right)\\left(\\frac{1}{4}\\right)^{2^k}. \\|\\vx_{k} - \\vx_*\\|_2 \\leq \\left(\\frac{L}{2\\epsilon}\\right)\\left(\\frac{1}{4}\\right)^{2^k}. Rates of convergence \u00b6 Rates of convergence measures how fast a sequence ${\\vx_k}_{k=0,1,2,\\dots} converges to its limit (assuming limit exists). Suppose \\vx_k \\rightarrow \\vx_* \\vx_k \\rightarrow \\vx_* , i.e. \\lim_{k\\rightarrow \\infty} \\|\\vx_k -\\vx_*\\| = 0 \\lim_{k\\rightarrow \\infty} \\|\\vx_k -\\vx_*\\| = 0 . Linear Convergence: There exists a number \\mu \\in (0,1) \\mu \\in (0,1) such that \\lim_{k\\rightarrow \\infty} \\frac{\\|\\vx_{k+1}-\\vx_*\\|}{\\|\\vx_k-\\vx_*\\|} = \\mu \\lim_{k\\rightarrow \\infty} \\frac{\\|\\vx_{k+1}-\\vx_*\\|}{\\|\\vx_k-\\vx_*\\|} = \\mu . For example, consider the sequence x_k = 2^{-k} x_k = 2^{-k} . Sublinear Convergence: A sequence \\{\\vx_k\\}_{k=0,1,2,\\dots} \\{\\vx_k\\}_{k=0,1,2,\\dots} converges sublinearly to \\vx_* \\vx_* if \\lim_{k\\rightarrow \\infty} \\frac{\\|\\vx_{k+1}-\\vx_*\\|}{\\|\\vx_k-\\vx_*\\|} = 1 \\lim_{k\\rightarrow \\infty} \\frac{\\|\\vx_{k+1}-\\vx_*\\|}{\\|\\vx_k-\\vx_*\\|} = 1 . For example, consider the sequence x_k = \\frac{1}{k+1} x_k = \\frac{1}{k+1} . Superlinear Convergence: A sequence \\{\\vx_k\\}_{k=0,1,2,\\dots} \\{\\vx_k\\}_{k=0,1,2,\\dots} converges superlinearly with order q q to \\vx_* \\vx_* if there exists a number M>0 M>0 such that \\lim_{k\\rightarrow \\infty} \\frac{\\|\\vx_{k+1}-\\vx_*\\|}{\\|\\vx_k-\\vx_*\\|^q} \\leq M \\lim_{k\\rightarrow \\infty} \\frac{\\|\\vx_{k+1}-\\vx_*\\|}{\\|\\vx_k-\\vx_*\\|^q} \\leq M . For example, consider the sequence x_k = \\left(\\frac{1}{2}\\right)^{2^k} x_k = \\left(\\frac{1}{2}\\right)^{2^k} . If q=2 q=2 , we say the convergence of quadratic.","title":"Newton's method"},{"location":"notes/Newtons_method/#newtons-method","text":"In Newton's method, we minimize a quadratic approximation of the function at each iterate. Consider the problem \\minimize_{\\vx \\in \\R^n} f(\\vx) \\minimize_{\\vx \\in \\R^n} f(\\vx) where f:\\R^n\\rightarrow \\R f:\\R^n\\rightarrow \\R is a twice continuously differentiable function. Given an iterate \\vx_k \\vx_k , define \\vx_{k+1} \\vx_{k+1} as the minimizer of the quadratic program given by \\vx_{k+1} = \\argmin_{\\vx} \\{f_k + \\nabla f_k\\trans(\\vx - \\vx_k) - \\frac{1}{2}(\\vx-\\vx_k)\\trans\\nabla^2f_k(\\vx-\\vx_k)\\}. \\vx_{k+1} = \\argmin_{\\vx} \\{f_k + \\nabla f_k\\trans(\\vx - \\vx_k) - \\frac{1}{2}(\\vx-\\vx_k)\\trans\\nabla^2f_k(\\vx-\\vx_k)\\}. In the above equation, f_k := f(\\vx_k) f_k := f(\\vx_k) . The minimizer of the quadratic program is well-defined if \\nabla^2 f_k \u227b 0 \\nabla^2 f_k \u227b 0 . In that case, \\vx_{k+1} = \\vx_{k} - (\\nabla^2 f_k)^{-1} \\nabla f_k = \\vx_k + \\vd_k, \\vx_{k+1} = \\vx_{k} - (\\nabla^2 f_k)^{-1} \\nabla f_k = \\vx_k + \\vd_k, where \\vd_k \\vd_k solves \\nabla^2 f_k d = -\\nabla f_k \\nabla^2 f_k d = -\\nabla f_k . The pure Newton's method is given by the following algorithm: Pure Newton's method Given: \\vx_0 \\vx_0 , tol > 0 for k = 0, 1, 2,\\dots k = 0, 1, 2,\\dots \\vg_k \u2190 \u2207f(\\vx_k) \\vg_k \u2190 \u2207f(\\vx_k) (compute gradient) \\mH_k \u2190 \u2207^2f(\\vx_k) \\mH_k \u2190 \u2207^2f(\\vx_k) (compute Hessian) \\vd_k \\vd_k solves \\mH_k \\vd = -\\vg_k \\mH_k \\vd = -\\vg_k (compute Newton step) \\vx_{k+1} \\leftarrow \\vx_k + \\vd_k \\vx_{k+1} \\leftarrow \\vx_k + \\vd_k STOP if \\|\\nabla f(\\vx_{k+1})\\|_2 \\leq \\|\\nabla f(\\vx_{k+1})\\|_2 \\leq tol","title":"Newton's Method"},{"location":"notes/Newtons_method/#convergence-of-pure-newtons-method","text":"Convergence of (pure) Newton's method requires the Hessian at every iteration to be positive definite, i.e. \\nabla^2f_k \\succ 0 \\nabla^2f_k \\succ 0 for all iterations k. This positive definiteness of the Hessian ensures that the Newton step is a descent direction since \\nabla^2f_k \\vd = -\\nabla f_k \\Rightarrow \\vd\\trans\\nabla^2f_k\\vd = -\\vd\\trans\\nabla f_k \\Rightarrow \\nabla f_k\\trans\\vd <.0 \\nabla^2f_k \\vd = -\\nabla f_k \\Rightarrow \\vd\\trans\\nabla^2f_k\\vd = -\\vd\\trans\\nabla f_k \\Rightarrow \\nabla f_k\\trans\\vd <.0 However, positive definiteness is not sufficient for convergence, i.e. the pure Newton method may still diverge even if \\nabla^2 f_k \\succ 0 \\nabla^2 f_k \\succ 0 . For example, consider the function f(x) = \\sqrt{1+x^2} f(x) = \\sqrt{1+x^2} . The derivative of f f is f'(x) = x(1+x^2)^{-\\frac{1}{2}} f'(x) = x(1+x^2)^{-\\frac{1}{2}} and the second-derivative is f''(x) = (1+x^2)^{-\\frac{3}{2}} f''(x) = (1+x^2)^{-\\frac{3}{2}} . So the Newton iteration is x_{k+1} = x_k - \\frac{f'(x_k)}{f''(x_k)} = -x_k^3. x_{k+1} = x_k - \\frac{f'(x_k)}{f''(x_k)} = -x_k^3. Note that x_{k} \\rightarrow \\left\\{\\begin{array}{ll}0 & \\text{if } |x_0|<1\\\\ \\pm 1 & \\text{if } |x_0|=1\\\\ \\pm\\infty & \\text{if } |x_0|>1\\end{array}\\right. x_{k} \\rightarrow \\left\\{\\begin{array}{ll}0 & \\text{if } |x_0|<1\\\\ \\pm 1 & \\text{if } |x_0|=1\\\\ \\pm\\infty & \\text{if } |x_0|>1\\end{array}\\right. Lemma (Convergence of pure Newton's method) Suppose f:\\R^n\\rightarrow \\R f:\\R^n\\rightarrow \\R is twice continuously differentiable and there exists an \\epsilon >0 \\epsilon >0 such that \\nabla^2 f(x)\\succeq \\epsilon \\mI \\nabla^2 f(x)\\succeq \\epsilon \\mI for all \\vx \\vx , there exists some L>0 L>0 such that \\|\\nabla^2 f(\\vx) -\\nabla^2 f(\\vy)\\|_2 \\leq L \\|\\vx-\\vy\\|_2 \\|\\nabla^2 f(\\vx) -\\nabla^2 f(\\vy)\\|_2 \\leq L \\|\\vx-\\vy\\|_2 for all \\vx \\vx and \\vy \\vy . If the minimizer \\vx_* \\vx_* of f f is unique, then the pure Newton iterations satisfies \\|\\vx_{k+1} - \\vx_*\\|_2 \\leq \\frac{L}{2\\epsilon} \\|\\vx-\\vx_*\\|_2^2. \\|\\vx_{k+1} - \\vx_*\\|_2 \\leq \\frac{L}{2\\epsilon} \\|\\vx-\\vx_*\\|_2^2. In addition, if \\|\\vx_0 - \\vx_*\\|_2 \\leq \\frac{\\epsilon}{L} \\|\\vx_0 - \\vx_*\\|_2 \\leq \\frac{\\epsilon}{L} , then each iterate k k satisfies \\|\\vx_{k} - \\vx_*\\|_2 \\leq \\left(\\frac{L}{2\\epsilon}\\right)\\left(\\frac{1}{4}\\right)^{2^k}. \\|\\vx_{k} - \\vx_*\\|_2 \\leq \\left(\\frac{L}{2\\epsilon}\\right)\\left(\\frac{1}{4}\\right)^{2^k}.","title":"Convergence of (pure) Newton's method"},{"location":"notes/Newtons_method/#rates-of-convergence","text":"Rates of convergence measures how fast a sequence ${\\vx_k}_{k=0,1,2,\\dots} converges to its limit (assuming limit exists). Suppose \\vx_k \\rightarrow \\vx_* \\vx_k \\rightarrow \\vx_* , i.e. \\lim_{k\\rightarrow \\infty} \\|\\vx_k -\\vx_*\\| = 0 \\lim_{k\\rightarrow \\infty} \\|\\vx_k -\\vx_*\\| = 0 . Linear Convergence: There exists a number \\mu \\in (0,1) \\mu \\in (0,1) such that \\lim_{k\\rightarrow \\infty} \\frac{\\|\\vx_{k+1}-\\vx_*\\|}{\\|\\vx_k-\\vx_*\\|} = \\mu \\lim_{k\\rightarrow \\infty} \\frac{\\|\\vx_{k+1}-\\vx_*\\|}{\\|\\vx_k-\\vx_*\\|} = \\mu . For example, consider the sequence x_k = 2^{-k} x_k = 2^{-k} . Sublinear Convergence: A sequence \\{\\vx_k\\}_{k=0,1,2,\\dots} \\{\\vx_k\\}_{k=0,1,2,\\dots} converges sublinearly to \\vx_* \\vx_* if \\lim_{k\\rightarrow \\infty} \\frac{\\|\\vx_{k+1}-\\vx_*\\|}{\\|\\vx_k-\\vx_*\\|} = 1 \\lim_{k\\rightarrow \\infty} \\frac{\\|\\vx_{k+1}-\\vx_*\\|}{\\|\\vx_k-\\vx_*\\|} = 1 . For example, consider the sequence x_k = \\frac{1}{k+1} x_k = \\frac{1}{k+1} . Superlinear Convergence: A sequence \\{\\vx_k\\}_{k=0,1,2,\\dots} \\{\\vx_k\\}_{k=0,1,2,\\dots} converges superlinearly with order q q to \\vx_* \\vx_* if there exists a number M>0 M>0 such that \\lim_{k\\rightarrow \\infty} \\frac{\\|\\vx_{k+1}-\\vx_*\\|}{\\|\\vx_k-\\vx_*\\|^q} \\leq M \\lim_{k\\rightarrow \\infty} \\frac{\\|\\vx_{k+1}-\\vx_*\\|}{\\|\\vx_k-\\vx_*\\|^q} \\leq M . For example, consider the sequence x_k = \\left(\\frac{1}{2}\\right)^{2^k} x_k = \\left(\\frac{1}{2}\\right)^{2^k} . If q=2 q=2 , we say the convergence of quadratic.","title":"Rates of convergence"},{"location":"notes/Non-linear_LS/","text":"Non-linear least squares \u00b6 The non-linear least squares problem is \\begin{equation}\\label{Non-linearleastsquares_prob} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|r(\\vx)\\|_2^2, \\end{equation} \\begin{equation}\\label{Non-linearleastsquares_prob} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|r(\\vx)\\|_2^2, \\end{equation} where r:\\R^n\u2192\\R^m r:\\R^n\u2192\\R^m is the residual vector. The i i th component of residual vector is r_{i}(\\vx):\\R^n\u2192\\R r_{i}(\\vx):\\R^n\u2192\\R . The non-linear least squares problem reduces to the linear least squares problem if r r is affine, i.e. r(\\vx) = \\mA\\vx-\\vb r(\\vx) = \\mA\\vx-\\vb . Example: Position estimation from ranges Let \\vx \\in \\R^2 \\vx \\in \\R^2 be an unknown vector. Fix m m beacon positions \\vb_{i} \\in \\R^2,\\ i = 1,\\dots,m \\vb_{i} \\in \\R^2,\\ i = 1,\\dots,m . Suppose we have noisy measurements \\vrho \\in \\R^m \\vrho \\in \\R^m of 2 2 -norm distance between a becon \\vb_{i} \\vb_{i} and the unknown signal \\vx \\vx , i.e. $$ \u03c1_{i} = |\\vx- \\vb|_2 + \u03bd_i \\quad \\text{for } i=1,\\dots,m. $$ Here, \\vnu \\in \\R^m \\vnu \\in \\R^m is noise/measurement error vector. The position estimation from ranges problem is to estimate \\vx \\vx given \\vrho \\vrho and \\vb_i, \\ i = 1,\\dots, m \\vb_i, \\ i = 1,\\dots, m . A natural approach to solve this problem is by finding \\vx \\vx that minimizes \\sum_{i=1}^m(\u03c1_{i} - \\|\\vx- \\vb\\|_2)^2 \\sum_{i=1}^m(\u03c1_{i} - \\|\\vx- \\vb\\|_2)^2 . Define r_i(\\vx) := \u03c1_{i} - \\|\\vx- \\vb\\|_2 r_i(\\vx) := \u03c1_{i} - \\|\\vx- \\vb\\|_2 . Then we can estimmate \\vx \\vx by solving the non-linear least squares problem \\min_{\\vx\\in \\R^n} \\frac{1}{2}\\|r(\\vx)\\|_2^2. \\min_{\\vx\\in \\R^n} \\frac{1}{2}\\|r(\\vx)\\|_2^2. In contrast to linear least squares program, the non-linear least squares program generally contain both global and local minimizers. We can will use the following approach to find a minimizer of NLLS. Gauss-Newton method for NLLS Given starting guess \\vx^{(0)} \\vx^{(0)} Repeat until covergence: linearize r(x) r(x) near current guess \\bar{\\vx} = \\vx^{k} \\bar{\\vx} = \\vx^{k} solve a linear least squares problem to get the next guess \\vx^{k+1} \\vx^{k+1} , Linearization of residual \u00b6 We can solve non-linear least squares problem \\eqref{Non-linearleastsquares_prob} by solving a sequence of linear least squares problem. These linear least squares subproblem results from linearization of r(\\vx) r(\\vx) at current estimate of the ground truth \\vx \\vx . The linear approximation of r(\\vx) r(\\vx) at a point \\bar{\\vx} \\in \\R^n \\bar{\\vx} \\in \\R^n is r(\\vx) = \\bmat r_1(\\vx)\\\\\\vdots\\\\ r_n(\\vx)\\emat \\approx \\bmat r_1(\\bar{\\vx}) +\\nabla r_1(\\bar{\\vx})\\trans(\\vx - \\bar{\\vx}) \\\\ \\vdots \\\\ r_m(\\bar{\\vx}) +\\nabla r_m(\\bar{\\vx})\\trans(\\vx - \\bar{\\vx}) \\emat = A(\\bar{\\vx}) \\vx -b(\\bar{\\vx}), r(\\vx) = \\bmat r_1(\\vx)\\\\\\vdots\\\\ r_n(\\vx)\\emat \\approx \\bmat r_1(\\bar{\\vx}) +\\nabla r_1(\\bar{\\vx})\\trans(\\vx - \\bar{\\vx}) \\\\ \\vdots \\\\ r_m(\\bar{\\vx}) +\\nabla r_m(\\bar{\\vx})\\trans(\\vx - \\bar{\\vx}) \\emat = A(\\bar{\\vx}) \\vx -b(\\bar{\\vx}), where A(\\bar{\\vx})\\in\\R^{m\\times n} A(\\bar{\\vx})\\in\\R^{m\\times n} is the Jacobian of the mappring r(x) r(x) at \\bar{\\vx} \\bar{\\vx} and b(\\bar{\\vx}) = A(\\bar{\\vx})\\vx - r(\\bar{\\vx}) \\in \\R^m b(\\bar{\\vx}) = A(\\bar{\\vx})\\vx - r(\\bar{\\vx}) \\in \\R^m . The Jacobian of r(x) r(x) at \\bar{\\vx} \\bar{\\vx} is A(\\bar{\\vx}) = \\bmat \\nabla r_1(\\bar{\\vx})\\trans\\\\ \\vdots \\\\ \\nabla r_m(\\bar{\\vx})\\trans\\emat. A(\\bar{\\vx}) = \\bmat \\nabla r_1(\\bar{\\vx})\\trans\\\\ \\vdots \\\\ \\nabla r_m(\\bar{\\vx})\\trans\\emat. We get the following minimization program after replacing r(\\vx) r(\\vx) with its linear approximation at \\vx^{(k)} \\vx^{(k)} : \\vx^{(k+1)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|A(\\vx^{(k)})\\vx - b(\\vx^{(k)})\\|_2^2. \\vx^{(k+1)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|A(\\vx^{(k)})\\vx - b(\\vx^{(k)})\\|_2^2. Starting at a current estimate \\vx^{(k)} \\vx^{(k)} , we can determine the \\vx^{(k+1)} \\vx^{(k+1)} by solving the above linear least squares program. Dampening \u00b6 For ease of exposition, let \\bar{\\mA} = A(\\vx^{(k)}), \\quad \\bar{\\vb} = b(\\vx^{(k)}), \\text{ and } \\bar{\\vr} = r(\\vx^{(k)}). \\bar{\\mA} = A(\\vx^{(k)}), \\quad \\bar{\\vb} = b(\\vx^{(k)}), \\text{ and } \\bar{\\vr} = r(\\vx^{(k)}). We assume that \\bar{\\mA} \\bar{\\mA} is full rank. Consider \\begin{align*} \\vx^{(k+1)} = &\\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vb}\\|_2^2\\\\ =& (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vb}\\\\ =& (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans(\\bar{\\mA}\\vx^{(k)} - \\bar{\\vr})\\\\ = &\\vx^{(k)} - (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vr} \\end{align*} \\begin{align*} \\vx^{(k+1)} = &\\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vb}\\|_2^2\\\\ =& (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vb}\\\\ =& (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans(\\bar{\\mA}\\vx^{(k)} - \\bar{\\vr})\\\\ = &\\vx^{(k)} - (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vr} \\end{align*} Here, \\vx^{(k+1)} \\vx^{(k+1)} is the k+1 k+1 Gauss-Newton estimate. Note that (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vr} (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vr} solves \\min_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vr}\\|_2^2 \\min_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vr}\\|_2^2 . Let \\vz^{(k)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vr}\\|_2^2. \\vz^{(k)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vr}\\|_2^2. The dampened Gauss-Newton step is \\vx^{(k+1)} = \\vx^{(k)} - \\alpha \\vz^{(k)}, \\vx^{(k+1)} = \\vx^{(k)} - \\alpha \\vz^{(k)}, where \\alpha \\in (0,1] \\alpha \\in (0,1] . Dampened Gauss-Newton method for NLLS Given starting guess \\vx^{(0)} \\vx^{(0)} Repeat until covergence: linearize r(x) r(x) near current guess \\bar{\\vx} = \\vx^{k} \\bar{\\vx} = \\vx^{k} : \\quad r(\\vx) \\approx r(\\bar{\\vx}) - A(\\bar{\\vx})(\\vx-\\bar{\\vx}) \\quad r(\\vx) \\approx r(\\bar{\\vx}) - A(\\bar{\\vx})(\\vx-\\bar{\\vx}) solve a linear least squares problem: \\quad \\vz^{(k)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n}\\|A(\\bar{\\vx})\\vx - r(\\bar{\\vx})\\|_2^2 \\quad \\vz^{(k)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n}\\|A(\\bar{\\vx})\\vx - r(\\bar{\\vx})\\|_2^2 take damped step: \\quad \\vx^{(k+1)} = \\vx^{(k)} - \\alpha^{(k)}\\vz^{(k)}, \\quad 0<\\alpha^{(k)}\\leq 1 \\quad \\vx^{(k+1)} = \\vx^{(k)} - \\alpha^{(k)}\\vz^{(k)}, \\quad 0<\\alpha^{(k)}\\leq 1 until converged","title":"Non-linear least squares"},{"location":"notes/Non-linear_LS/#non-linear-least-squares","text":"The non-linear least squares problem is \\begin{equation}\\label{Non-linearleastsquares_prob} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|r(\\vx)\\|_2^2, \\end{equation} \\begin{equation}\\label{Non-linearleastsquares_prob} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|r(\\vx)\\|_2^2, \\end{equation} where r:\\R^n\u2192\\R^m r:\\R^n\u2192\\R^m is the residual vector. The i i th component of residual vector is r_{i}(\\vx):\\R^n\u2192\\R r_{i}(\\vx):\\R^n\u2192\\R . The non-linear least squares problem reduces to the linear least squares problem if r r is affine, i.e. r(\\vx) = \\mA\\vx-\\vb r(\\vx) = \\mA\\vx-\\vb . Example: Position estimation from ranges Let \\vx \\in \\R^2 \\vx \\in \\R^2 be an unknown vector. Fix m m beacon positions \\vb_{i} \\in \\R^2,\\ i = 1,\\dots,m \\vb_{i} \\in \\R^2,\\ i = 1,\\dots,m . Suppose we have noisy measurements \\vrho \\in \\R^m \\vrho \\in \\R^m of 2 2 -norm distance between a becon \\vb_{i} \\vb_{i} and the unknown signal \\vx \\vx , i.e. $$ \u03c1_{i} = |\\vx- \\vb|_2 + \u03bd_i \\quad \\text{for } i=1,\\dots,m. $$ Here, \\vnu \\in \\R^m \\vnu \\in \\R^m is noise/measurement error vector. The position estimation from ranges problem is to estimate \\vx \\vx given \\vrho \\vrho and \\vb_i, \\ i = 1,\\dots, m \\vb_i, \\ i = 1,\\dots, m . A natural approach to solve this problem is by finding \\vx \\vx that minimizes \\sum_{i=1}^m(\u03c1_{i} - \\|\\vx- \\vb\\|_2)^2 \\sum_{i=1}^m(\u03c1_{i} - \\|\\vx- \\vb\\|_2)^2 . Define r_i(\\vx) := \u03c1_{i} - \\|\\vx- \\vb\\|_2 r_i(\\vx) := \u03c1_{i} - \\|\\vx- \\vb\\|_2 . Then we can estimmate \\vx \\vx by solving the non-linear least squares problem \\min_{\\vx\\in \\R^n} \\frac{1}{2}\\|r(\\vx)\\|_2^2. \\min_{\\vx\\in \\R^n} \\frac{1}{2}\\|r(\\vx)\\|_2^2. In contrast to linear least squares program, the non-linear least squares program generally contain both global and local minimizers. We can will use the following approach to find a minimizer of NLLS. Gauss-Newton method for NLLS Given starting guess \\vx^{(0)} \\vx^{(0)} Repeat until covergence: linearize r(x) r(x) near current guess \\bar{\\vx} = \\vx^{k} \\bar{\\vx} = \\vx^{k} solve a linear least squares problem to get the next guess \\vx^{k+1} \\vx^{k+1} ,","title":"Non-linear least squares"},{"location":"notes/Non-linear_LS/#linearization-of-residual","text":"We can solve non-linear least squares problem \\eqref{Non-linearleastsquares_prob} by solving a sequence of linear least squares problem. These linear least squares subproblem results from linearization of r(\\vx) r(\\vx) at current estimate of the ground truth \\vx \\vx . The linear approximation of r(\\vx) r(\\vx) at a point \\bar{\\vx} \\in \\R^n \\bar{\\vx} \\in \\R^n is r(\\vx) = \\bmat r_1(\\vx)\\\\\\vdots\\\\ r_n(\\vx)\\emat \\approx \\bmat r_1(\\bar{\\vx}) +\\nabla r_1(\\bar{\\vx})\\trans(\\vx - \\bar{\\vx}) \\\\ \\vdots \\\\ r_m(\\bar{\\vx}) +\\nabla r_m(\\bar{\\vx})\\trans(\\vx - \\bar{\\vx}) \\emat = A(\\bar{\\vx}) \\vx -b(\\bar{\\vx}), r(\\vx) = \\bmat r_1(\\vx)\\\\\\vdots\\\\ r_n(\\vx)\\emat \\approx \\bmat r_1(\\bar{\\vx}) +\\nabla r_1(\\bar{\\vx})\\trans(\\vx - \\bar{\\vx}) \\\\ \\vdots \\\\ r_m(\\bar{\\vx}) +\\nabla r_m(\\bar{\\vx})\\trans(\\vx - \\bar{\\vx}) \\emat = A(\\bar{\\vx}) \\vx -b(\\bar{\\vx}), where A(\\bar{\\vx})\\in\\R^{m\\times n} A(\\bar{\\vx})\\in\\R^{m\\times n} is the Jacobian of the mappring r(x) r(x) at \\bar{\\vx} \\bar{\\vx} and b(\\bar{\\vx}) = A(\\bar{\\vx})\\vx - r(\\bar{\\vx}) \\in \\R^m b(\\bar{\\vx}) = A(\\bar{\\vx})\\vx - r(\\bar{\\vx}) \\in \\R^m . The Jacobian of r(x) r(x) at \\bar{\\vx} \\bar{\\vx} is A(\\bar{\\vx}) = \\bmat \\nabla r_1(\\bar{\\vx})\\trans\\\\ \\vdots \\\\ \\nabla r_m(\\bar{\\vx})\\trans\\emat. A(\\bar{\\vx}) = \\bmat \\nabla r_1(\\bar{\\vx})\\trans\\\\ \\vdots \\\\ \\nabla r_m(\\bar{\\vx})\\trans\\emat. We get the following minimization program after replacing r(\\vx) r(\\vx) with its linear approximation at \\vx^{(k)} \\vx^{(k)} : \\vx^{(k+1)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|A(\\vx^{(k)})\\vx - b(\\vx^{(k)})\\|_2^2. \\vx^{(k+1)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|A(\\vx^{(k)})\\vx - b(\\vx^{(k)})\\|_2^2. Starting at a current estimate \\vx^{(k)} \\vx^{(k)} , we can determine the \\vx^{(k+1)} \\vx^{(k+1)} by solving the above linear least squares program.","title":"Linearization of residual"},{"location":"notes/Non-linear_LS/#dampening","text":"For ease of exposition, let \\bar{\\mA} = A(\\vx^{(k)}), \\quad \\bar{\\vb} = b(\\vx^{(k)}), \\text{ and } \\bar{\\vr} = r(\\vx^{(k)}). \\bar{\\mA} = A(\\vx^{(k)}), \\quad \\bar{\\vb} = b(\\vx^{(k)}), \\text{ and } \\bar{\\vr} = r(\\vx^{(k)}). We assume that \\bar{\\mA} \\bar{\\mA} is full rank. Consider \\begin{align*} \\vx^{(k+1)} = &\\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vb}\\|_2^2\\\\ =& (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vb}\\\\ =& (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans(\\bar{\\mA}\\vx^{(k)} - \\bar{\\vr})\\\\ = &\\vx^{(k)} - (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vr} \\end{align*} \\begin{align*} \\vx^{(k+1)} = &\\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vb}\\|_2^2\\\\ =& (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vb}\\\\ =& (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans(\\bar{\\mA}\\vx^{(k)} - \\bar{\\vr})\\\\ = &\\vx^{(k)} - (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vr} \\end{align*} Here, \\vx^{(k+1)} \\vx^{(k+1)} is the k+1 k+1 Gauss-Newton estimate. Note that (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vr} (\\bar{\\mA}\\trans\\bar{\\mA})^{-1}\\bar{\\mA}\\trans\\bar{\\vr} solves \\min_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vr}\\|_2^2 \\min_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vr}\\|_2^2 . Let \\vz^{(k)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vr}\\|_2^2. \\vz^{(k)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n} \\|\\bar{\\mA}\\vx - \\bar{\\vr}\\|_2^2. The dampened Gauss-Newton step is \\vx^{(k+1)} = \\vx^{(k)} - \\alpha \\vz^{(k)}, \\vx^{(k+1)} = \\vx^{(k)} - \\alpha \\vz^{(k)}, where \\alpha \\in (0,1] \\alpha \\in (0,1] . Dampened Gauss-Newton method for NLLS Given starting guess \\vx^{(0)} \\vx^{(0)} Repeat until covergence: linearize r(x) r(x) near current guess \\bar{\\vx} = \\vx^{k} \\bar{\\vx} = \\vx^{k} : \\quad r(\\vx) \\approx r(\\bar{\\vx}) - A(\\bar{\\vx})(\\vx-\\bar{\\vx}) \\quad r(\\vx) \\approx r(\\bar{\\vx}) - A(\\bar{\\vx})(\\vx-\\bar{\\vx}) solve a linear least squares problem: \\quad \\vz^{(k)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n}\\|A(\\bar{\\vx})\\vx - r(\\bar{\\vx})\\|_2^2 \\quad \\vz^{(k)} = \\mathop{\\text{argmin}}_{\\vx\\in\\R^n}\\|A(\\bar{\\vx})\\vx - r(\\bar{\\vx})\\|_2^2 take damped step: \\quad \\vx^{(k+1)} = \\vx^{(k)} - \\alpha^{(k)}\\vz^{(k)}, \\quad 0<\\alpha^{(k)}\\leq 1 \\quad \\vx^{(k+1)} = \\vx^{(k)} - \\alpha^{(k)}\\vz^{(k)}, \\quad 0<\\alpha^{(k)}\\leq 1 until converged","title":"Dampening"},{"location":"notes/QR_factorization/","text":"QR factorization \u00b6 Orthogonal and orthonormal vectors \u00b6 Let \\vx, \\vy\\in\\R^n \\vx, \\vy\\in\\R^n be any two vectors. By cosine identity, we have \\vx\\trans\\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) \\vx\\trans\\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) where, \\theta \\theta is the angle between \\vx \\vx and \\vy \\vy . So, \\vx \\vx and \\vy \\vy are orthogonal ( \\theta = 0 \\theta = 0 ), if \\vx\\trans\\vy = 0 \\vx\\trans\\vy = 0 . Furthermore, we say \\vx \\vx and \\vy \\vy are orthonomal if \\vx \\vx and \\vy \\vy have unit 2-norm and are orthogonal, i.e. \\vx\\trans\\vy = 0, \\quad \\vx\\trans\\vx =1 , \\quad \\vy\\trans\\vy = 1. \\vx\\trans\\vy = 0, \\quad \\vx\\trans\\vx =1 , \\quad \\vy\\trans\\vy = 1. Orthogonal matrices \u00b6 A matrix \\mQ \\mQ is orthogonal if it is square and its columns are all pairwise orthogonal. For an orthogonal matrix \\mQ = \\bmat\\vq_1 &\\dots&\\vq_n\\emat \\mQ = \\bmat\\vq_1 &\\dots&\\vq_n\\emat , we have \\mQ\\trans\\mQ = \\mQ\\mQ\\trans = \\mI. \\mQ\\trans\\mQ = \\mQ\\mQ\\trans = \\mI. Since, a matrix \\mB \\mB is the inverse of a matrix \\mA \\mA if \\mB\\mA =\\mA\\mB = \\mI \\mB\\mA =\\mA\\mB = \\mI , the inverse of an orthognal matrix is its transpose, i.e. \\mQ^{-1} = \\mQ\\trans. \\mQ^{-1} = \\mQ\\trans. Orthogonal matrices have many good properties. One such property is that inner products are invariant under orthogonal transfromations. So, for any vectors \\vx,\\ \\vy \\vx,\\ \\vy , we have (\\mQ\\vx)\\trans(\\mQ\\vy) = \\vx\\trans\\mQ\\trans\\mQ\\vy = \\vx\\trans\\vy. (\\mQ\\vx)\\trans(\\mQ\\vy) = \\vx\\trans\\mQ\\trans\\mQ\\vy = \\vx\\trans\\vy. This also implies that 2-norm is invariant to orthogonal transformations as \\|\\mQ\\vx\\|_2 = \\|\\vx\\|_2 \\|\\mQ\\vx\\|_2 = \\|\\vx\\|_2 . Another property of othogonal matrices is that their determinant is either 1 1 or -1 -1 . This can be observed from the fact that for a matrix \\mA \\mA and \\mB \\mB , we have \\det(\\mA\\mB) = \\det(\\mA)\\det(\\mB) \\det(\\mA\\mB) = \\det(\\mA)\\det(\\mB) and \\det(\\mA)= \\det(\\mA\\trans) \\det(\\mA)= \\det(\\mA\\trans) . So, from \\mQ\\trans\\mQ = \\mI \\mQ\\trans\\mQ = \\mI , we get \\det(\\mQ\\trans\\mQ) = \\det(\\mI) \\iff \\det(\\mQ)^2 = 1 \\iff \\det(\\mQ) = \u00b11. \\det(\\mQ\\trans\\mQ) = \\det(\\mI) \\iff \\det(\\mQ)^2 = 1 \\iff \\det(\\mQ) = \u00b11. QR factorization \u00b6 Let \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} . A factorization of \\mA \\mA with \\mA = \\mQ \\mR \\mA = \\mQ \\mR where \\mQ\\in\\R^{m\\times m} \\mQ\\in\\R^{m\\times m} is an orthogonal matrix and \\mR \\in \\R^{m\\times n} \\mR \\in \\R^{m\\times n} is an upper triangular matrix is called a QR QR factorization of \\mA \\mA . In the case with m\\geq n m\\geq n and \\rank(\\mA) = k \\leq n \\rank(\\mA) = k \\leq n , the QR QR facorization will have the following shape: In the above figure, \\mQ \\mQ is an orthogonal matrix. \\mR \\mR is a upper triangular matrix, i.e. R_{ij}=0 R_{ij}=0 whenever i>j i>j . \\hat{\\mQ} \\hat{\\mQ} spans the range of \\mA \\mA . \\bar{\\mQ} \\bar{\\mQ} spans the nullspace of \\mA\\trans \\mA\\trans . In Julia, we can compute the QR decompisition of a matrix using: m = 4 n = 3 A = randn ( m , n ) F = qr ( A ) Let \\mQ = \\bmat \\vq_1 \\dots \\vq_m \\emat \\mQ = \\bmat \\vq_1 \\dots \\vq_m \\emat . We can express the columns of \\mA \\mA as \\begin{align*} \\va_{1} &= r_{11} \\vq_{1}\\\\ \\va_{2} &= r_{12} \\vq_{1}+r_{22}\\vq_2\\\\ &\\vdots\\\\ \\va_{n} &= r_{1n} \\vq_{1} + r_{2n}\\vq_2+\\dots+r_{kn}\\vq_{k}\\\\ \\end{align*} \\begin{align*} \\va_{1} &= r_{11} \\vq_{1}\\\\ \\va_{2} &= r_{12} \\vq_{1}+r_{22}\\vq_2\\\\ &\\vdots\\\\ \\va_{n} &= r_{1n} \\vq_{1} + r_{2n}\\vq_2+\\dots+r_{kn}\\vq_{k}\\\\ \\end{align*} So, we can compactly write the matrix \\mA \\mA as \\mA = \\hat{\\mQ}\\hat{\\mR} \\mA = \\hat{\\mQ}\\hat{\\mR} . This is the reduced (thin or economode) QR factorization of \\mA \\mA . In the case when m\\geq n m\\geq n and \\mA \\mA is full rank, we get following figure: Solving least squares via QR \u00b6 The QR QR factorization can be used to solve the least squares problem \\min_{\\vx \\in\\R^n}\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2, \\min_{\\vx \\in\\R^n}\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2, where \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} and \\vb\\in \\R^m \\vb\\in \\R^m . We consider the case where m\\geq n m\\geq n , but QR factorization can be used to solve the other case as well. Consider \\begin{align*} \\|\\mA\\vx-\\vb\\|_2^2 &= (\\mA\\vx-\\vb)\\trans(\\mA\\vx-\\vb)\\\\ &= (\\mA\\vx-\\vb)\\trans\\mQ\\mQ\\trans(\\mA\\vx-\\vb)\\\\ & = \\|\\mQ\\trans(\\mA\\vx-\\vb)\\|_2^2\\\\ &=\\left\\|\\bmat\\hat{\\mR}\\\\\\vzero\\emat\\vx-\\bmat\\hat{\\mQ}\\trans\\\\\\bar{\\mQ}\\trans\\emat\\vb\\right\\|_2^2\\\\ & = \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2+\\|\\bar{\\mQ}\\trans\\vb\\|_2^2 \\end{align*} \\begin{align*} \\|\\mA\\vx-\\vb\\|_2^2 &= (\\mA\\vx-\\vb)\\trans(\\mA\\vx-\\vb)\\\\ &= (\\mA\\vx-\\vb)\\trans\\mQ\\mQ\\trans(\\mA\\vx-\\vb)\\\\ & = \\|\\mQ\\trans(\\mA\\vx-\\vb)\\|_2^2\\\\ &=\\left\\|\\bmat\\hat{\\mR}\\\\\\vzero\\emat\\vx-\\bmat\\hat{\\mQ}\\trans\\\\\\bar{\\mQ}\\trans\\emat\\vb\\right\\|_2^2\\\\ & = \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2+\\|\\bar{\\mQ}\\trans\\vb\\|_2^2 \\end{align*} So, minimizing \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2 \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2 will minimize \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2 \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2 . In the case \\mA \\mA is full rank, we get an invertible \\hat{\\mR} \\hat{\\mR} and the least squares solution which satisfies \\begin{equation}\\label{QR_leastsquares} \\hat{\\mR}\\vx = \\hat{\\mQ}\\trans\\vb, \\end{equation} \\begin{equation}\\label{QR_leastsquares} \\hat{\\mR}\\vx = \\hat{\\mQ}\\trans\\vb, \\end{equation} will be unique. In the case when \\mA \\mA is not full rank, there will be a infinitely many solutions to the least squares problem. For both cases, we can find a least squares solution by solving \\eqref{QR_leastsquares} via back substitution. The figure below shows the geometric prespective of using a QR factorization to solve the least squares problem. For every \\vb \\in \\R^m \\vb \\in \\R^m , \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb is the orthogonal projection of \\vb \\vb onto the \\range(\\hat{\\mQ}) = \\range(\\mA) \\range(\\hat{\\mQ}) = \\range(\\mA) . The least squares solution finds a point \\vx \\vx such that \\mA\\vx \\mA\\vx is equal tothe orthogonal projection of \\vb \\vb onto the \\range(\\mA) \\range(\\mA) . So, we get \\mA\\vx = \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb \\mA\\vx = \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb , which simplifies to \\eqref{QR_leastsquares}.","title":"QR factorization"},{"location":"notes/QR_factorization/#qr-factorization","text":"","title":"QR factorization"},{"location":"notes/QR_factorization/#orthogonal-and-orthonormal-vectors","text":"Let \\vx, \\vy\\in\\R^n \\vx, \\vy\\in\\R^n be any two vectors. By cosine identity, we have \\vx\\trans\\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) \\vx\\trans\\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) where, \\theta \\theta is the angle between \\vx \\vx and \\vy \\vy . So, \\vx \\vx and \\vy \\vy are orthogonal ( \\theta = 0 \\theta = 0 ), if \\vx\\trans\\vy = 0 \\vx\\trans\\vy = 0 . Furthermore, we say \\vx \\vx and \\vy \\vy are orthonomal if \\vx \\vx and \\vy \\vy have unit 2-norm and are orthogonal, i.e. \\vx\\trans\\vy = 0, \\quad \\vx\\trans\\vx =1 , \\quad \\vy\\trans\\vy = 1. \\vx\\trans\\vy = 0, \\quad \\vx\\trans\\vx =1 , \\quad \\vy\\trans\\vy = 1.","title":"Orthogonal and orthonormal vectors"},{"location":"notes/QR_factorization/#orthogonal-matrices","text":"A matrix \\mQ \\mQ is orthogonal if it is square and its columns are all pairwise orthogonal. For an orthogonal matrix \\mQ = \\bmat\\vq_1 &\\dots&\\vq_n\\emat \\mQ = \\bmat\\vq_1 &\\dots&\\vq_n\\emat , we have \\mQ\\trans\\mQ = \\mQ\\mQ\\trans = \\mI. \\mQ\\trans\\mQ = \\mQ\\mQ\\trans = \\mI. Since, a matrix \\mB \\mB is the inverse of a matrix \\mA \\mA if \\mB\\mA =\\mA\\mB = \\mI \\mB\\mA =\\mA\\mB = \\mI , the inverse of an orthognal matrix is its transpose, i.e. \\mQ^{-1} = \\mQ\\trans. \\mQ^{-1} = \\mQ\\trans. Orthogonal matrices have many good properties. One such property is that inner products are invariant under orthogonal transfromations. So, for any vectors \\vx,\\ \\vy \\vx,\\ \\vy , we have (\\mQ\\vx)\\trans(\\mQ\\vy) = \\vx\\trans\\mQ\\trans\\mQ\\vy = \\vx\\trans\\vy. (\\mQ\\vx)\\trans(\\mQ\\vy) = \\vx\\trans\\mQ\\trans\\mQ\\vy = \\vx\\trans\\vy. This also implies that 2-norm is invariant to orthogonal transformations as \\|\\mQ\\vx\\|_2 = \\|\\vx\\|_2 \\|\\mQ\\vx\\|_2 = \\|\\vx\\|_2 . Another property of othogonal matrices is that their determinant is either 1 1 or -1 -1 . This can be observed from the fact that for a matrix \\mA \\mA and \\mB \\mB , we have \\det(\\mA\\mB) = \\det(\\mA)\\det(\\mB) \\det(\\mA\\mB) = \\det(\\mA)\\det(\\mB) and \\det(\\mA)= \\det(\\mA\\trans) \\det(\\mA)= \\det(\\mA\\trans) . So, from \\mQ\\trans\\mQ = \\mI \\mQ\\trans\\mQ = \\mI , we get \\det(\\mQ\\trans\\mQ) = \\det(\\mI) \\iff \\det(\\mQ)^2 = 1 \\iff \\det(\\mQ) = \u00b11. \\det(\\mQ\\trans\\mQ) = \\det(\\mI) \\iff \\det(\\mQ)^2 = 1 \\iff \\det(\\mQ) = \u00b11.","title":"Orthogonal matrices"},{"location":"notes/QR_factorization/#qr-factorization_1","text":"Let \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} . A factorization of \\mA \\mA with \\mA = \\mQ \\mR \\mA = \\mQ \\mR where \\mQ\\in\\R^{m\\times m} \\mQ\\in\\R^{m\\times m} is an orthogonal matrix and \\mR \\in \\R^{m\\times n} \\mR \\in \\R^{m\\times n} is an upper triangular matrix is called a QR QR factorization of \\mA \\mA . In the case with m\\geq n m\\geq n and \\rank(\\mA) = k \\leq n \\rank(\\mA) = k \\leq n , the QR QR facorization will have the following shape: In the above figure, \\mQ \\mQ is an orthogonal matrix. \\mR \\mR is a upper triangular matrix, i.e. R_{ij}=0 R_{ij}=0 whenever i>j i>j . \\hat{\\mQ} \\hat{\\mQ} spans the range of \\mA \\mA . \\bar{\\mQ} \\bar{\\mQ} spans the nullspace of \\mA\\trans \\mA\\trans . In Julia, we can compute the QR decompisition of a matrix using: m = 4 n = 3 A = randn ( m , n ) F = qr ( A ) Let \\mQ = \\bmat \\vq_1 \\dots \\vq_m \\emat \\mQ = \\bmat \\vq_1 \\dots \\vq_m \\emat . We can express the columns of \\mA \\mA as \\begin{align*} \\va_{1} &= r_{11} \\vq_{1}\\\\ \\va_{2} &= r_{12} \\vq_{1}+r_{22}\\vq_2\\\\ &\\vdots\\\\ \\va_{n} &= r_{1n} \\vq_{1} + r_{2n}\\vq_2+\\dots+r_{kn}\\vq_{k}\\\\ \\end{align*} \\begin{align*} \\va_{1} &= r_{11} \\vq_{1}\\\\ \\va_{2} &= r_{12} \\vq_{1}+r_{22}\\vq_2\\\\ &\\vdots\\\\ \\va_{n} &= r_{1n} \\vq_{1} + r_{2n}\\vq_2+\\dots+r_{kn}\\vq_{k}\\\\ \\end{align*} So, we can compactly write the matrix \\mA \\mA as \\mA = \\hat{\\mQ}\\hat{\\mR} \\mA = \\hat{\\mQ}\\hat{\\mR} . This is the reduced (thin or economode) QR factorization of \\mA \\mA . In the case when m\\geq n m\\geq n and \\mA \\mA is full rank, we get following figure:","title":"QR factorization"},{"location":"notes/QR_factorization/#solving-least-squares-via-qr","text":"The QR QR factorization can be used to solve the least squares problem \\min_{\\vx \\in\\R^n}\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2, \\min_{\\vx \\in\\R^n}\\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2, where \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} and \\vb\\in \\R^m \\vb\\in \\R^m . We consider the case where m\\geq n m\\geq n , but QR factorization can be used to solve the other case as well. Consider \\begin{align*} \\|\\mA\\vx-\\vb\\|_2^2 &= (\\mA\\vx-\\vb)\\trans(\\mA\\vx-\\vb)\\\\ &= (\\mA\\vx-\\vb)\\trans\\mQ\\mQ\\trans(\\mA\\vx-\\vb)\\\\ & = \\|\\mQ\\trans(\\mA\\vx-\\vb)\\|_2^2\\\\ &=\\left\\|\\bmat\\hat{\\mR}\\\\\\vzero\\emat\\vx-\\bmat\\hat{\\mQ}\\trans\\\\\\bar{\\mQ}\\trans\\emat\\vb\\right\\|_2^2\\\\ & = \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2+\\|\\bar{\\mQ}\\trans\\vb\\|_2^2 \\end{align*} \\begin{align*} \\|\\mA\\vx-\\vb\\|_2^2 &= (\\mA\\vx-\\vb)\\trans(\\mA\\vx-\\vb)\\\\ &= (\\mA\\vx-\\vb)\\trans\\mQ\\mQ\\trans(\\mA\\vx-\\vb)\\\\ & = \\|\\mQ\\trans(\\mA\\vx-\\vb)\\|_2^2\\\\ &=\\left\\|\\bmat\\hat{\\mR}\\\\\\vzero\\emat\\vx-\\bmat\\hat{\\mQ}\\trans\\\\\\bar{\\mQ}\\trans\\emat\\vb\\right\\|_2^2\\\\ & = \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2+\\|\\bar{\\mQ}\\trans\\vb\\|_2^2 \\end{align*} So, minimizing \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2 \\|\\hat{\\mR}\\vx - \\hat{\\mQ}\\trans\\vb\\|_2^2 will minimize \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2 \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2 . In the case \\mA \\mA is full rank, we get an invertible \\hat{\\mR} \\hat{\\mR} and the least squares solution which satisfies \\begin{equation}\\label{QR_leastsquares} \\hat{\\mR}\\vx = \\hat{\\mQ}\\trans\\vb, \\end{equation} \\begin{equation}\\label{QR_leastsquares} \\hat{\\mR}\\vx = \\hat{\\mQ}\\trans\\vb, \\end{equation} will be unique. In the case when \\mA \\mA is not full rank, there will be a infinitely many solutions to the least squares problem. For both cases, we can find a least squares solution by solving \\eqref{QR_leastsquares} via back substitution. The figure below shows the geometric prespective of using a QR factorization to solve the least squares problem. For every \\vb \\in \\R^m \\vb \\in \\R^m , \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb is the orthogonal projection of \\vb \\vb onto the \\range(\\hat{\\mQ}) = \\range(\\mA) \\range(\\hat{\\mQ}) = \\range(\\mA) . The least squares solution finds a point \\vx \\vx such that \\mA\\vx \\mA\\vx is equal tothe orthogonal projection of \\vb \\vb onto the \\range(\\mA) \\range(\\mA) . So, we get \\mA\\vx = \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb \\mA\\vx = \\hat{\\mQ}\\hat{\\mQ}\\trans\\vb , which simplifies to \\eqref{QR_leastsquares}.","title":"Solving least squares via QR"},{"location":"notes/Quasi_newton/","text":"Quasi-Newton methods \u00b6 In this lecture we will study quasi-Newton methods, where we use an approximate Hessian to get the descent direction. Recall that \\vd \\vd is a descent direction at \\vx \\vx if \\vd\\trans \\nabla f(\\vx) < 0 \\vd\\trans \\nabla f(\\vx) < 0 . Some examples of descent directions that we have looked at are: Descent method \\vd \\vd \\vd\\trans \\nabla f(\\vx) \\vd\\trans \\nabla f(\\vx) Gradient descent -\\nabla f(\\vx) -\\nabla f(\\vx) -\\|\\nabla f(\\vx)\\|_2^2 -\\|\\nabla f(\\vx)\\|_2^2 Newton -\\nabla^2 f(\\vx)^{-1}\\nabla f(\\vx) -\\nabla^2 f(\\vx)^{-1}\\nabla f(\\vx) -\\nabla f(\\vx) \\nabla^2 f(\\vx)^{-1} \\nabla f(\\vx) -\\nabla f(\\vx) \\nabla^2 f(\\vx)^{-1} \\nabla f(\\vx) Diagonal approx. - \\mD_{\\vx}^{-1} \\nabla f(\\vx) - \\mD_{\\vx}^{-1} \\nabla f(\\vx) -\\nabla f(\\vx) \\mD_(\\vx)^{-1} \\nabla f(\\vx) -\\nabla f(\\vx) \\mD_(\\vx)^{-1} \\nabla f(\\vx) In the above table, the diagonal matrix \\mD \\mD is an estimate of the Hessian and is given by (\\mD_{\\vx})_{ij} = \\left\\{ \\begin{array}{ll} (\\nabla^2 f(\\vx))_{ij} & \\text{if } i = j\\\\ 0 & \\text{if } i\\neq j \\end{array} \\right. \\quad \\text{ for } i,j \\in \\{1,2, \\dots, n\\}. (\\mD_{\\vx})_{ij} = \\left\\{ \\begin{array}{ll} (\\nabla^2 f(\\vx))_{ij} & \\text{if } i = j\\\\ 0 & \\text{if } i\\neq j \\end{array} \\right. \\quad \\text{ for } i,j \\in \\{1,2, \\dots, n\\}. For each iteration of the descent scheme, quasi-Newton methods use approximation of Hessian, like the diagonal approximation, to compute the descent direction. Quasi-Newton methods require an initial estimate of the Hessian and procedure to obtain the subsequent estimates of the Hessians at the corresponding iterate. Some well known methods are: Symmetric rank-1 update Bryoden-Fletcher-Goldfarb-Shanno (BFGS) update Symmetric rank-1 update \u00b6 Given an estimate of the Hessian at \\vx_{k-1} \\vx_{k-1} , \\tilde{\\mH_{k-1}} \\in \\R^{n\\times n} \\tilde{\\mH_{k-1}} \\in \\R^{n\\times n} , the central idea of symmetric rank-1 update is to find \\tilde{\\mH_{k}} = \\tilde{\\mH_{k-1}} + \\vv\\vv\\trans \\tilde{\\mH_{k}} = \\tilde{\\mH_{k-1}} + \\vv\\vv\\trans , where \\vv \\vv is a vector in \\R^n \\R^n . Let \\vs_K = \\vx_k - \\vx_{k+1} \\vs_K = \\vx_k - \\vx_{k+1} and \\vy = \\nabla f(\\vx_k) - \\nabla f(\\vx_{k-1}) \\vy = \\nabla f(\\vx_k) - \\nabla f(\\vx_{k-1}) . A general rank-1 update satisfies \\underbrace{\\mH_{k} = \\mH_{k-1} + \\vu\\vv\\trans}_{\\text{rank-1 update}} \\quad\\text{ and } \\underbrace{\\tilde{\\mH}_k \\vs_k = \\vy_k}_{\\text{Secant condition}}. \\underbrace{\\mH_{k} = \\mH_{k-1} + \\vu\\vv\\trans}_{\\text{rank-1 update}} \\quad\\text{ and } \\underbrace{\\tilde{\\mH}_k \\vs_k = \\vy_k}_{\\text{Secant condition}}. Note that if \\vx_k \\vx_k is c;ose to \\vx_{k-1} \\vx_{k-1} and f f is smooth then \\mH_k \\mH_k is a good approximation of the Hessian of f f at \\vx_k \\vx_k in the direction of \\vs_k \\vs_k . Plugging the rank-1 update in the secant condition, we get \\begin{align*} &(\\tilde{\\mH}_{k-1} + \\vu\\vv\\trans) \\vs_k = \\vy_k\\\\ \\Rightarrow & \\vu = \\frac{\\vy_k - \\tilde{\\mH}_{k-1}\\vs_k}{\\vv\\trans\\vs_k}. \\end{align*} \\begin{align*} &(\\tilde{\\mH}_{k-1} + \\vu\\vv\\trans) \\vs_k = \\vy_k\\\\ \\Rightarrow & \\vu = \\frac{\\vy_k - \\tilde{\\mH}_{k-1}\\vs_k}{\\vv\\trans\\vs_k}. \\end{align*} Here \\vv_k \\vv_k is arbitrary and to satisfy the symmetric update condition, we can set \\vv = \\alpha \\vu \\vv = \\alpha \\vu for any \\alpha \\in \\R \\alpha \\in \\R . A symmetric rank-1 update to the estimate of the Hessian is \\vv = \\vy_k - \\tilde{\\mH}_{k-1}\\vs_k \\quad \\text{ and } \\tilde{\\mH_k} = \\tilde{\\mH}_{k-1} + \\frac{1}{\\vv\\trans\\vs_k} \\vv\\vv\\trans. \\vv = \\vy_k - \\tilde{\\mH}_{k-1}\\vs_k \\quad \\text{ and } \\tilde{\\mH_k} = \\tilde{\\mH}_{k-1} + \\frac{1}{\\vv\\trans\\vs_k} \\vv\\vv\\trans. However, note that the Newton update requires the inverse of the Hessian to compute the descent direction. Thus, we need estimates of \\mB_k = \\tilde{H_k}^{-1} \\mB_k = \\tilde{H_k}^{-1} for the quasi-Newton method. These are obtained using the same symmetric rank-1 update routine. That is, \\mB_k \\mB_k needs to satisfy \\mB_k \\vy_k = \\vs_k \\mB_k \\vy_k = \\vs_k and \\mB_k = \\mB_{k-1} + \\vu\\vv\\trans \\mB_k = \\mB_{k-1} + \\vu\\vv\\trans , for some \\vu, \\vv \\in \\R^n \\vu, \\vv \\in \\R^n . A symmetric rank-1 update to the estimate of the inverse of the Hessian is \\vv = \\vs_k - {\\mB_{k-1}}\\vy_k \\quad \\text{ and } {\\mB_k} = {\\mB_{k-1}} + \\frac{1}{\\vv\\trans\\vy_k} \\vv\\vv\\trans. \\vv = \\vs_k - {\\mB_{k-1}}\\vy_k \\quad \\text{ and } {\\mB_k} = {\\mB_{k-1}} + \\frac{1}{\\vv\\trans\\vy_k} \\vv\\vv\\trans. BFGS and L-BFGS \u00b6 BFGS is a rank-2 update scheme where the updates to the estimate of the Hessian \\tilde{\\mH}_{k-1} \\tilde{\\mH}_{k-1} take the form \\tilde{\\mH}_{k} = \\tilde{\\mH}_{k-1} +\\vu\\vv\\trans \\tilde{\\mH}_{k} = \\tilde{\\mH}_{k-1} +\\vu\\vv\\trans , where \\vu, \\vv \\in \\R^{n\\times 2} \\vu, \\vv \\in \\R^{n\\times 2} . The updated estimate of the Hessian \\tilde{\\mH}_k \\tilde{\\mH}_k satisfy \\tilde{\\mH}_k \\vs_k = \\vy_k,\\quad \\|\\tilde{\\mH}_{k} - \\tilde{\\mH}_{k-1}\\| \\text{ is small, and } \\tilde{\\mH}_{k} \u227b 0. \\tilde{\\mH}_k \\vs_k = \\vy_k,\\quad \\|\\tilde{\\mH}_{k} - \\tilde{\\mH}_{k-1}\\| \\text{ is small, and } \\tilde{\\mH}_{k} \u227b 0. The BFGS update to \\tilde{\\mH}_{k-1} \\tilde{\\mH}_{k-1} is \\tilde{\\mH}_{k} = \\tilde{\\mH}_{k-1} + \\frac{1}{\\vy_k\\trans\\vs_k}\\vy_k\\vy_k\\trans - \\frac{1}{\\vs_k\\trans\\tilde{\\mH}_{k-1}\\vs_k}\\tilde{\\mH}_{k-1}\\vs_k\\vs_k\\trans \\tilde{\\mH}_{k-1}, \\tilde{\\mH}_{k} = \\tilde{\\mH}_{k-1} + \\frac{1}{\\vy_k\\trans\\vs_k}\\vy_k\\vy_k\\trans - \\frac{1}{\\vs_k\\trans\\tilde{\\mH}_{k-1}\\vs_k}\\tilde{\\mH}_{k-1}\\vs_k\\vs_k\\trans \\tilde{\\mH}_{k-1}, and the inverse update to \\mB_{k-1} = (\\tilde{\\mH}_{k-1})^{-1} \\mB_{k-1} = (\\tilde{\\mH}_{k-1})^{-1} is \\mB_k = \\left(\\mI - \\frac{1}{\\vy_k\\trans\\vs_k}\\vs_k\\vy_k\\trans\\right)\\mB_{k-1}\\left(\\mI - \\frac{1}{\\vy_k\\trans\\vs_k}\\vy_k\\vs_k\\trans\\right) + \\frac{1}{\\vy_k\\trans\\vs_k} \\vs_k\\vs_k\\trans. \\mB_k = \\left(\\mI - \\frac{1}{\\vy_k\\trans\\vs_k}\\vs_k\\vy_k\\trans\\right)\\mB_{k-1}\\left(\\mI - \\frac{1}{\\vy_k\\trans\\vs_k}\\vy_k\\vs_k\\trans\\right) + \\frac{1}{\\vy_k\\trans\\vs_k} \\vs_k\\vs_k\\trans. Note that the k k th estimate of the Hessian (or inverse Hessian) in BFGS can be computed using \\{\\vs_i\\}_{i = 1}^k \\{\\vs_i\\}_{i = 1}^k and \\{\\vy_i\\}_{i=1}^k \\{\\vy_i\\}_{i=1}^k and the initial estimate of the Hessian (or inverse Hessian). When implemented correctly, we do not need to form any matrices and is very fast. The Limited memory BFGS (L-BFGS) improves the storage requirement by only using the last L L number of iterates for \\vs_i \\vs_i and \\vy_i \\vy_i to compute the estimates. This reduces the storage cost to O(nL) O(nL) .","title":"Quasi-Newton methods"},{"location":"notes/Quasi_newton/#quasi-newton-methods","text":"In this lecture we will study quasi-Newton methods, where we use an approximate Hessian to get the descent direction. Recall that \\vd \\vd is a descent direction at \\vx \\vx if \\vd\\trans \\nabla f(\\vx) < 0 \\vd\\trans \\nabla f(\\vx) < 0 . Some examples of descent directions that we have looked at are: Descent method \\vd \\vd \\vd\\trans \\nabla f(\\vx) \\vd\\trans \\nabla f(\\vx) Gradient descent -\\nabla f(\\vx) -\\nabla f(\\vx) -\\|\\nabla f(\\vx)\\|_2^2 -\\|\\nabla f(\\vx)\\|_2^2 Newton -\\nabla^2 f(\\vx)^{-1}\\nabla f(\\vx) -\\nabla^2 f(\\vx)^{-1}\\nabla f(\\vx) -\\nabla f(\\vx) \\nabla^2 f(\\vx)^{-1} \\nabla f(\\vx) -\\nabla f(\\vx) \\nabla^2 f(\\vx)^{-1} \\nabla f(\\vx) Diagonal approx. - \\mD_{\\vx}^{-1} \\nabla f(\\vx) - \\mD_{\\vx}^{-1} \\nabla f(\\vx) -\\nabla f(\\vx) \\mD_(\\vx)^{-1} \\nabla f(\\vx) -\\nabla f(\\vx) \\mD_(\\vx)^{-1} \\nabla f(\\vx) In the above table, the diagonal matrix \\mD \\mD is an estimate of the Hessian and is given by (\\mD_{\\vx})_{ij} = \\left\\{ \\begin{array}{ll} (\\nabla^2 f(\\vx))_{ij} & \\text{if } i = j\\\\ 0 & \\text{if } i\\neq j \\end{array} \\right. \\quad \\text{ for } i,j \\in \\{1,2, \\dots, n\\}. (\\mD_{\\vx})_{ij} = \\left\\{ \\begin{array}{ll} (\\nabla^2 f(\\vx))_{ij} & \\text{if } i = j\\\\ 0 & \\text{if } i\\neq j \\end{array} \\right. \\quad \\text{ for } i,j \\in \\{1,2, \\dots, n\\}. For each iteration of the descent scheme, quasi-Newton methods use approximation of Hessian, like the diagonal approximation, to compute the descent direction. Quasi-Newton methods require an initial estimate of the Hessian and procedure to obtain the subsequent estimates of the Hessians at the corresponding iterate. Some well known methods are: Symmetric rank-1 update Bryoden-Fletcher-Goldfarb-Shanno (BFGS) update","title":"Quasi-Newton methods"},{"location":"notes/Quasi_newton/#symmetric-rank-1-update","text":"Given an estimate of the Hessian at \\vx_{k-1} \\vx_{k-1} , \\tilde{\\mH_{k-1}} \\in \\R^{n\\times n} \\tilde{\\mH_{k-1}} \\in \\R^{n\\times n} , the central idea of symmetric rank-1 update is to find \\tilde{\\mH_{k}} = \\tilde{\\mH_{k-1}} + \\vv\\vv\\trans \\tilde{\\mH_{k}} = \\tilde{\\mH_{k-1}} + \\vv\\vv\\trans , where \\vv \\vv is a vector in \\R^n \\R^n . Let \\vs_K = \\vx_k - \\vx_{k+1} \\vs_K = \\vx_k - \\vx_{k+1} and \\vy = \\nabla f(\\vx_k) - \\nabla f(\\vx_{k-1}) \\vy = \\nabla f(\\vx_k) - \\nabla f(\\vx_{k-1}) . A general rank-1 update satisfies \\underbrace{\\mH_{k} = \\mH_{k-1} + \\vu\\vv\\trans}_{\\text{rank-1 update}} \\quad\\text{ and } \\underbrace{\\tilde{\\mH}_k \\vs_k = \\vy_k}_{\\text{Secant condition}}. \\underbrace{\\mH_{k} = \\mH_{k-1} + \\vu\\vv\\trans}_{\\text{rank-1 update}} \\quad\\text{ and } \\underbrace{\\tilde{\\mH}_k \\vs_k = \\vy_k}_{\\text{Secant condition}}. Note that if \\vx_k \\vx_k is c;ose to \\vx_{k-1} \\vx_{k-1} and f f is smooth then \\mH_k \\mH_k is a good approximation of the Hessian of f f at \\vx_k \\vx_k in the direction of \\vs_k \\vs_k . Plugging the rank-1 update in the secant condition, we get \\begin{align*} &(\\tilde{\\mH}_{k-1} + \\vu\\vv\\trans) \\vs_k = \\vy_k\\\\ \\Rightarrow & \\vu = \\frac{\\vy_k - \\tilde{\\mH}_{k-1}\\vs_k}{\\vv\\trans\\vs_k}. \\end{align*} \\begin{align*} &(\\tilde{\\mH}_{k-1} + \\vu\\vv\\trans) \\vs_k = \\vy_k\\\\ \\Rightarrow & \\vu = \\frac{\\vy_k - \\tilde{\\mH}_{k-1}\\vs_k}{\\vv\\trans\\vs_k}. \\end{align*} Here \\vv_k \\vv_k is arbitrary and to satisfy the symmetric update condition, we can set \\vv = \\alpha \\vu \\vv = \\alpha \\vu for any \\alpha \\in \\R \\alpha \\in \\R . A symmetric rank-1 update to the estimate of the Hessian is \\vv = \\vy_k - \\tilde{\\mH}_{k-1}\\vs_k \\quad \\text{ and } \\tilde{\\mH_k} = \\tilde{\\mH}_{k-1} + \\frac{1}{\\vv\\trans\\vs_k} \\vv\\vv\\trans. \\vv = \\vy_k - \\tilde{\\mH}_{k-1}\\vs_k \\quad \\text{ and } \\tilde{\\mH_k} = \\tilde{\\mH}_{k-1} + \\frac{1}{\\vv\\trans\\vs_k} \\vv\\vv\\trans. However, note that the Newton update requires the inverse of the Hessian to compute the descent direction. Thus, we need estimates of \\mB_k = \\tilde{H_k}^{-1} \\mB_k = \\tilde{H_k}^{-1} for the quasi-Newton method. These are obtained using the same symmetric rank-1 update routine. That is, \\mB_k \\mB_k needs to satisfy \\mB_k \\vy_k = \\vs_k \\mB_k \\vy_k = \\vs_k and \\mB_k = \\mB_{k-1} + \\vu\\vv\\trans \\mB_k = \\mB_{k-1} + \\vu\\vv\\trans , for some \\vu, \\vv \\in \\R^n \\vu, \\vv \\in \\R^n . A symmetric rank-1 update to the estimate of the inverse of the Hessian is \\vv = \\vs_k - {\\mB_{k-1}}\\vy_k \\quad \\text{ and } {\\mB_k} = {\\mB_{k-1}} + \\frac{1}{\\vv\\trans\\vy_k} \\vv\\vv\\trans. \\vv = \\vs_k - {\\mB_{k-1}}\\vy_k \\quad \\text{ and } {\\mB_k} = {\\mB_{k-1}} + \\frac{1}{\\vv\\trans\\vy_k} \\vv\\vv\\trans.","title":"Symmetric rank-1 update"},{"location":"notes/Quasi_newton/#bfgs-and-l-bfgs","text":"BFGS is a rank-2 update scheme where the updates to the estimate of the Hessian \\tilde{\\mH}_{k-1} \\tilde{\\mH}_{k-1} take the form \\tilde{\\mH}_{k} = \\tilde{\\mH}_{k-1} +\\vu\\vv\\trans \\tilde{\\mH}_{k} = \\tilde{\\mH}_{k-1} +\\vu\\vv\\trans , where \\vu, \\vv \\in \\R^{n\\times 2} \\vu, \\vv \\in \\R^{n\\times 2} . The updated estimate of the Hessian \\tilde{\\mH}_k \\tilde{\\mH}_k satisfy \\tilde{\\mH}_k \\vs_k = \\vy_k,\\quad \\|\\tilde{\\mH}_{k} - \\tilde{\\mH}_{k-1}\\| \\text{ is small, and } \\tilde{\\mH}_{k} \u227b 0. \\tilde{\\mH}_k \\vs_k = \\vy_k,\\quad \\|\\tilde{\\mH}_{k} - \\tilde{\\mH}_{k-1}\\| \\text{ is small, and } \\tilde{\\mH}_{k} \u227b 0. The BFGS update to \\tilde{\\mH}_{k-1} \\tilde{\\mH}_{k-1} is \\tilde{\\mH}_{k} = \\tilde{\\mH}_{k-1} + \\frac{1}{\\vy_k\\trans\\vs_k}\\vy_k\\vy_k\\trans - \\frac{1}{\\vs_k\\trans\\tilde{\\mH}_{k-1}\\vs_k}\\tilde{\\mH}_{k-1}\\vs_k\\vs_k\\trans \\tilde{\\mH}_{k-1}, \\tilde{\\mH}_{k} = \\tilde{\\mH}_{k-1} + \\frac{1}{\\vy_k\\trans\\vs_k}\\vy_k\\vy_k\\trans - \\frac{1}{\\vs_k\\trans\\tilde{\\mH}_{k-1}\\vs_k}\\tilde{\\mH}_{k-1}\\vs_k\\vs_k\\trans \\tilde{\\mH}_{k-1}, and the inverse update to \\mB_{k-1} = (\\tilde{\\mH}_{k-1})^{-1} \\mB_{k-1} = (\\tilde{\\mH}_{k-1})^{-1} is \\mB_k = \\left(\\mI - \\frac{1}{\\vy_k\\trans\\vs_k}\\vs_k\\vy_k\\trans\\right)\\mB_{k-1}\\left(\\mI - \\frac{1}{\\vy_k\\trans\\vs_k}\\vy_k\\vs_k\\trans\\right) + \\frac{1}{\\vy_k\\trans\\vs_k} \\vs_k\\vs_k\\trans. \\mB_k = \\left(\\mI - \\frac{1}{\\vy_k\\trans\\vs_k}\\vs_k\\vy_k\\trans\\right)\\mB_{k-1}\\left(\\mI - \\frac{1}{\\vy_k\\trans\\vs_k}\\vy_k\\vs_k\\trans\\right) + \\frac{1}{\\vy_k\\trans\\vs_k} \\vs_k\\vs_k\\trans. Note that the k k th estimate of the Hessian (or inverse Hessian) in BFGS can be computed using \\{\\vs_i\\}_{i = 1}^k \\{\\vs_i\\}_{i = 1}^k and \\{\\vy_i\\}_{i=1}^k \\{\\vy_i\\}_{i=1}^k and the initial estimate of the Hessian (or inverse Hessian). When implemented correctly, we do not need to form any matrices and is very fast. The Limited memory BFGS (L-BFGS) improves the storage requirement by only using the last L L number of iterates for \\vs_i \\vs_i and \\vy_i \\vy_i to compute the estimates. This reduces the storage cost to O(nL) O(nL) .","title":"BFGS and L-BFGS"},{"location":"notes/Regularized_LS/","text":"Regularized least squares \u00b6 In the least squares problem, we minimized 2-norm squared of the data misfit relative to a linear model. In contrast to the least squares formulation, many problem need to balance competing objectives. For example, consider the problem of finding \\vx_0 \\in \\R^n \\vx_0 \\in \\R^n from noisy linear measurements \\vb = \\mA\\vx_0 + \\vw_\\vb \\vb = \\mA\\vx_0 + \\vw_\\vb and \\vg = \\mF\\vx_0-\\vw_\\vg \\vg = \\mF\\vx_0-\\vw_\\vg . Here \\vw_\\vb \\vw_\\vb and \\vw_\\vg \\vw_\\vg are noise vectors. In order to solve this problem, we need to find a \\vx \\vx that makes \\func{f}_1(\\vx) = \\|\\mA\\vx - \\vb\\|_2^2 \\func{f}_1(\\vx) = \\|\\mA\\vx - \\vb\\|_2^2 small, and \\func{f}_2(\\vx) = \\|\\mB\\vx - \\vg\\|_2^2 \\func{f}_2(\\vx) = \\|\\mB\\vx - \\vg\\|_2^2 small. Generally, we can make \\func{f}_1(\\vx) \\func{f}_1(\\vx) or \\func{f}_2(\\vx) \\func{f}_2(\\vx) small, but not both. The figure below shows this relationship between \\func{f}_1(\\vx) \\func{f}_1(\\vx) and \\func{f}_2(\\vx) \\func{f}_2(\\vx) . In the figure, the points in the boundary of two regions are called the Pareto optimal solutions. In order to find these optimal solutions, we minimize the following weighted sum objective: \\begin{equation}\\label{Regularized_LS_weight} f_1(\\vx)+\\gamma f_2(\\vx) = \\|\\mA\\vx-\\vg\\|_2^2+\u03b3\\|\\mF\\vx-\\vg\\|_2^2. \\end{equation} \\begin{equation}\\label{Regularized_LS_weight} f_1(\\vx)+\\gamma f_2(\\vx) = \\|\\mA\\vx-\\vg\\|_2^2+\u03b3\\|\\mF\\vx-\\vg\\|_2^2. \\end{equation} The parameter \\gamma \\gamma is non-negative and defines relative weight between the objectives. For example, in the case of \\gamma =1 \\gamma =1 , the optimal point that minimizes \\eqref{Regularized_LS_weight} is the point \\vx \\vx on the optimal trade-off curve with f_1(\\vx) = f_2(\\vx) f_1(\\vx) = f_2(\\vx) . Note that for a fixed \\gamma \\gamma and \\alpha \\in \\R \\alpha \\in \\R , the set \\ell(\\gamma,\\alpha) = \\{(f_1(\\vx),f_2(\\vx)):f_1(\\vx) +\\gamma f_2(\\vx) = \\alpha, \\vx \\in \\R^n\\} \\ell(\\gamma,\\alpha) = \\{(f_1(\\vx),f_2(\\vx)):f_1(\\vx) +\\gamma f_2(\\vx) = \\alpha, \\vx \\in \\R^n\\} correspond to a line with slope of -\\gamma -\\gamma . Another way to visualize the optimal solution is to find the line that is tangent to the optimal trade-off cuve, see figure below. Example: Signal denoising \u00b6 Suppose we observe noisy measurements of a signal: \\vb = \\hat{\\vx} + \\vw, \\vb = \\hat{\\vx} + \\vw, where \\hat{\\vx}\\in\\R^n \\hat{\\vx}\\in\\R^n is the signal and \\vw \\in \\R^n \\vw \\in \\R^n is noise. A simple apporach to find \\hat{\\vx} \\hat{\\vx} is to solve: \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\vx-\\vb\\|_2^2. \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\vx-\\vb\\|_2^2. This minimization program doesnot enforce any structure on \\vx \\vx . However, if we have prior information that the signal is \"smooth\", then we might balance the least squares fit against the smoothness of the solution in the following way: \\begin{equation}\\label{Regularized_LS_identity} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\vx-\\vb\\|_2^2 +\\frac{\\gamma}{2} \\underbrace{\\sum_{i=1}^{n-1}(x_i - x_{i+})^2}_{f_2(\\vx)}. \\end{equation} \\begin{equation}\\label{Regularized_LS_identity} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\vx-\\vb\\|_2^2 +\\frac{\\gamma}{2} \\underbrace{\\sum_{i=1}^{n-1}(x_i - x_{i+})^2}_{f_2(\\vx)}. \\end{equation} Here, f_2(x) f_2(x) promotes smoothness. We can alternatively write the above minimization program in matrix notation. Define the finite differencem matrix \\mD = \\bmat 1 & -1 & 0 & \\cdots & 0 & 0\\\\ 0 & 1 & -1 & 0 & \\cdots & 0\\\\ & & \\ddots & \\ddots & & \\\\ 0 & \\cdots & 0 & 1 & -1 & 0\\\\ 0 & 0 & \\cdots & 0 & 1 & -1\\emat \\in \\R^{{n-1}\\times n} \\mD = \\bmat 1 & -1 & 0 & \\cdots & 0 & 0\\\\ 0 & 1 & -1 & 0 & \\cdots & 0\\\\ & & \\ddots & \\ddots & & \\\\ 0 & \\cdots & 0 & 1 & -1 & 0\\\\ 0 & 0 & \\cdots & 0 & 1 & -1\\emat \\in \\R^{{n-1}\\times n} So, we can rewrite f_2(\\vx) = \\sum_{i=1}^{n-1}(x_i - x_{i+})^2 = \\|\\mD\\vx\\|_2^2 f_2(\\vx) = \\sum_{i=1}^{n-1}(x_i - x_{i+})^2 = \\|\\mD\\vx\\|_2^2 . This allows for a reformulation of the weighted leas squares objective into a familiar least squares objective: \\|\\vx-\\vb\\|_2^2+\\gamma\\|\\mD\\vx\\|_2^2 = \\bigg\\|\\underbrace{\\bmat\\mI\\\\\\sqrt{\\gamma}\\mD\\emat}_{\\hat{\\mA}}\\vx - \\underbrace{\\bmat\\vb\\\\ \\vzero\\emat}_{\\hat{\\vb}}\\bigg\\|_2^2. \\|\\vx-\\vb\\|_2^2+\\gamma\\|\\mD\\vx\\|_2^2 = \\bigg\\|\\underbrace{\\bmat\\mI\\\\\\sqrt{\\gamma}\\mD\\emat}_{\\hat{\\mA}}\\vx - \\underbrace{\\bmat\\vb\\\\ \\vzero\\emat}_{\\hat{\\vb}}\\bigg\\|_2^2. So the solution to the weighted least squares minimization program \\eqref{Regularized_LS_identity} satisfies the normal equation \\hat{\\mA}\\trans\\hat{\\mA}\\vx = \\hat{\\mA}\\trans\\hat{\\vb} \\hat{\\mA}\\trans\\hat{\\mA}\\vx = \\hat{\\mA}\\trans\\hat{\\vb} , which simplifies to (\\mI + \\gamma\\mD\\trans\\mD)\\vx = \\vb. (\\mI + \\gamma\\mD\\trans\\mD)\\vx = \\vb. Regularized least squares (aka Tikhonov) \u00b6 We now generalize the result to noisy linear observations of a signal. In this case, the model is \\vb = \\mA\\vx + \\vw, \\vb = \\mA\\vx + \\vw, where we added the measurement matrix \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} . The corresponding wighted-sum least squares program is \\begin{equation} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 + \\frac{\\gamma}{2}\\|\\mD\\vx\\|_2^2, \\end{equation} \\begin{equation} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 + \\frac{\\gamma}{2}\\|\\mD\\vx\\|_2^2, \\end{equation} where \\|\\mD\\vx\\|_2^2 \\|\\mD\\vx\\|_2^2 is called the regularization penalty and \\gamma \\gamma is called the regularization parameter. The objective function can be reformulated as an least squares objective \\|\\mA\\vx-\\vb\\|_2^2+\\gamma\\|\\mD\\vx\\|_2^2 = \\bigg\\|\\underbrace{\\bmat\\mA\\\\\\sqrt{\\gamma}\\mD\\emat}_{\\hat{\\mA}}\\vx - \\underbrace{\\bmat\\vb\\\\ \\vzero\\emat}_{\\hat{\\vb}}\\bigg\\|_2^2. \\|\\mA\\vx-\\vb\\|_2^2+\\gamma\\|\\mD\\vx\\|_2^2 = \\bigg\\|\\underbrace{\\bmat\\mA\\\\\\sqrt{\\gamma}\\mD\\emat}_{\\hat{\\mA}}\\vx - \\underbrace{\\bmat\\vb\\\\ \\vzero\\emat}_{\\hat{\\vb}}\\bigg\\|_2^2. and the corresponding normal equations is (\\mA\\trans\\mA + \\gamma\\mD\\trans\\mD)\\vx = \\mA\\trans\\vb. (\\mA\\trans\\mA + \\gamma\\mD\\trans\\mD)\\vx = \\mA\\trans\\vb.","title":"Regularized least squares"},{"location":"notes/Regularized_LS/#regularized-least-squares","text":"In the least squares problem, we minimized 2-norm squared of the data misfit relative to a linear model. In contrast to the least squares formulation, many problem need to balance competing objectives. For example, consider the problem of finding \\vx_0 \\in \\R^n \\vx_0 \\in \\R^n from noisy linear measurements \\vb = \\mA\\vx_0 + \\vw_\\vb \\vb = \\mA\\vx_0 + \\vw_\\vb and \\vg = \\mF\\vx_0-\\vw_\\vg \\vg = \\mF\\vx_0-\\vw_\\vg . Here \\vw_\\vb \\vw_\\vb and \\vw_\\vg \\vw_\\vg are noise vectors. In order to solve this problem, we need to find a \\vx \\vx that makes \\func{f}_1(\\vx) = \\|\\mA\\vx - \\vb\\|_2^2 \\func{f}_1(\\vx) = \\|\\mA\\vx - \\vb\\|_2^2 small, and \\func{f}_2(\\vx) = \\|\\mB\\vx - \\vg\\|_2^2 \\func{f}_2(\\vx) = \\|\\mB\\vx - \\vg\\|_2^2 small. Generally, we can make \\func{f}_1(\\vx) \\func{f}_1(\\vx) or \\func{f}_2(\\vx) \\func{f}_2(\\vx) small, but not both. The figure below shows this relationship between \\func{f}_1(\\vx) \\func{f}_1(\\vx) and \\func{f}_2(\\vx) \\func{f}_2(\\vx) . In the figure, the points in the boundary of two regions are called the Pareto optimal solutions. In order to find these optimal solutions, we minimize the following weighted sum objective: \\begin{equation}\\label{Regularized_LS_weight} f_1(\\vx)+\\gamma f_2(\\vx) = \\|\\mA\\vx-\\vg\\|_2^2+\u03b3\\|\\mF\\vx-\\vg\\|_2^2. \\end{equation} \\begin{equation}\\label{Regularized_LS_weight} f_1(\\vx)+\\gamma f_2(\\vx) = \\|\\mA\\vx-\\vg\\|_2^2+\u03b3\\|\\mF\\vx-\\vg\\|_2^2. \\end{equation} The parameter \\gamma \\gamma is non-negative and defines relative weight between the objectives. For example, in the case of \\gamma =1 \\gamma =1 , the optimal point that minimizes \\eqref{Regularized_LS_weight} is the point \\vx \\vx on the optimal trade-off curve with f_1(\\vx) = f_2(\\vx) f_1(\\vx) = f_2(\\vx) . Note that for a fixed \\gamma \\gamma and \\alpha \\in \\R \\alpha \\in \\R , the set \\ell(\\gamma,\\alpha) = \\{(f_1(\\vx),f_2(\\vx)):f_1(\\vx) +\\gamma f_2(\\vx) = \\alpha, \\vx \\in \\R^n\\} \\ell(\\gamma,\\alpha) = \\{(f_1(\\vx),f_2(\\vx)):f_1(\\vx) +\\gamma f_2(\\vx) = \\alpha, \\vx \\in \\R^n\\} correspond to a line with slope of -\\gamma -\\gamma . Another way to visualize the optimal solution is to find the line that is tangent to the optimal trade-off cuve, see figure below.","title":"Regularized least squares"},{"location":"notes/Regularized_LS/#example-signal-denoising","text":"Suppose we observe noisy measurements of a signal: \\vb = \\hat{\\vx} + \\vw, \\vb = \\hat{\\vx} + \\vw, where \\hat{\\vx}\\in\\R^n \\hat{\\vx}\\in\\R^n is the signal and \\vw \\in \\R^n \\vw \\in \\R^n is noise. A simple apporach to find \\hat{\\vx} \\hat{\\vx} is to solve: \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\vx-\\vb\\|_2^2. \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\vx-\\vb\\|_2^2. This minimization program doesnot enforce any structure on \\vx \\vx . However, if we have prior information that the signal is \"smooth\", then we might balance the least squares fit against the smoothness of the solution in the following way: \\begin{equation}\\label{Regularized_LS_identity} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\vx-\\vb\\|_2^2 +\\frac{\\gamma}{2} \\underbrace{\\sum_{i=1}^{n-1}(x_i - x_{i+})^2}_{f_2(\\vx)}. \\end{equation} \\begin{equation}\\label{Regularized_LS_identity} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\vx-\\vb\\|_2^2 +\\frac{\\gamma}{2} \\underbrace{\\sum_{i=1}^{n-1}(x_i - x_{i+})^2}_{f_2(\\vx)}. \\end{equation} Here, f_2(x) f_2(x) promotes smoothness. We can alternatively write the above minimization program in matrix notation. Define the finite differencem matrix \\mD = \\bmat 1 & -1 & 0 & \\cdots & 0 & 0\\\\ 0 & 1 & -1 & 0 & \\cdots & 0\\\\ & & \\ddots & \\ddots & & \\\\ 0 & \\cdots & 0 & 1 & -1 & 0\\\\ 0 & 0 & \\cdots & 0 & 1 & -1\\emat \\in \\R^{{n-1}\\times n} \\mD = \\bmat 1 & -1 & 0 & \\cdots & 0 & 0\\\\ 0 & 1 & -1 & 0 & \\cdots & 0\\\\ & & \\ddots & \\ddots & & \\\\ 0 & \\cdots & 0 & 1 & -1 & 0\\\\ 0 & 0 & \\cdots & 0 & 1 & -1\\emat \\in \\R^{{n-1}\\times n} So, we can rewrite f_2(\\vx) = \\sum_{i=1}^{n-1}(x_i - x_{i+})^2 = \\|\\mD\\vx\\|_2^2 f_2(\\vx) = \\sum_{i=1}^{n-1}(x_i - x_{i+})^2 = \\|\\mD\\vx\\|_2^2 . This allows for a reformulation of the weighted leas squares objective into a familiar least squares objective: \\|\\vx-\\vb\\|_2^2+\\gamma\\|\\mD\\vx\\|_2^2 = \\bigg\\|\\underbrace{\\bmat\\mI\\\\\\sqrt{\\gamma}\\mD\\emat}_{\\hat{\\mA}}\\vx - \\underbrace{\\bmat\\vb\\\\ \\vzero\\emat}_{\\hat{\\vb}}\\bigg\\|_2^2. \\|\\vx-\\vb\\|_2^2+\\gamma\\|\\mD\\vx\\|_2^2 = \\bigg\\|\\underbrace{\\bmat\\mI\\\\\\sqrt{\\gamma}\\mD\\emat}_{\\hat{\\mA}}\\vx - \\underbrace{\\bmat\\vb\\\\ \\vzero\\emat}_{\\hat{\\vb}}\\bigg\\|_2^2. So the solution to the weighted least squares minimization program \\eqref{Regularized_LS_identity} satisfies the normal equation \\hat{\\mA}\\trans\\hat{\\mA}\\vx = \\hat{\\mA}\\trans\\hat{\\vb} \\hat{\\mA}\\trans\\hat{\\mA}\\vx = \\hat{\\mA}\\trans\\hat{\\vb} , which simplifies to (\\mI + \\gamma\\mD\\trans\\mD)\\vx = \\vb. (\\mI + \\gamma\\mD\\trans\\mD)\\vx = \\vb.","title":"Example: Signal denoising"},{"location":"notes/Regularized_LS/#regularized-least-squares-aka-tikhonov","text":"We now generalize the result to noisy linear observations of a signal. In this case, the model is \\vb = \\mA\\vx + \\vw, \\vb = \\mA\\vx + \\vw, where we added the measurement matrix \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} . The corresponding wighted-sum least squares program is \\begin{equation} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 + \\frac{\\gamma}{2}\\|\\mD\\vx\\|_2^2, \\end{equation} \\begin{equation} \\min_{\\vx\\in\\R^n} \\frac{1}{2}\\|\\mA\\vx-\\vb\\|_2^2 + \\frac{\\gamma}{2}\\|\\mD\\vx\\|_2^2, \\end{equation} where \\|\\mD\\vx\\|_2^2 \\|\\mD\\vx\\|_2^2 is called the regularization penalty and \\gamma \\gamma is called the regularization parameter. The objective function can be reformulated as an least squares objective \\|\\mA\\vx-\\vb\\|_2^2+\\gamma\\|\\mD\\vx\\|_2^2 = \\bigg\\|\\underbrace{\\bmat\\mA\\\\\\sqrt{\\gamma}\\mD\\emat}_{\\hat{\\mA}}\\vx - \\underbrace{\\bmat\\vb\\\\ \\vzero\\emat}_{\\hat{\\vb}}\\bigg\\|_2^2. \\|\\mA\\vx-\\vb\\|_2^2+\\gamma\\|\\mD\\vx\\|_2^2 = \\bigg\\|\\underbrace{\\bmat\\mA\\\\\\sqrt{\\gamma}\\mD\\emat}_{\\hat{\\mA}}\\vx - \\underbrace{\\bmat\\vb\\\\ \\vzero\\emat}_{\\hat{\\vb}}\\bigg\\|_2^2. and the corresponding normal equations is (\\mA\\trans\\mA + \\gamma\\mD\\trans\\mD)\\vx = \\mA\\trans\\vb. (\\mA\\trans\\mA + \\gamma\\mD\\trans\\mD)\\vx = \\mA\\trans\\vb.","title":"Regularized least squares (aka Tikhonov)"},{"location":"notes/background/","text":"Mathematical background \u00b6 Vectors \u00b6 Vectors are arrays of numbers ordered as a column. For example, a vector \\vx \\in \\R^n \\vx \\in \\R^n contain n n entries with x_i x_i as its i i th entry: \\vx \\in \\R^n,\\quad \\vx = \\begin{bmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{bmatrix},\\quad \\vx = \\begin{pmatrix}x_1, x_2,\\dots,x_n\\end{pmatrix}. \\vx \\in \\R^n,\\quad \\vx = \\begin{bmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{bmatrix},\\quad \\vx = \\begin{pmatrix}x_1, x_2,\\dots,x_n\\end{pmatrix}. Some special vectors that are commonly used are the zero vector, ones vector, and the standard basis vectors. These are defined next: Zero vector: A vector \\vx \\vx is the zero vector if all of its entries are zero. The zero vector is denoted by \\vzero \\vzero . Ones vector: A vector \\vx \\vx is the ones vector if all of its entries are one. The ones vector is denoted by \\ve \\ve . i i th standard basis vector: A vector \\vx \\in \\R^n \\vx \\in \\R^n is the i i th standard basis vector if x_j = \\Big\\{\\begin{array}{cc}1 & i = j\\\\0 & \\text{otherwise}\\end{array} x_j = \\Big\\{\\begin{array}{cc}1 & i = j\\\\0 & \\text{otherwise}\\end{array} . The i i th standard basis vector is denoted by \\ve_i \\ve_i . For example, in \\R^3 \\R^3 , the 2 2 nd standard basis vector is \\ve_2 = \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix} \\ve_2 = \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix} . A vector \\vx \\in \\R^n \\vx \\in \\R^n is equal to another vector \\vy \\in \\R^n \\vy \\in \\R^n if x_i = y_i x_i = y_i for all i \\in \\{1,2,\\dots,n\\} i \\in \\{1,2,\\dots,n\\} . Similarly, if all of the entries of a vector \\vx \\vx is equal to a scalar c \\in \\R c \\in \\R , then we write \\vx = c \\vx = c . These notions can be extended to inequalites in the following way: A vector \\vx \\in \\R^n \\vx \\in \\R^n and a scalar c \\in \\R c \\in \\R satisfy \\vx \\geq c \\vx \\geq c if and only if x_i \\geq c x_i \\geq c for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . A vector \\vx \\in \\R^n \\vx \\in \\R^n and a scalar c \\in \\R c \\in \\R satisfy \\vx > c \\vx > c if and only if x_i > c x_i > c for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . Vectors \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n satisfy \\vx \\geq \\vy \\vx \\geq \\vy if and only if x_i \\geq y_i x_i \\geq y_i for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . Vectors \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n satisfy \\vx \\geq \\vy \\vx \\geq \\vy if and only if x_i \\geq y_i x_i \\geq y_i for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . Block vector is a concise representation of a vector obtained by stacking vectors. For example, if \\vx \\in \\R^m \\vx \\in \\R^m , \\vy \\in \\R^n \\vy \\in \\R^n , and \\vz \\in \\R^p \\vz \\in \\R^p , then \\va = \\begin{bmatrix} \\vx \\\\ \\dots \\\\ \\vy \\\\ \\dots \\\\ \\vz\\end{bmatrix}\\in \\R^{m+n+p} \\va = \\begin{bmatrix} \\vx \\\\ \\dots \\\\ \\vy \\\\ \\dots \\\\ \\vz\\end{bmatrix}\\in \\R^{m+n+p} is a block vector (also called stacked vector or conncatenated vector) obtained by stacking \\vx \\vx , \\vy \\vy , and \\vz \\vz vertically. This block vector \\va \\va can also be written as \\begin{pmatrix}\\vx,\\vy,\\vz \\end{pmatrix} \\begin{pmatrix}\\vx,\\vy,\\vz \\end{pmatrix} . If the dimension of the vectors are equal (i.e. m = n = p m = n = p ), then these vector can be stacked horizontally as well. This will produce a matrix \\mB = \\begin{bmatrix} \\vx & \\vy & \\vz\\end{bmatrix} \\in \\R^{n\\times 3} \\mB = \\begin{bmatrix} \\vx & \\vy & \\vz\\end{bmatrix} \\in \\R^{n\\times 3} where \\vx,\\vy \\vx,\\vy , and \\vz \\vz are the first, second, and third columns of \\mB \\mB , respectively. Matrices \u00b6 A matrix is an array of numbers and is another way of collecting numbers. For example, a matrix \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} contain n n rows and m m columns with a total of m\\cdot n m\\cdot n number of entries: \\mA \\in \\R^{m\\times n}, \\quad \\mA = \\begin{bmatrix}A_{1,1}& \\dots & A_{1,n}\\\\\\vdots&\\ddots &\\vdots\\\\A_{m,1}&\\dots&A_{m,n}\\end{bmatrix}. \\mA \\in \\R^{m\\times n}, \\quad \\mA = \\begin{bmatrix}A_{1,1}& \\dots & A_{1,n}\\\\\\vdots&\\ddots &\\vdots\\\\A_{m,1}&\\dots&A_{m,n}\\end{bmatrix}. The tranpose of this matrix \\mA \\mA is denoted by \\mA^\\intercal\\in\\R^{n\\times m} \\mA^\\intercal\\in\\R^{n\\times m} where \\mA^\\intercal =\\begin{bmatrix}A_{1,1}& \\dots & A_{m,1}\\\\\\vdots&\\ddots &\\vdots\\\\A_{1,n}&\\dots&A_{m,n}\\end{bmatrix} \\mA^\\intercal =\\begin{bmatrix}A_{1,1}& \\dots & A_{m,1}\\\\\\vdots&\\ddots &\\vdots\\\\A_{1,n}&\\dots&A_{m,n}\\end{bmatrix} A matrix \\mA \\mA is the zero matrix if all of its entrie are zero. The zero matrix is denoted by \\mzero \\mzero . For a matrix \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} , if m>n m>n then the matrix \\mA \\mA is a tall matrix, if m< n m< n then the matrix \\mA \\mA is a wide matrix, and if m = n m = n then the matrix \\mA \\mA is a square matrix. Matrices \\mA \\mA and \\mB \\mB are equal if these matrices are of same size and every element is equal: $$ \\mA = \\mB \\iff A_{ij} = B_{ij},\\quad \\forall i = 1,\\dots,m, \\quad j = 1,\\dots,n. $$ If all of the entries of a matrix \\mA \\mA are a scalar c \\in \\R c \\in \\R then we write \\mA = c \\mA = c . These notions are extended to inequalities in the following way: \\mA \\geq c \\mA \\geq c if and only if A_{ij} \\geq c A_{ij} \\geq c for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. \\mA > c \\mA > c if and only if A_{ij} > c A_{ij} > c for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. \\mA \\geq \\mB \\mA \\geq \\mB if and only if these matrices are of same size and A_{ij} \\geq B_{ij} A_{ij} \\geq B_{ij} for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. \\mA > \\mB \\mA > \\mB if and only if these matrices are of same size and A_{ij} > B_{ij} A_{ij} > B_{ij} for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. Inner products \u00b6 Inner product is a way to multiply two vectors and matrices to produce a scalar. For \\setV = \\R^n\\text{ or } \\R^{m\\times n} \\setV = \\R^n\\text{ or } \\R^{m\\times n} , inner product is a map \\langle\\cdot,\\cdot\\rangle:\\setV\\times \\setV\\rightarrow \\R \\langle\\cdot,\\cdot\\rangle:\\setV\\times \\setV\\rightarrow \\R that satisfies the following properties for all \\vx, \\vy, \\vz \\in \\setV \\vx, \\vy, \\vz \\in \\setV and \\alpha \\in \\R \\alpha \\in \\R : Non-negativity: \\langle \\vx,\\vx\\rangle \\geq 0 \\langle \\vx,\\vx\\rangle \\geq 0 and \\langle \\vx,\\vx\\rangle = 0 \\langle \\vx,\\vx\\rangle = 0 if and only if \\vx = 0 \\vx = 0 . Symmetry: \\langle \\vx, \\vy\\rangle = \\langle \\vy,\\vx\\rangle \\langle \\vx, \\vy\\rangle = \\langle \\vy,\\vx\\rangle . Linearity: \\langle\\alpha \\vx, \\vy\\rangle = \\alpha \\langle\\vx,\\vy\\rangle \\langle\\alpha \\vx, \\vy\\rangle = \\alpha \\langle\\vx,\\vy\\rangle and \\langle \\vx+\\vy,\\vz\\rangle = \\langle\\vx,\\vz\\rangle + \\langle\\vy,\\vz\\rangle \\langle \\vx+\\vy,\\vz\\rangle = \\langle\\vx,\\vz\\rangle + \\langle\\vy,\\vz\\rangle . For \\setV = \\R^n \\setV = \\R^n , the Euclidean inner product between two vector \\vx, \\vy\\in\\R^n \\vx, \\vy\\in\\R^n is also denoted as \\vx^\\intercal\\vy \\vx^\\intercal\\vy and \\vx^\\intercal\\vy = x_1y_1+x_2+y_2+\\dots+x_ny_n. \\vx^\\intercal\\vy = x_1y_1+x_2+y_2+\\dots+x_ny_n. \\exa{1} \\exa{1} Fix i \\in \\{1,\\dots,n\\} i \\in \\{1,\\dots,n\\} . The inner product of i i th standard basis vector \\ve_i \\ve_i with an arbitary vector \\vx \\vx is the i i th entry of \\vx \\vx , i.e. \\ve_i^\\intercal\\vx = x_i \\ve_i^\\intercal\\vx = x_i . \\exa{2} \\exa{2} The inner product of the vector of ones \\ve \\ve with an arbitary vector \\vx \\vx is in the sum of entres of \\vx \\vx , i.e. \\ve^\\intercal\\vx = x_1+x_2+\\dots+x_n \\ve^\\intercal\\vx = x_1+x_2+\\dots+x_n . \\exa{3} \\exa{3} The inner product on an arbitary vector \\vx \\vx with itself is the sum of squares of the entries of \\vx \\vx , i.e. \\vx^\\intercal\\vx =x_1^2+x_2^2+\\dots+x_n^2 \\vx^\\intercal\\vx =x_1^2+x_2^2+\\dots+x_n^2 . \\exa{4} \\exa{4} Let \\vp = (p_1,p_2,\\dots,p_n) \\vp = (p_1,p_2,\\dots,p_n) be a vector of prices and \\vq = (q_1,q_2,\\dots,q_n) \\vq = (q_1,q_2,\\dots,q_n) be the corresponding vector of quantities. The total cost is the inner product of \\vp \\vp and \\vq \\vq , i.e. \\vp^\\intercal\\vq = \\sum_{i = 1}^np_iq_i \\vp^\\intercal\\vq = \\sum_{i = 1}^np_iq_i . \\exa{5} \\exa{5} Let \\vw \\vw be a vector of assest allocations ( \\ve^\\intercal\\vw = 1, \\vw\\geq 0 \\ve^\\intercal\\vw = 1, \\vw\\geq 0 ) and \\vp \\vp be the corresponding vector of asset prices. The total portfolio value is \\vw^\\intercal\\vp \\vw^\\intercal\\vp . For \\setV = \\R^{m\\times n} \\setV = \\R^{m\\times n} , the trace inner product between two matrices \\mA, \\mB \\in \\R^{m\\times n} \\mA, \\mB \\in \\R^{m\\times n} is also denoted as \\text{tr}(\\mA^\\intercal\\mB) \\text{tr}(\\mA^\\intercal\\mB) and \\trace{\\mA^\\intercal\\mB} = \\sum_{i=1}^m\\sum_{j=1}^nA_{ij}B_{ij} =\\vec{\\mA}^\\intercal\\vec{\\mB}, \\trace{\\mA^\\intercal\\mB} = \\sum_{i=1}^m\\sum_{j=1}^nA_{ij}B_{ij} =\\vec{\\mA}^\\intercal\\vec{\\mB}, where for a square matrix \\mS \\in \\R^{n\\times n} \\mS \\in \\R^{n\\times n} , \\trace{\\mS} =\\sum_{i=1}^n S_{ii} \\trace{\\mS} =\\sum_{i=1}^n S_{ii} , and for a matrix \\mA \\mA with columns \\va_1,\\va_2,\\dots,\\va_n \\in \\R^m \\va_1,\\va_2,\\dots,\\va_n \\in \\R^m , \\vec{\\mA} = \\begin{bmatrix}\\va_1 \\\\\\va_2\\\\\\vdots\\\\\\va_n \\end{bmatrix}. \\vec{\\mA} = \\begin{bmatrix}\\va_1 \\\\\\va_2\\\\\\vdots\\\\\\va_n \\end{bmatrix}. Norms \u00b6 Norms are a measure of distances of a vector from the origin. The most common norm is the \"Euclidean norm\", i.e. 2-norm: \\|\\vx\\|_2 = \\sqrt{x_1^2+\\dots+x_n^2} \\|\\vx\\|_2 = \\sqrt{x_1^2+\\dots+x_n^2} This 2-norm is the norm induced by the Euclidean inner product on \\R^n \\R^n , where \\|\\vx\\|_2 = \\sqrt{\\vx^\\intercal\\vx} \\|\\vx\\|_2 = \\sqrt{\\vx^\\intercal\\vx} . The 2-norm reults in the familiar pythagorean identity, i.e. for any vectors \\va, \\vb, \\va, \\vb, and \\vc \\vc , \\|(\\va,\\vb,\\vc)\\|_2^2 = \\|\\va\\|_2^2+\\|\\vb\\|_2^2+\\|\\vc\\|_2^2. \\|(\\va,\\vb,\\vc)\\|_2^2 = \\|\\va\\|_2^2+\\|\\vb\\|_2^2+\\|\\vc\\|_2^2. An important inequlity that relates the inner product of two arbitary vectors with the norms of those vectors induced by the inner product is the Cauchy-Schwartz inequality. In the case of \\R^n \\R^n , the Cauchy-Schwartz inequality states that for all \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n , we have \\vx^\\intercal\\vy \\leq \\|\\vx\\|_2\\|\\vy\\|_2. \\vx^\\intercal\\vy \\leq \\|\\vx\\|_2\\|\\vy\\|_2. For \\R^n \\R^n , the Cauchy Schwartz inequality follows from the Cosine inequality, i.e. for any vectors \\vx, \\vy \\vx, \\vy with angle \\theta \\theta between \\vx \\vx and \\vy \\vy , we have \\vx^\\intercal \\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) \\vx^\\intercal \\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) . So, the Cauchy-Schwartz inequality attains the equality if and only if the two vectors are co-linear. The Cauchy-Schwartz inequality gives us another inequailty that norms, in general, satisfy. This inequailty is the triangle inequality and states that \\begin{equation}\\label{background:triangle} \\text{for any vectors } \\vx, \\vy \\in \\R^n,\\ \\|\\vx+\\vy\\|_2\\leq \\|\\vx\\|_2+\\|\\vy\\|_2 \\end{equation} \\begin{equation}\\label{background:triangle} \\text{for any vectors } \\vx, \\vy \\in \\R^n,\\ \\|\\vx+\\vy\\|_2\\leq \\|\\vx\\|_2+\\|\\vy\\|_2 \\end{equation} \\proof \\proof We will use Cauchy-Schwartz inequality to prove \\eqref{background:triangle}. Consider \\begin{align*} \\|\\vx+\\vy\\|_2^2 &= (\\vx+\\vy)^\\intercal(\\vx+\\vy)\\\\ & = \\vx^\\intercal\\vx + 2\\vx^\\intercal\\vy+\\vy^\\intercal\\vy\\\\ & = \\|\\vx\\|_2^2 + 2\\vx^\\intercal\\vy + \\|\\vy\\|_2^2\\\\ & \\leq \\|\\vx\\|_2^2 + 2\\|\\vx\\|_2\\|\\vy\\|_2+\\|\\vy\\|_2^2\\\\ & = (\\|\\vx\\|_2+\\|\\vy\\|_2)^2, \\end{align*} \\begin{align*} \\|\\vx+\\vy\\|_2^2 &= (\\vx+\\vy)^\\intercal(\\vx+\\vy)\\\\ & = \\vx^\\intercal\\vx + 2\\vx^\\intercal\\vy+\\vy^\\intercal\\vy\\\\ & = \\|\\vx\\|_2^2 + 2\\vx^\\intercal\\vy + \\|\\vy\\|_2^2\\\\ & \\leq \\|\\vx\\|_2^2 + 2\\|\\vx\\|_2\\|\\vy\\|_2+\\|\\vy\\|_2^2\\\\ & = (\\|\\vx\\|_2+\\|\\vy\\|_2)^2, \\end{align*} where the second equality follows from linearty of inner product and the inequality follows from the Cauchy-Schwartz inequality. q.e.d. More generally, norms are any function \\|\\cdot\\|:\\R^n \\rightarrow \\R \\|\\cdot\\|:\\R^n \\rightarrow \\R that satisfy the following conditions for all \\vx,\\vy \\in\\R^n \\vx,\\vy \\in\\R^n and \\beta \\in \\R \\beta \\in \\R : Homogeneity: \\|\\beta\\vx\\| = |\\beta|\\cdot\\|\\vx\\| \\|\\beta\\vx\\| = |\\beta|\\cdot\\|\\vx\\| . Triangle inequality: \\|\\vx+\\vy\\|\\leq \\|\\vx\\|+\\|\\vy\\| \\|\\vx+\\vy\\|\\leq \\|\\vx\\|+\\|\\vy\\| . Non-negativity: \\|\\vx\\|\\geq 0 \\|\\vx\\|\\geq 0 and \\|\\vx\\|=0 \\|\\vx\\|=0 if and only if \\vx = \\vzero \\vx = \\vzero . Other useful norms that satify the above conditions are: 1-norm: The 1-norm of \\vx \\vx is \\|\\vx\\|_1:=|x_1|+\\dots+|x_n| \\|\\vx\\|_1:=|x_1|+\\dots+|x_n| . Sup-norm: The sup-norm of \\vx \\vx is the largest entry of \\vx \\vx in absolute value, i.e. \\|\\vx\\|_\\infty = \\max_{j}|x_j| \\|\\vx\\|_\\infty = \\max_{j}|x_j| . p-norm: For p\\geq 1 p\\geq 1 , the p-norm of \\vx \\vx is \\|\\vx\\|_p = \\left(\\sum_{j=1}^n|x_j|^p\\right)^\\tfrac{1}{p} \\|\\vx\\|_p = \\left(\\sum_{j=1}^n|x_j|^p\\right)^\\tfrac{1}{p} . Linear functions \u00b6 Linear function in a mapping between two Eucliean spaces that preserve the operations of addition and scalar multiplication. Specifially, a function \\func{f}:\\R^n\\rightarrow\\R^m \\func{f}:\\R^n\\rightarrow\\R^m is linear if for all \\vx,\\vy\\in\\R^n \\vx,\\vy\\in\\R^n and \\alpha\\in\\R \\alpha\\in\\R , we have \\func{f}(\\alpha\\vx+\\vy) = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) \\func{f}(\\alpha\\vx+\\vy) = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) Here, \\func{f} \\func{f} takes a vector of dimension n n as its input and outputs a vector of dimension of m m . If m=1 m=1 , then we say the function \\func{f} \\func{f} is a real valued function. \\prop \\prop A real valued function \\func{f}:\\R^n\\rightarrow \\R \\func{f}:\\R^n\\rightarrow \\R is linear if and only if \\func{f} = \\va^\\intercal\\vx \\func{f} = \\va^\\intercal\\vx for some \\va \\in\\R^n \\va \\in\\R^n . \\proof \\proof We will first prove the forward direction, i.e. if \\func{f}:\\R^n\\rightarrow\\R \\func{f}:\\R^n\\rightarrow\\R is a linear function then \\func{f}(\\vx)=\\va^\\intercal\\vx \\func{f}(\\vx)=\\va^\\intercal\\vx . For i\\in\\{1,\\dots,n\\} i\\in\\{1,\\dots,n\\} , let a_{i} = \\func{f}(\\ve_i) a_{i} = \\func{f}(\\ve_i) . Then since \\vx = x_1\\ve_1+\\dots+x_n\\ve_n \\vx = x_1\\ve_1+\\dots+x_n\\ve_n , we have \\begin{align*} \\func{f}(\\vx) &= \\func{f}(x_1\\ve_1+\\dots+x_n\\ve_n)\\\\ &=x_1\\func{f}(\\ve_1)+\\dots+x_n\\func{f}(\\ve_n)\\\\ &= x_1\\va_1+\\dots+x_n\\va_n\\\\ &= \\va^\\intercal\\vx. \\end{align*} \\begin{align*} \\func{f}(\\vx) &= \\func{f}(x_1\\ve_1+\\dots+x_n\\ve_n)\\\\ &=x_1\\func{f}(\\ve_1)+\\dots+x_n\\func{f}(\\ve_n)\\\\ &= x_1\\va_1+\\dots+x_n\\va_n\\\\ &= \\va^\\intercal\\vx. \\end{align*} Second, we prove the reverse direction, i.e. if \\func{f}(\\vx) = \\va^\\intercal\\vx \\func{f}(\\vx) = \\va^\\intercal\\vx for some \\va \\in \\R^n \\va \\in \\R^n then \\func{f} \\func{f} is a real valued linear function. Clearly, \\func{f}(x) \\func{f}(x) is a real valued function. Let \\vx,\\vy\\in \\R^n \\vx,\\vy\\in \\R^n and \\alpha \\in \\R \\alpha \\in \\R . Consider \\begin{align*} \\func{f}(\\alpha\\vx+\\vy) &= \\va^\\intercal(\\alpha\\vx+\\vy)\\\\ &=\\va^\\intercal(\\alpha\\vx)+\\va^\\intercal\\vy\\\\ & = \\alpha \\va^\\intercal\\vx+\\va^\\intercal\\vy\\\\ & = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) \\end{align*} \\begin{align*} \\func{f}(\\alpha\\vx+\\vy) &= \\va^\\intercal(\\alpha\\vx+\\vy)\\\\ &=\\va^\\intercal(\\alpha\\vx)+\\va^\\intercal\\vy\\\\ & = \\alpha \\va^\\intercal\\vx+\\va^\\intercal\\vy\\\\ & = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) \\end{align*} where the second inequality follows from linearity of inner product. q.e.d.","title":"Mathematical background"},{"location":"notes/background/#mathematical-background","text":"","title":"Mathematical background"},{"location":"notes/background/#vectors","text":"Vectors are arrays of numbers ordered as a column. For example, a vector \\vx \\in \\R^n \\vx \\in \\R^n contain n n entries with x_i x_i as its i i th entry: \\vx \\in \\R^n,\\quad \\vx = \\begin{bmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{bmatrix},\\quad \\vx = \\begin{pmatrix}x_1, x_2,\\dots,x_n\\end{pmatrix}. \\vx \\in \\R^n,\\quad \\vx = \\begin{bmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{bmatrix},\\quad \\vx = \\begin{pmatrix}x_1, x_2,\\dots,x_n\\end{pmatrix}. Some special vectors that are commonly used are the zero vector, ones vector, and the standard basis vectors. These are defined next: Zero vector: A vector \\vx \\vx is the zero vector if all of its entries are zero. The zero vector is denoted by \\vzero \\vzero . Ones vector: A vector \\vx \\vx is the ones vector if all of its entries are one. The ones vector is denoted by \\ve \\ve . i i th standard basis vector: A vector \\vx \\in \\R^n \\vx \\in \\R^n is the i i th standard basis vector if x_j = \\Big\\{\\begin{array}{cc}1 & i = j\\\\0 & \\text{otherwise}\\end{array} x_j = \\Big\\{\\begin{array}{cc}1 & i = j\\\\0 & \\text{otherwise}\\end{array} . The i i th standard basis vector is denoted by \\ve_i \\ve_i . For example, in \\R^3 \\R^3 , the 2 2 nd standard basis vector is \\ve_2 = \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix} \\ve_2 = \\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix} . A vector \\vx \\in \\R^n \\vx \\in \\R^n is equal to another vector \\vy \\in \\R^n \\vy \\in \\R^n if x_i = y_i x_i = y_i for all i \\in \\{1,2,\\dots,n\\} i \\in \\{1,2,\\dots,n\\} . Similarly, if all of the entries of a vector \\vx \\vx is equal to a scalar c \\in \\R c \\in \\R , then we write \\vx = c \\vx = c . These notions can be extended to inequalites in the following way: A vector \\vx \\in \\R^n \\vx \\in \\R^n and a scalar c \\in \\R c \\in \\R satisfy \\vx \\geq c \\vx \\geq c if and only if x_i \\geq c x_i \\geq c for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . A vector \\vx \\in \\R^n \\vx \\in \\R^n and a scalar c \\in \\R c \\in \\R satisfy \\vx > c \\vx > c if and only if x_i > c x_i > c for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . Vectors \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n satisfy \\vx \\geq \\vy \\vx \\geq \\vy if and only if x_i \\geq y_i x_i \\geq y_i for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . Vectors \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n satisfy \\vx \\geq \\vy \\vx \\geq \\vy if and only if x_i \\geq y_i x_i \\geq y_i for all i \\in \\{1,2,\\dots, n\\} i \\in \\{1,2,\\dots, n\\} . Block vector is a concise representation of a vector obtained by stacking vectors. For example, if \\vx \\in \\R^m \\vx \\in \\R^m , \\vy \\in \\R^n \\vy \\in \\R^n , and \\vz \\in \\R^p \\vz \\in \\R^p , then \\va = \\begin{bmatrix} \\vx \\\\ \\dots \\\\ \\vy \\\\ \\dots \\\\ \\vz\\end{bmatrix}\\in \\R^{m+n+p} \\va = \\begin{bmatrix} \\vx \\\\ \\dots \\\\ \\vy \\\\ \\dots \\\\ \\vz\\end{bmatrix}\\in \\R^{m+n+p} is a block vector (also called stacked vector or conncatenated vector) obtained by stacking \\vx \\vx , \\vy \\vy , and \\vz \\vz vertically. This block vector \\va \\va can also be written as \\begin{pmatrix}\\vx,\\vy,\\vz \\end{pmatrix} \\begin{pmatrix}\\vx,\\vy,\\vz \\end{pmatrix} . If the dimension of the vectors are equal (i.e. m = n = p m = n = p ), then these vector can be stacked horizontally as well. This will produce a matrix \\mB = \\begin{bmatrix} \\vx & \\vy & \\vz\\end{bmatrix} \\in \\R^{n\\times 3} \\mB = \\begin{bmatrix} \\vx & \\vy & \\vz\\end{bmatrix} \\in \\R^{n\\times 3} where \\vx,\\vy \\vx,\\vy , and \\vz \\vz are the first, second, and third columns of \\mB \\mB , respectively.","title":"Vectors"},{"location":"notes/background/#matrices","text":"A matrix is an array of numbers and is another way of collecting numbers. For example, a matrix \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} contain n n rows and m m columns with a total of m\\cdot n m\\cdot n number of entries: \\mA \\in \\R^{m\\times n}, \\quad \\mA = \\begin{bmatrix}A_{1,1}& \\dots & A_{1,n}\\\\\\vdots&\\ddots &\\vdots\\\\A_{m,1}&\\dots&A_{m,n}\\end{bmatrix}. \\mA \\in \\R^{m\\times n}, \\quad \\mA = \\begin{bmatrix}A_{1,1}& \\dots & A_{1,n}\\\\\\vdots&\\ddots &\\vdots\\\\A_{m,1}&\\dots&A_{m,n}\\end{bmatrix}. The tranpose of this matrix \\mA \\mA is denoted by \\mA^\\intercal\\in\\R^{n\\times m} \\mA^\\intercal\\in\\R^{n\\times m} where \\mA^\\intercal =\\begin{bmatrix}A_{1,1}& \\dots & A_{m,1}\\\\\\vdots&\\ddots &\\vdots\\\\A_{1,n}&\\dots&A_{m,n}\\end{bmatrix} \\mA^\\intercal =\\begin{bmatrix}A_{1,1}& \\dots & A_{m,1}\\\\\\vdots&\\ddots &\\vdots\\\\A_{1,n}&\\dots&A_{m,n}\\end{bmatrix} A matrix \\mA \\mA is the zero matrix if all of its entrie are zero. The zero matrix is denoted by \\mzero \\mzero . For a matrix \\mA \\in \\R^{m\\times n} \\mA \\in \\R^{m\\times n} , if m>n m>n then the matrix \\mA \\mA is a tall matrix, if m< n m< n then the matrix \\mA \\mA is a wide matrix, and if m = n m = n then the matrix \\mA \\mA is a square matrix. Matrices \\mA \\mA and \\mB \\mB are equal if these matrices are of same size and every element is equal: $$ \\mA = \\mB \\iff A_{ij} = B_{ij},\\quad \\forall i = 1,\\dots,m, \\quad j = 1,\\dots,n. $$ If all of the entries of a matrix \\mA \\mA are a scalar c \\in \\R c \\in \\R then we write \\mA = c \\mA = c . These notions are extended to inequalities in the following way: \\mA \\geq c \\mA \\geq c if and only if A_{ij} \\geq c A_{ij} \\geq c for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. \\mA > c \\mA > c if and only if A_{ij} > c A_{ij} > c for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. \\mA \\geq \\mB \\mA \\geq \\mB if and only if these matrices are of same size and A_{ij} \\geq B_{ij} A_{ij} \\geq B_{ij} for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n. \\mA > \\mB \\mA > \\mB if and only if these matrices are of same size and A_{ij} > B_{ij} A_{ij} > B_{ij} for all i = 1,\\dots,m i = 1,\\dots,m and j = 1,\\dots,n. j = 1,\\dots,n.","title":"Matrices"},{"location":"notes/background/#inner-products","text":"Inner product is a way to multiply two vectors and matrices to produce a scalar. For \\setV = \\R^n\\text{ or } \\R^{m\\times n} \\setV = \\R^n\\text{ or } \\R^{m\\times n} , inner product is a map \\langle\\cdot,\\cdot\\rangle:\\setV\\times \\setV\\rightarrow \\R \\langle\\cdot,\\cdot\\rangle:\\setV\\times \\setV\\rightarrow \\R that satisfies the following properties for all \\vx, \\vy, \\vz \\in \\setV \\vx, \\vy, \\vz \\in \\setV and \\alpha \\in \\R \\alpha \\in \\R : Non-negativity: \\langle \\vx,\\vx\\rangle \\geq 0 \\langle \\vx,\\vx\\rangle \\geq 0 and \\langle \\vx,\\vx\\rangle = 0 \\langle \\vx,\\vx\\rangle = 0 if and only if \\vx = 0 \\vx = 0 . Symmetry: \\langle \\vx, \\vy\\rangle = \\langle \\vy,\\vx\\rangle \\langle \\vx, \\vy\\rangle = \\langle \\vy,\\vx\\rangle . Linearity: \\langle\\alpha \\vx, \\vy\\rangle = \\alpha \\langle\\vx,\\vy\\rangle \\langle\\alpha \\vx, \\vy\\rangle = \\alpha \\langle\\vx,\\vy\\rangle and \\langle \\vx+\\vy,\\vz\\rangle = \\langle\\vx,\\vz\\rangle + \\langle\\vy,\\vz\\rangle \\langle \\vx+\\vy,\\vz\\rangle = \\langle\\vx,\\vz\\rangle + \\langle\\vy,\\vz\\rangle . For \\setV = \\R^n \\setV = \\R^n , the Euclidean inner product between two vector \\vx, \\vy\\in\\R^n \\vx, \\vy\\in\\R^n is also denoted as \\vx^\\intercal\\vy \\vx^\\intercal\\vy and \\vx^\\intercal\\vy = x_1y_1+x_2+y_2+\\dots+x_ny_n. \\vx^\\intercal\\vy = x_1y_1+x_2+y_2+\\dots+x_ny_n. \\exa{1} \\exa{1} Fix i \\in \\{1,\\dots,n\\} i \\in \\{1,\\dots,n\\} . The inner product of i i th standard basis vector \\ve_i \\ve_i with an arbitary vector \\vx \\vx is the i i th entry of \\vx \\vx , i.e. \\ve_i^\\intercal\\vx = x_i \\ve_i^\\intercal\\vx = x_i . \\exa{2} \\exa{2} The inner product of the vector of ones \\ve \\ve with an arbitary vector \\vx \\vx is in the sum of entres of \\vx \\vx , i.e. \\ve^\\intercal\\vx = x_1+x_2+\\dots+x_n \\ve^\\intercal\\vx = x_1+x_2+\\dots+x_n . \\exa{3} \\exa{3} The inner product on an arbitary vector \\vx \\vx with itself is the sum of squares of the entries of \\vx \\vx , i.e. \\vx^\\intercal\\vx =x_1^2+x_2^2+\\dots+x_n^2 \\vx^\\intercal\\vx =x_1^2+x_2^2+\\dots+x_n^2 . \\exa{4} \\exa{4} Let \\vp = (p_1,p_2,\\dots,p_n) \\vp = (p_1,p_2,\\dots,p_n) be a vector of prices and \\vq = (q_1,q_2,\\dots,q_n) \\vq = (q_1,q_2,\\dots,q_n) be the corresponding vector of quantities. The total cost is the inner product of \\vp \\vp and \\vq \\vq , i.e. \\vp^\\intercal\\vq = \\sum_{i = 1}^np_iq_i \\vp^\\intercal\\vq = \\sum_{i = 1}^np_iq_i . \\exa{5} \\exa{5} Let \\vw \\vw be a vector of assest allocations ( \\ve^\\intercal\\vw = 1, \\vw\\geq 0 \\ve^\\intercal\\vw = 1, \\vw\\geq 0 ) and \\vp \\vp be the corresponding vector of asset prices. The total portfolio value is \\vw^\\intercal\\vp \\vw^\\intercal\\vp . For \\setV = \\R^{m\\times n} \\setV = \\R^{m\\times n} , the trace inner product between two matrices \\mA, \\mB \\in \\R^{m\\times n} \\mA, \\mB \\in \\R^{m\\times n} is also denoted as \\text{tr}(\\mA^\\intercal\\mB) \\text{tr}(\\mA^\\intercal\\mB) and \\trace{\\mA^\\intercal\\mB} = \\sum_{i=1}^m\\sum_{j=1}^nA_{ij}B_{ij} =\\vec{\\mA}^\\intercal\\vec{\\mB}, \\trace{\\mA^\\intercal\\mB} = \\sum_{i=1}^m\\sum_{j=1}^nA_{ij}B_{ij} =\\vec{\\mA}^\\intercal\\vec{\\mB}, where for a square matrix \\mS \\in \\R^{n\\times n} \\mS \\in \\R^{n\\times n} , \\trace{\\mS} =\\sum_{i=1}^n S_{ii} \\trace{\\mS} =\\sum_{i=1}^n S_{ii} , and for a matrix \\mA \\mA with columns \\va_1,\\va_2,\\dots,\\va_n \\in \\R^m \\va_1,\\va_2,\\dots,\\va_n \\in \\R^m , \\vec{\\mA} = \\begin{bmatrix}\\va_1 \\\\\\va_2\\\\\\vdots\\\\\\va_n \\end{bmatrix}. \\vec{\\mA} = \\begin{bmatrix}\\va_1 \\\\\\va_2\\\\\\vdots\\\\\\va_n \\end{bmatrix}.","title":"Inner products"},{"location":"notes/background/#norms","text":"Norms are a measure of distances of a vector from the origin. The most common norm is the \"Euclidean norm\", i.e. 2-norm: \\|\\vx\\|_2 = \\sqrt{x_1^2+\\dots+x_n^2} \\|\\vx\\|_2 = \\sqrt{x_1^2+\\dots+x_n^2} This 2-norm is the norm induced by the Euclidean inner product on \\R^n \\R^n , where \\|\\vx\\|_2 = \\sqrt{\\vx^\\intercal\\vx} \\|\\vx\\|_2 = \\sqrt{\\vx^\\intercal\\vx} . The 2-norm reults in the familiar pythagorean identity, i.e. for any vectors \\va, \\vb, \\va, \\vb, and \\vc \\vc , \\|(\\va,\\vb,\\vc)\\|_2^2 = \\|\\va\\|_2^2+\\|\\vb\\|_2^2+\\|\\vc\\|_2^2. \\|(\\va,\\vb,\\vc)\\|_2^2 = \\|\\va\\|_2^2+\\|\\vb\\|_2^2+\\|\\vc\\|_2^2. An important inequlity that relates the inner product of two arbitary vectors with the norms of those vectors induced by the inner product is the Cauchy-Schwartz inequality. In the case of \\R^n \\R^n , the Cauchy-Schwartz inequality states that for all \\vx, \\vy \\in \\R^n \\vx, \\vy \\in \\R^n , we have \\vx^\\intercal\\vy \\leq \\|\\vx\\|_2\\|\\vy\\|_2. \\vx^\\intercal\\vy \\leq \\|\\vx\\|_2\\|\\vy\\|_2. For \\R^n \\R^n , the Cauchy Schwartz inequality follows from the Cosine inequality, i.e. for any vectors \\vx, \\vy \\vx, \\vy with angle \\theta \\theta between \\vx \\vx and \\vy \\vy , we have \\vx^\\intercal \\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) \\vx^\\intercal \\vy = \\|\\vx\\|_2\\|\\vy\\|_2\\cos(\\theta) . So, the Cauchy-Schwartz inequality attains the equality if and only if the two vectors are co-linear. The Cauchy-Schwartz inequality gives us another inequailty that norms, in general, satisfy. This inequailty is the triangle inequality and states that \\begin{equation}\\label{background:triangle} \\text{for any vectors } \\vx, \\vy \\in \\R^n,\\ \\|\\vx+\\vy\\|_2\\leq \\|\\vx\\|_2+\\|\\vy\\|_2 \\end{equation} \\begin{equation}\\label{background:triangle} \\text{for any vectors } \\vx, \\vy \\in \\R^n,\\ \\|\\vx+\\vy\\|_2\\leq \\|\\vx\\|_2+\\|\\vy\\|_2 \\end{equation} \\proof \\proof We will use Cauchy-Schwartz inequality to prove \\eqref{background:triangle}. Consider \\begin{align*} \\|\\vx+\\vy\\|_2^2 &= (\\vx+\\vy)^\\intercal(\\vx+\\vy)\\\\ & = \\vx^\\intercal\\vx + 2\\vx^\\intercal\\vy+\\vy^\\intercal\\vy\\\\ & = \\|\\vx\\|_2^2 + 2\\vx^\\intercal\\vy + \\|\\vy\\|_2^2\\\\ & \\leq \\|\\vx\\|_2^2 + 2\\|\\vx\\|_2\\|\\vy\\|_2+\\|\\vy\\|_2^2\\\\ & = (\\|\\vx\\|_2+\\|\\vy\\|_2)^2, \\end{align*} \\begin{align*} \\|\\vx+\\vy\\|_2^2 &= (\\vx+\\vy)^\\intercal(\\vx+\\vy)\\\\ & = \\vx^\\intercal\\vx + 2\\vx^\\intercal\\vy+\\vy^\\intercal\\vy\\\\ & = \\|\\vx\\|_2^2 + 2\\vx^\\intercal\\vy + \\|\\vy\\|_2^2\\\\ & \\leq \\|\\vx\\|_2^2 + 2\\|\\vx\\|_2\\|\\vy\\|_2+\\|\\vy\\|_2^2\\\\ & = (\\|\\vx\\|_2+\\|\\vy\\|_2)^2, \\end{align*} where the second equality follows from linearty of inner product and the inequality follows from the Cauchy-Schwartz inequality. q.e.d. More generally, norms are any function \\|\\cdot\\|:\\R^n \\rightarrow \\R \\|\\cdot\\|:\\R^n \\rightarrow \\R that satisfy the following conditions for all \\vx,\\vy \\in\\R^n \\vx,\\vy \\in\\R^n and \\beta \\in \\R \\beta \\in \\R : Homogeneity: \\|\\beta\\vx\\| = |\\beta|\\cdot\\|\\vx\\| \\|\\beta\\vx\\| = |\\beta|\\cdot\\|\\vx\\| . Triangle inequality: \\|\\vx+\\vy\\|\\leq \\|\\vx\\|+\\|\\vy\\| \\|\\vx+\\vy\\|\\leq \\|\\vx\\|+\\|\\vy\\| . Non-negativity: \\|\\vx\\|\\geq 0 \\|\\vx\\|\\geq 0 and \\|\\vx\\|=0 \\|\\vx\\|=0 if and only if \\vx = \\vzero \\vx = \\vzero . Other useful norms that satify the above conditions are: 1-norm: The 1-norm of \\vx \\vx is \\|\\vx\\|_1:=|x_1|+\\dots+|x_n| \\|\\vx\\|_1:=|x_1|+\\dots+|x_n| . Sup-norm: The sup-norm of \\vx \\vx is the largest entry of \\vx \\vx in absolute value, i.e. \\|\\vx\\|_\\infty = \\max_{j}|x_j| \\|\\vx\\|_\\infty = \\max_{j}|x_j| . p-norm: For p\\geq 1 p\\geq 1 , the p-norm of \\vx \\vx is \\|\\vx\\|_p = \\left(\\sum_{j=1}^n|x_j|^p\\right)^\\tfrac{1}{p} \\|\\vx\\|_p = \\left(\\sum_{j=1}^n|x_j|^p\\right)^\\tfrac{1}{p} .","title":"Norms"},{"location":"notes/background/#linear-functions","text":"Linear function in a mapping between two Eucliean spaces that preserve the operations of addition and scalar multiplication. Specifially, a function \\func{f}:\\R^n\\rightarrow\\R^m \\func{f}:\\R^n\\rightarrow\\R^m is linear if for all \\vx,\\vy\\in\\R^n \\vx,\\vy\\in\\R^n and \\alpha\\in\\R \\alpha\\in\\R , we have \\func{f}(\\alpha\\vx+\\vy) = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) \\func{f}(\\alpha\\vx+\\vy) = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) Here, \\func{f} \\func{f} takes a vector of dimension n n as its input and outputs a vector of dimension of m m . If m=1 m=1 , then we say the function \\func{f} \\func{f} is a real valued function. \\prop \\prop A real valued function \\func{f}:\\R^n\\rightarrow \\R \\func{f}:\\R^n\\rightarrow \\R is linear if and only if \\func{f} = \\va^\\intercal\\vx \\func{f} = \\va^\\intercal\\vx for some \\va \\in\\R^n \\va \\in\\R^n . \\proof \\proof We will first prove the forward direction, i.e. if \\func{f}:\\R^n\\rightarrow\\R \\func{f}:\\R^n\\rightarrow\\R is a linear function then \\func{f}(\\vx)=\\va^\\intercal\\vx \\func{f}(\\vx)=\\va^\\intercal\\vx . For i\\in\\{1,\\dots,n\\} i\\in\\{1,\\dots,n\\} , let a_{i} = \\func{f}(\\ve_i) a_{i} = \\func{f}(\\ve_i) . Then since \\vx = x_1\\ve_1+\\dots+x_n\\ve_n \\vx = x_1\\ve_1+\\dots+x_n\\ve_n , we have \\begin{align*} \\func{f}(\\vx) &= \\func{f}(x_1\\ve_1+\\dots+x_n\\ve_n)\\\\ &=x_1\\func{f}(\\ve_1)+\\dots+x_n\\func{f}(\\ve_n)\\\\ &= x_1\\va_1+\\dots+x_n\\va_n\\\\ &= \\va^\\intercal\\vx. \\end{align*} \\begin{align*} \\func{f}(\\vx) &= \\func{f}(x_1\\ve_1+\\dots+x_n\\ve_n)\\\\ &=x_1\\func{f}(\\ve_1)+\\dots+x_n\\func{f}(\\ve_n)\\\\ &= x_1\\va_1+\\dots+x_n\\va_n\\\\ &= \\va^\\intercal\\vx. \\end{align*} Second, we prove the reverse direction, i.e. if \\func{f}(\\vx) = \\va^\\intercal\\vx \\func{f}(\\vx) = \\va^\\intercal\\vx for some \\va \\in \\R^n \\va \\in \\R^n then \\func{f} \\func{f} is a real valued linear function. Clearly, \\func{f}(x) \\func{f}(x) is a real valued function. Let \\vx,\\vy\\in \\R^n \\vx,\\vy\\in \\R^n and \\alpha \\in \\R \\alpha \\in \\R . Consider \\begin{align*} \\func{f}(\\alpha\\vx+\\vy) &= \\va^\\intercal(\\alpha\\vx+\\vy)\\\\ &=\\va^\\intercal(\\alpha\\vx)+\\va^\\intercal\\vy\\\\ & = \\alpha \\va^\\intercal\\vx+\\va^\\intercal\\vy\\\\ & = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) \\end{align*} \\begin{align*} \\func{f}(\\alpha\\vx+\\vy) &= \\va^\\intercal(\\alpha\\vx+\\vy)\\\\ &=\\va^\\intercal(\\alpha\\vx)+\\va^\\intercal\\vy\\\\ & = \\alpha \\va^\\intercal\\vx+\\va^\\intercal\\vy\\\\ & = \\alpha\\func{f}(\\vx)+\\func{f}(\\vy) \\end{align*} where the second inequality follows from linearity of inner product. q.e.d.","title":"Linear functions"},{"location":"notes/introduction/","text":"Introduction \u00b6 This is the introduction to the class. We will cover unconstrained and constrained optimization. Modelling \u00b6 This is how you model. See eq 1 Let's create another reference to optimimality","title":"Introduction"},{"location":"notes/introduction/#introduction","text":"This is the introduction to the class. We will cover unconstrained and constrained optimization.","title":"Introduction"},{"location":"notes/introduction/#modelling","text":"This is how you model. See eq 1 Let's create another reference to optimimality","title":"Modelling"},{"location":"notes/unconstrained/","text":"Unconstrained Optimization \u00b6 In this lecture we consider unconstrained optimization given by \\begin{equation}\\label{unconstrained_prob} \\mathop{\\text{minimize}}_{\\vx \\in \\set{S}} f(\\vx),\\end{equation} \\begin{equation}\\label{unconstrained_prob} \\mathop{\\text{minimize}}_{\\vx \\in \\set{S}} f(\\vx),\\end{equation} where f:\\R^n \\rightarrow \\R f:\\R^n \\rightarrow \\R and \\set{S}\\subset \\R^n \\set{S}\\subset \\R^n . In the course, we will primarily consider minimization problems. This is because a maximizer of f(\\vx) f(\\vx) is the minimzer of -f(\\vx) -f(\\vx) . Optimality \u00b6 A point \\vx^*\\in \\set{S} \\vx^*\\in \\set{S} is a global minimizer of f f if f(\\vx^*) \\leq f(\\vx) f(\\vx^*) \\leq f(\\vx) for all \\vx\\in \\mS \\vx\\in \\mS global maximizer of f f if f(\\vx^*) \\geq f(\\vx) f(\\vx^*) \\geq f(\\vx) for all \\vx\\in \\mS \\vx\\in \\mS local minimizer of f f if f(\\vx^*) \\leq f(\\vx) f(\\vx^*) \\leq f(\\vx) for all \\vx\\in \\mS \\vx\\in \\mS where \\|\\vx-\\vx^*\\|_2 \\leq\\epsilon \\|\\vx-\\vx^*\\|_2 \\leq\\epsilon local maximizer of f f if f(\\vx^*) \\geq f(\\vx) f(\\vx^*) \\geq f(\\vx) for all \\vx\\in \\mS \\vx\\in \\mS where \\|\\vx-\\vx^*\\|_2 \\leq\\epsilon \\|\\vx-\\vx^*\\|_2 \\leq\\epsilon for some \\epsilon > 0 \\epsilon > 0 . A maximizer (or minimizer) \\vx^* \\vx^* of \\eqref{unconstrained_prob} is a strict maximizer (or minimizer) if for all points in a neighborhood of \\vx^* \\vx^* , the objective f(x) f(x) does not attain the value of f(\\vx^{*}) f(\\vx^{*}) . Equivalently, this definition of strict maximizer/minimizer in the global case can be stated as: \\vx^* \\vx^* is a strict global min or max if for all \\vx\\in \\set{S} \\vx\\in \\set{S} , we have f(\\vx^*) = f(\\vx) f(\\vx^*) = f(\\vx) if and only if \\vx = \\vx^* \\vx = \\vx^* . The above figure shows that even if the optimality is attained, the optimal point may not be unique. There are cases when the optimal point is not attained or even exist. An example of these cases are shown below \\exa{1} \\exa{1} Consider the minimization program \\mathop{\\text{minimize}}_{x,y} \\{f(x,y) = x + y: x^2 + y^2 \\leq 2\\}. \\mathop{\\text{minimize}}_{x,y} \\{f(x,y) = x + y: x^2 + y^2 \\leq 2\\}. The figure below shows the interaction bewtween the objective fuction and set \\{(x,y):x^2 + y^2 \\leq 2\\} \\{(x,y):x^2 + y^2 \\leq 2\\} . The pink lines are the \"level sets\" and the arrows are directions of descent. The optimal point is \\bmat -1\\\\ -1 \\emat \\bmat -1\\\\ -1 \\emat . \\exa{2} \\exa{2} Consider the minimization program \\mathop{\\text{minimize}}_{x,y} \\{f(x,y) = \\frac{x+y}{x^2+y^2+1}:x,y\\in\\R\\}. \\mathop{\\text{minimize}}_{x,y} \\{f(x,y) = \\frac{x+y}{x^2+y^2+1}:x,y\\in\\R\\}. The surface and contour plots of f(x,y) f(x,y) are using Plots ; pyplot () f ( x , y ) = ( x + y ) / ( x ^ 2 + y ^ 2 + 1 ) x = range ( - 4 , stop = 4 , length = 100 ) y = range ( - 5 , stop = 5 , length = 100 ) pyplot ( size = ( 700 , 200 )) plot ( plot ( x , y , f , st =: surface , camera = ( 100 , 30 )), plot ( x , y , f , st =: contour , camera = ( 100 , 30 ))) The global minimizer is \\begin{pmatrix} \\frac{1}{\\sqrt{2}},& \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}},& \\frac{1}{\\sqrt{2}} \\end{pmatrix} and the global maximizer is \\bmatp -\\frac{1}{\\sqrt{2}}, & -\\frac{1}{\\sqrt{2}} \\ematp \\bmatp -\\frac{1}{\\sqrt{2}}, & -\\frac{1}{\\sqrt{2}} \\ematp . Sufficient conditions in 1-d \u00b6 We will first look at 1-d function f:\\R\u2192\\R f:\\R\u2192\\R and state sufficent conditions for optimality of a point \\vx \\vx in the interrior of the set \\set{S} \\set{S} . Consider the following 1-d function The critial points of f f in the above figure are x \\in \\{ b ,\\ c,\\ d,\\ e\\} x \\in \\{ b ,\\ c,\\ d,\\ e\\} and these points satisfy f'(x) =0 f'(x) =0 . We may be able to use second derivative information to determine if these critial points are minimizer or maximizer. Precisely, for any function f:\\R\\rightarrow\\R f:\\R\\rightarrow\\R , a point x = x^* x = x^* is a local minimizer if f'(x) = 0 f'(x) = 0 and f''(x)>0 f''(x)>0 , and a local maximizer if f'(x) = 0 f'(x) = 0 and f''(x)<0 f''(x)<0 . However, if both f'(x) = 0 f'(x) = 0 and f''(x)=0 f''(x)=0 , there is not enough information to draw any conclusion. As an example, consider f(x) = x^3 f(x) = x^3 and f(x) = x^4 f(x) = x^4 . For both function, f'(x) = 0 f'(x) = 0 and f''(x) = 0 f''(x) = 0 , however x = 0 x = 0 is saddle point of x^3 x^3 and x = 0 x = 0 is a minimizer of x^4 x^4 . A proof of the sufficient condition of optimality follows directly from Taylor approximation of the function f(x) f(x) . Consider the case where f'(x^*) = 0 f'(x^*) = 0 and f''(x^*)>0 f''(x^*)>0 for some x^*\\in\\set{S} x^*\\in\\set{S} . Then by Taylor's theorem of f(x) f(x) at x = x^* x = x^* , we get f(x) = f(x^*) + \\underbrace{f'(x^*)(x-x^*)}_{ =\\ 0} + \\underbrace{\\frac{1}{2}f''(x^*)(x-x^*)^2}_{\\text{strictly positive}} + \\underbrace{O((x-x^*)^3)}_{\\text{small}} f(x) = f(x^*) + \\underbrace{f'(x^*)(x-x^*)}_{ =\\ 0} + \\underbrace{\\frac{1}{2}f''(x^*)(x-x^*)^2}_{\\text{strictly positive}} + \\underbrace{O((x-x^*)^3)}_{\\text{small}} for x x close to x^* x^* . So f(x) > f(x^*) f(x) > f(x^*) for all x x near x^* x^* , which imples x^* x^* is local minimizer. Similarly, we can prove f(x^*)=0 f(x^*)=0 and f''(x^*)<0 f''(x^*)<0 implies x^* x^* is a local maximizer. In the next section we will generalize these sufficient condition for any real valued function f:\\R^n \\rightarrow \\R f:\\R^n \\rightarrow \\R . Gradient, Hessian and directional derivatives \u00b6 For a differentiable function f:\\R^n\\to\\R f:\\R^n\\to\\R , the gradient of f f at \\vx \\vx is a vector in \\R^n \\R^n . The gradient of f f at \\vx \\vx contain the partial derivative of f f at x x and is given by \\nabla f(x) = \\bmat\\frac{\\partial f(x)}{\\partial x_1}\\\\\\frac{\\partial f(x)}{\\partial x_2}\\\\\\vdots\\\\ \\frac{\\partial f(x)}{\\partial x_n}\\\\\\emat. \\nabla f(x) = \\bmat\\frac{\\partial f(x)}{\\partial x_1}\\\\\\frac{\\partial f(x)}{\\partial x_2}\\\\\\vdots\\\\ \\frac{\\partial f(x)}{\\partial x_n}\\\\\\emat. Similarly, the Hessian of f f at x x is a symmetric matrix in \\R^{n\\times n} \\R^{n\\times n} that contain second partial derivative information and is given by \\nabla^2 f(x) = \\bmat \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_2} & \\cdots & \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_n}\\\\ \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_2}& \\cdots & \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_2} & \\cdots & \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_n}\\\\ \\emat \\nabla^2 f(x) = \\bmat \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_2} & \\cdots & \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_n}\\\\ \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_2}& \\cdots & \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_2} & \\cdots & \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_n}\\\\ \\emat For example, for a function f(x) = x_1^2 + 8x_1x_2 - 2x_3^3 f(x) = x_1^2 + 8x_1x_2 - 2x_3^3 , its gradiet is \\nabla f(x) = \\bmat 2x_1 + 8x_2\\\\8x_1\\\\-6x_3^2\\emat \\nabla f(x) = \\bmat 2x_1 + 8x_2\\\\8x_1\\\\-6x_3^2\\emat , and its Hessian is \\nabla^2 f(x) = \\bmat 2 & 8 & 0 \\\\ 8 & 0 & 0 \\\\ 0 & 0 & -12x_3\\emat. \\nabla^2 f(x) = \\bmat 2 & 8 & 0 \\\\ 8 & 0 & 0 \\\\ 0 & 0 & -12x_3\\emat. Next, we look at directional derivatives. For a differentiable function f:\\R^n\\to \\R f:\\R^n\\to \\R , the directional derivative in the direction of \\vd \\in \\R^n \\vd \\in \\R^n is f'(\\vx;\\vd) = \\lim_{\\alpha\\to 0^+} \\frac{f(\\vx+\\alpha \\vd) - f(\\vx)}{\\alpha} = \\nabla f(\\vx)\\trans \\vd f'(\\vx;\\vd) = \\lim_{\\alpha\\to 0^+} \\frac{f(\\vx+\\alpha \\vd) - f(\\vx)}{\\alpha} = \\nabla f(\\vx)\\trans \\vd We say a f f function is flat at \\vx^* \\vx^* if f'(\\vx) = 0 f'(\\vx) = 0 . In terms of directional derivative, f'(\\vx) = 0 f'(\\vx) = 0 is equivalent to the directional derivative of f f equaling zero for all directions \\vd\\in \\R^n \\vd\\in \\R^n , i.e. \\forall \\vd\\in \\R^n,\\; f'(\\vx;\\vd) = 0 \\iff \\nabla f(\\vx) = 0 \\forall \\vd\\in \\R^n,\\; f'(\\vx;\\vd) = 0 \\iff \\nabla f(\\vx) = 0 Similarly, for a twice-differentiable function f:\\R^n\\to \\Re f:\\R^n\\to \\Re , the directional second derivative is f''(\\vx;\\vd) = \\lim_{\\alpha\\to 0^+} \\frac{f'(\\vx+\\alpha \\vd;\\vd) - f'(\\vx;\\vd)}{\\alpha} = \\vd\\trans\\nabla^2 f(\\vx)\\vd. f''(\\vx;\\vd) = \\lim_{\\alpha\\to 0^+} \\frac{f'(\\vx+\\alpha \\vd;\\vd) - f'(\\vx;\\vd)}{\\alpha} = \\vd\\trans\\nabla^2 f(\\vx)\\vd. The second directional deriviate f''(\\vx;\\vd) f''(\\vx;\\vd) provides curvature information of f f at a point \\vx \\vx along the direction \\vd \\vd . This can be used to characterize convex functions. Recall that a function is convex if for all \\vx,\\vy \\vx,\\vy in its domain, and all 0\\leq \\theta \\leq 1 0\\leq \\theta \\leq 1 , we have f(\\theta \\vx + (1-\\theta) \\vy) \\leq \\theta f(\\vx) + (1-\\theta) f(\\vy). f(\\theta \\vx + (1-\\theta) \\vy) \\leq \\theta f(\\vx) + (1-\\theta) f(\\vy). Equivalently, a function is convex if its directional second derivative is positive for all directions d\\in \\R^n d\\in \\R^n , i.e. \\forall\\ \\vx \\in \\text{dom}(f),\\; \\forall \\vd\\in \\R^n,\\quad f''(\\vx;\\vd)\\geq 0 \\forall\\ \\vx \\in \\text{dom}(f),\\; \\forall \\vd\\in \\R^n,\\quad f''(\\vx;\\vd)\\geq 0 Sufficient conditions of optimality \u00b6 We will state optimality conditions for the following minimization program \\mathop{\\text{minimize}}_{\\vx\\in \\set{S}} \\quad f(\\vx), \\mathop{\\text{minimize}}_{\\vx\\in \\set{S}} \\quad f(\\vx), where f : \\R^n\\to\\R f : \\R^n\\to\\R is a real valued function. We say a point \\vx^*\\in \\set{S} \\vx^*\\in \\set{S} is a minimizer of f(\\vx) f(\\vx) if \\nabla f(\\vx^*) = 0 \\nabla f(\\vx^*) = 0 and \\vd^T\\nabla^2 f(\\vx^*) \\vd > 0 \\vd^T\\nabla^2 f(\\vx^*) \\vd > 0 for all \\vd\\in \\R^n \\vd\\in \\R^n . Note that a square symmetrix matrix \\mA \\in \\R^{n\\times n} \\mA \\in \\R^{n\\times n} , like the Hessian matrix, is positive definite if \\vz\\trans\\mA\\vz >0 \\vz\\trans\\mA\\vz >0 for all \\vz \\in \\R^n \\vz \\in \\R^n (denoted by \\mA \\succ 0 \\mA \\succ 0 ). a maximizer of f(\\vx) f(\\vx) if \\nabla f(\\vx^*) = 0 \\nabla f(\\vx^*) = 0 and \\vd^T\\nabla^2 f(\\vx^*) \\vd < 0 \\vd^T\\nabla^2 f(\\vx^*) \\vd < 0 for all \\vd\\in \\R^n \\vd\\in \\R^n . In this case, \\nabla^2 f(\\vx^*) \\nabla^2 f(\\vx^*) is called a negative definite matrix and is denoted by \\mA \\prec 0 \\mA \\prec 0 . Lastly, if \\nabla f(\\vx^*) = 0 \\nabla f(\\vx^*) = 0 and \\nabla^2 f(x^*) \\nabla^2 f(x^*) is indefinite, i.e. neither positive nor negative definite, then \\vx^* \\vx^* is a saddle point. When \\nabla^2 f(x^*) \\nabla^2 f(x^*) is indefinite then there exits directions where the 1-d slices of the function f f are convex and concave.","title":"Unconstrained optimization"},{"location":"notes/unconstrained/#unconstrained-optimization","text":"In this lecture we consider unconstrained optimization given by \\begin{equation}\\label{unconstrained_prob} \\mathop{\\text{minimize}}_{\\vx \\in \\set{S}} f(\\vx),\\end{equation} \\begin{equation}\\label{unconstrained_prob} \\mathop{\\text{minimize}}_{\\vx \\in \\set{S}} f(\\vx),\\end{equation} where f:\\R^n \\rightarrow \\R f:\\R^n \\rightarrow \\R and \\set{S}\\subset \\R^n \\set{S}\\subset \\R^n . In the course, we will primarily consider minimization problems. This is because a maximizer of f(\\vx) f(\\vx) is the minimzer of -f(\\vx) -f(\\vx) .","title":"Unconstrained Optimization"},{"location":"notes/unconstrained/#optimality","text":"A point \\vx^*\\in \\set{S} \\vx^*\\in \\set{S} is a global minimizer of f f if f(\\vx^*) \\leq f(\\vx) f(\\vx^*) \\leq f(\\vx) for all \\vx\\in \\mS \\vx\\in \\mS global maximizer of f f if f(\\vx^*) \\geq f(\\vx) f(\\vx^*) \\geq f(\\vx) for all \\vx\\in \\mS \\vx\\in \\mS local minimizer of f f if f(\\vx^*) \\leq f(\\vx) f(\\vx^*) \\leq f(\\vx) for all \\vx\\in \\mS \\vx\\in \\mS where \\|\\vx-\\vx^*\\|_2 \\leq\\epsilon \\|\\vx-\\vx^*\\|_2 \\leq\\epsilon local maximizer of f f if f(\\vx^*) \\geq f(\\vx) f(\\vx^*) \\geq f(\\vx) for all \\vx\\in \\mS \\vx\\in \\mS where \\|\\vx-\\vx^*\\|_2 \\leq\\epsilon \\|\\vx-\\vx^*\\|_2 \\leq\\epsilon for some \\epsilon > 0 \\epsilon > 0 . A maximizer (or minimizer) \\vx^* \\vx^* of \\eqref{unconstrained_prob} is a strict maximizer (or minimizer) if for all points in a neighborhood of \\vx^* \\vx^* , the objective f(x) f(x) does not attain the value of f(\\vx^{*}) f(\\vx^{*}) . Equivalently, this definition of strict maximizer/minimizer in the global case can be stated as: \\vx^* \\vx^* is a strict global min or max if for all \\vx\\in \\set{S} \\vx\\in \\set{S} , we have f(\\vx^*) = f(\\vx) f(\\vx^*) = f(\\vx) if and only if \\vx = \\vx^* \\vx = \\vx^* . The above figure shows that even if the optimality is attained, the optimal point may not be unique. There are cases when the optimal point is not attained or even exist. An example of these cases are shown below \\exa{1} \\exa{1} Consider the minimization program \\mathop{\\text{minimize}}_{x,y} \\{f(x,y) = x + y: x^2 + y^2 \\leq 2\\}. \\mathop{\\text{minimize}}_{x,y} \\{f(x,y) = x + y: x^2 + y^2 \\leq 2\\}. The figure below shows the interaction bewtween the objective fuction and set \\{(x,y):x^2 + y^2 \\leq 2\\} \\{(x,y):x^2 + y^2 \\leq 2\\} . The pink lines are the \"level sets\" and the arrows are directions of descent. The optimal point is \\bmat -1\\\\ -1 \\emat \\bmat -1\\\\ -1 \\emat . \\exa{2} \\exa{2} Consider the minimization program \\mathop{\\text{minimize}}_{x,y} \\{f(x,y) = \\frac{x+y}{x^2+y^2+1}:x,y\\in\\R\\}. \\mathop{\\text{minimize}}_{x,y} \\{f(x,y) = \\frac{x+y}{x^2+y^2+1}:x,y\\in\\R\\}. The surface and contour plots of f(x,y) f(x,y) are using Plots ; pyplot () f ( x , y ) = ( x + y ) / ( x ^ 2 + y ^ 2 + 1 ) x = range ( - 4 , stop = 4 , length = 100 ) y = range ( - 5 , stop = 5 , length = 100 ) pyplot ( size = ( 700 , 200 )) plot ( plot ( x , y , f , st =: surface , camera = ( 100 , 30 )), plot ( x , y , f , st =: contour , camera = ( 100 , 30 ))) The global minimizer is \\begin{pmatrix} \\frac{1}{\\sqrt{2}},& \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}},& \\frac{1}{\\sqrt{2}} \\end{pmatrix} and the global maximizer is \\bmatp -\\frac{1}{\\sqrt{2}}, & -\\frac{1}{\\sqrt{2}} \\ematp \\bmatp -\\frac{1}{\\sqrt{2}}, & -\\frac{1}{\\sqrt{2}} \\ematp .","title":"Optimality"},{"location":"notes/unconstrained/#sufficient-conditions-in-1-d","text":"We will first look at 1-d function f:\\R\u2192\\R f:\\R\u2192\\R and state sufficent conditions for optimality of a point \\vx \\vx in the interrior of the set \\set{S} \\set{S} . Consider the following 1-d function The critial points of f f in the above figure are x \\in \\{ b ,\\ c,\\ d,\\ e\\} x \\in \\{ b ,\\ c,\\ d,\\ e\\} and these points satisfy f'(x) =0 f'(x) =0 . We may be able to use second derivative information to determine if these critial points are minimizer or maximizer. Precisely, for any function f:\\R\\rightarrow\\R f:\\R\\rightarrow\\R , a point x = x^* x = x^* is a local minimizer if f'(x) = 0 f'(x) = 0 and f''(x)>0 f''(x)>0 , and a local maximizer if f'(x) = 0 f'(x) = 0 and f''(x)<0 f''(x)<0 . However, if both f'(x) = 0 f'(x) = 0 and f''(x)=0 f''(x)=0 , there is not enough information to draw any conclusion. As an example, consider f(x) = x^3 f(x) = x^3 and f(x) = x^4 f(x) = x^4 . For both function, f'(x) = 0 f'(x) = 0 and f''(x) = 0 f''(x) = 0 , however x = 0 x = 0 is saddle point of x^3 x^3 and x = 0 x = 0 is a minimizer of x^4 x^4 . A proof of the sufficient condition of optimality follows directly from Taylor approximation of the function f(x) f(x) . Consider the case where f'(x^*) = 0 f'(x^*) = 0 and f''(x^*)>0 f''(x^*)>0 for some x^*\\in\\set{S} x^*\\in\\set{S} . Then by Taylor's theorem of f(x) f(x) at x = x^* x = x^* , we get f(x) = f(x^*) + \\underbrace{f'(x^*)(x-x^*)}_{ =\\ 0} + \\underbrace{\\frac{1}{2}f''(x^*)(x-x^*)^2}_{\\text{strictly positive}} + \\underbrace{O((x-x^*)^3)}_{\\text{small}} f(x) = f(x^*) + \\underbrace{f'(x^*)(x-x^*)}_{ =\\ 0} + \\underbrace{\\frac{1}{2}f''(x^*)(x-x^*)^2}_{\\text{strictly positive}} + \\underbrace{O((x-x^*)^3)}_{\\text{small}} for x x close to x^* x^* . So f(x) > f(x^*) f(x) > f(x^*) for all x x near x^* x^* , which imples x^* x^* is local minimizer. Similarly, we can prove f(x^*)=0 f(x^*)=0 and f''(x^*)<0 f''(x^*)<0 implies x^* x^* is a local maximizer. In the next section we will generalize these sufficient condition for any real valued function f:\\R^n \\rightarrow \\R f:\\R^n \\rightarrow \\R .","title":"Sufficient conditions in 1-d"},{"location":"notes/unconstrained/#gradient-hessian-and-directional-derivatives","text":"For a differentiable function f:\\R^n\\to\\R f:\\R^n\\to\\R , the gradient of f f at \\vx \\vx is a vector in \\R^n \\R^n . The gradient of f f at \\vx \\vx contain the partial derivative of f f at x x and is given by \\nabla f(x) = \\bmat\\frac{\\partial f(x)}{\\partial x_1}\\\\\\frac{\\partial f(x)}{\\partial x_2}\\\\\\vdots\\\\ \\frac{\\partial f(x)}{\\partial x_n}\\\\\\emat. \\nabla f(x) = \\bmat\\frac{\\partial f(x)}{\\partial x_1}\\\\\\frac{\\partial f(x)}{\\partial x_2}\\\\\\vdots\\\\ \\frac{\\partial f(x)}{\\partial x_n}\\\\\\emat. Similarly, the Hessian of f f at x x is a symmetric matrix in \\R^{n\\times n} \\R^{n\\times n} that contain second partial derivative information and is given by \\nabla^2 f(x) = \\bmat \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_2} & \\cdots & \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_n}\\\\ \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_2}& \\cdots & \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_2} & \\cdots & \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_n}\\\\ \\emat \\nabla^2 f(x) = \\bmat \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_2} & \\cdots & \\frac{\\partial f^2(x)}{\\partial x_1\\partial x_n}\\\\ \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_2}& \\cdots & \\frac{\\partial f^2(x)}{\\partial x_2\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_1} & \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_2} & \\cdots & \\frac{\\partial f^2(x)}{\\partial x_n\\partial x_n}\\\\ \\emat For example, for a function f(x) = x_1^2 + 8x_1x_2 - 2x_3^3 f(x) = x_1^2 + 8x_1x_2 - 2x_3^3 , its gradiet is \\nabla f(x) = \\bmat 2x_1 + 8x_2\\\\8x_1\\\\-6x_3^2\\emat \\nabla f(x) = \\bmat 2x_1 + 8x_2\\\\8x_1\\\\-6x_3^2\\emat , and its Hessian is \\nabla^2 f(x) = \\bmat 2 & 8 & 0 \\\\ 8 & 0 & 0 \\\\ 0 & 0 & -12x_3\\emat. \\nabla^2 f(x) = \\bmat 2 & 8 & 0 \\\\ 8 & 0 & 0 \\\\ 0 & 0 & -12x_3\\emat. Next, we look at directional derivatives. For a differentiable function f:\\R^n\\to \\R f:\\R^n\\to \\R , the directional derivative in the direction of \\vd \\in \\R^n \\vd \\in \\R^n is f'(\\vx;\\vd) = \\lim_{\\alpha\\to 0^+} \\frac{f(\\vx+\\alpha \\vd) - f(\\vx)}{\\alpha} = \\nabla f(\\vx)\\trans \\vd f'(\\vx;\\vd) = \\lim_{\\alpha\\to 0^+} \\frac{f(\\vx+\\alpha \\vd) - f(\\vx)}{\\alpha} = \\nabla f(\\vx)\\trans \\vd We say a f f function is flat at \\vx^* \\vx^* if f'(\\vx) = 0 f'(\\vx) = 0 . In terms of directional derivative, f'(\\vx) = 0 f'(\\vx) = 0 is equivalent to the directional derivative of f f equaling zero for all directions \\vd\\in \\R^n \\vd\\in \\R^n , i.e. \\forall \\vd\\in \\R^n,\\; f'(\\vx;\\vd) = 0 \\iff \\nabla f(\\vx) = 0 \\forall \\vd\\in \\R^n,\\; f'(\\vx;\\vd) = 0 \\iff \\nabla f(\\vx) = 0 Similarly, for a twice-differentiable function f:\\R^n\\to \\Re f:\\R^n\\to \\Re , the directional second derivative is f''(\\vx;\\vd) = \\lim_{\\alpha\\to 0^+} \\frac{f'(\\vx+\\alpha \\vd;\\vd) - f'(\\vx;\\vd)}{\\alpha} = \\vd\\trans\\nabla^2 f(\\vx)\\vd. f''(\\vx;\\vd) = \\lim_{\\alpha\\to 0^+} \\frac{f'(\\vx+\\alpha \\vd;\\vd) - f'(\\vx;\\vd)}{\\alpha} = \\vd\\trans\\nabla^2 f(\\vx)\\vd. The second directional deriviate f''(\\vx;\\vd) f''(\\vx;\\vd) provides curvature information of f f at a point \\vx \\vx along the direction \\vd \\vd . This can be used to characterize convex functions. Recall that a function is convex if for all \\vx,\\vy \\vx,\\vy in its domain, and all 0\\leq \\theta \\leq 1 0\\leq \\theta \\leq 1 , we have f(\\theta \\vx + (1-\\theta) \\vy) \\leq \\theta f(\\vx) + (1-\\theta) f(\\vy). f(\\theta \\vx + (1-\\theta) \\vy) \\leq \\theta f(\\vx) + (1-\\theta) f(\\vy). Equivalently, a function is convex if its directional second derivative is positive for all directions d\\in \\R^n d\\in \\R^n , i.e. \\forall\\ \\vx \\in \\text{dom}(f),\\; \\forall \\vd\\in \\R^n,\\quad f''(\\vx;\\vd)\\geq 0 \\forall\\ \\vx \\in \\text{dom}(f),\\; \\forall \\vd\\in \\R^n,\\quad f''(\\vx;\\vd)\\geq 0","title":"Gradient, Hessian and directional derivatives"},{"location":"notes/unconstrained/#sufficient-conditions-of-optimality","text":"We will state optimality conditions for the following minimization program \\mathop{\\text{minimize}}_{\\vx\\in \\set{S}} \\quad f(\\vx), \\mathop{\\text{minimize}}_{\\vx\\in \\set{S}} \\quad f(\\vx), where f : \\R^n\\to\\R f : \\R^n\\to\\R is a real valued function. We say a point \\vx^*\\in \\set{S} \\vx^*\\in \\set{S} is a minimizer of f(\\vx) f(\\vx) if \\nabla f(\\vx^*) = 0 \\nabla f(\\vx^*) = 0 and \\vd^T\\nabla^2 f(\\vx^*) \\vd > 0 \\vd^T\\nabla^2 f(\\vx^*) \\vd > 0 for all \\vd\\in \\R^n \\vd\\in \\R^n . Note that a square symmetrix matrix \\mA \\in \\R^{n\\times n} \\mA \\in \\R^{n\\times n} , like the Hessian matrix, is positive definite if \\vz\\trans\\mA\\vz >0 \\vz\\trans\\mA\\vz >0 for all \\vz \\in \\R^n \\vz \\in \\R^n (denoted by \\mA \\succ 0 \\mA \\succ 0 ). a maximizer of f(\\vx) f(\\vx) if \\nabla f(\\vx^*) = 0 \\nabla f(\\vx^*) = 0 and \\vd^T\\nabla^2 f(\\vx^*) \\vd < 0 \\vd^T\\nabla^2 f(\\vx^*) \\vd < 0 for all \\vd\\in \\R^n \\vd\\in \\R^n . In this case, \\nabla^2 f(\\vx^*) \\nabla^2 f(\\vx^*) is called a negative definite matrix and is denoted by \\mA \\prec 0 \\mA \\prec 0 . Lastly, if \\nabla f(\\vx^*) = 0 \\nabla f(\\vx^*) = 0 and \\nabla^2 f(x^*) \\nabla^2 f(x^*) is indefinite, i.e. neither positive nor negative definite, then \\vx^* \\vx^* is a saddle point. When \\nabla^2 f(x^*) \\nabla^2 f(x^*) is indefinite then there exits directions where the 1-d slices of the function f f are convex and concave.","title":"Sufficient conditions of optimality"},{"location":"notes/constrained/","text":"Constrained optimization \u00b6 These are the new notes","title":"Constrained optimization"},{"location":"notes/constrained/#constrained-optimization","text":"These are the new notes","title":"Constrained optimization"}]}